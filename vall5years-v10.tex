%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{report}
\usepackage[a4paper]{geometry}
\usepackage[myheadings]{fullpage}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{amsmath}
\usepackage{graphicx, wrapfig, subcaption, setspace, booktabs}
\usepackage[T1]{fontenc}
\usepackage[font=small, labelfont=bf]{caption}
\usepackage{fourier}
\usepackage[protrusion=true, expansion=true]{microtype}
\usepackage[english]{babel}
\usepackage{sectsty}
\usepackage{url, lipsum}
\usepackage{listings}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}
\onehalfspacing
\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}

%-------------------------------------------------------------------------------
% HEADER & FOOTER
%-------------------------------------------------------------------------------
\pagestyle{fancy}
\fancyhf{}
\setlength\headheight{15pt}
\fancyhead[L]{Jaime G\'omez-Ram\'irez }
\fancyhead[R]{Fundaci\'on Reina Sof\'ia}
\fancyfoot[R]{Page \thepage\ of \pageref{LastPage}}
%-------------------------------------------------------------------------------
% TITLE PAGE
%-------------------------------------------------------------------------------

\begin{document}

\title{ \normalsize \textsc{5 years of the Vallecas Project}
		\\ [2.0cm]
		\HRule{0.5pt} \\
		\LARGE \textbf{\uppercase{Machine Learning in the Vallecas Project}}
		\HRule{2pt} \\ [0.5cm]
		\normalsize \today \vspace*{5\baselineskip}}

\date{}

\author{
		Jaime G\'omez-Ram\'irez \\
		Fundaci\'on Reina Sof\'ia \\
		Centre for Research in Neurodegenarative Diseases }

\maketitle
\tableofcontents
\newpage

%-------------------------------------------------------------------------------
% Section title formatting
\sectionfont{\scshape}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
% BODY
%-------------------------------------------------------------------------------

\section*{Abstract}

We utilize Machine Learning techniques, in particular, Deep Networks to perfom predictive analytics on The Vallecas Project dataset that covers five years of data collection.


\section{Introduction}
The Vallecas Project for early detection of AD is the most ambitious population-based study in Spain. The project is carried out in the Queen Sofia Foundation Alzheimer Center by a multidisciplinary team of researchers from the CIEN Foundation. The main objective of the Vallecas Project is to elucidate, through tracking of progression of the cohort, the best combination of  parameters, clinical and others \footnote{Note in the inception of the project only clinical parameters were considered, here we advocate for a more inclusive set of parameters including neuropsychological, and also features related to the lifestyle such as nutrition and physical exercise} , that are informative about the medium and long-term features that distinguish individuals who will develop future cognitive impairment from those who will not. Thus, it intends to identify various markers to eventually determine the potential risk that each individual could have to develop the disease in the future.

\section{CRISP-DM model}
%James Wu, Foundations of predictive analytics
The voluminous and complex (heterogeneity and dimensionality) dataset collected in the Vallecas Project make it particularly to build analytics of any kind, let alone predictive analytics.
In order to try to cope with this difficulty, we use a methodology well suited for machine learning analytics called CRISP-DM model (CRoss Industry Standard Process for Data Mining).
The CRISP-DM model consists of 5 phases:
\begin{enumerate}
%1. Problem understanding
\item
\item Definition of the problem. The data set is contains a number of features and we want to clarify the relationships between those inputs and most importantly which if any and in which measure have predictive power about conversion to MCI and dementia.
\item Assessment of scenarios for analysis. The main resource available are spreadsheets that contain demographic e.g age, school years etc. genetic: APOE, cognitive performance metrics from neuropsychological tests together with features related to life style  including nutrition and physical exercise among others. MRI and fMRI are also available but need ti be integrated with the above mentioned features, in this we will not deal with neuroimaging data.
%2. Data Understanding
\item Data understanding:
\item Data understanding
\item Data collection
\item Data description: format, volume, description of attributes \footnote{Here we will refer indistinctly features, attributes, inputs and dimensions}
\item Exploratory data analysis (EDA): charts, plots to visualize data features find associations and correlations. This task comprises: explore and visualize data, select attributes (most important, remove redundant or dependent attributes), test hypothesis about correlations and associations.
\item Data quality analysis: Deal with missing values, inconsistent values
%3. Data preparation
\item Clean, wrangle and curate the dataset before the learning machinery is launched to build models. The most time consuming $(60\%) of the time$
\item Data integration : multiple. datasets, this will have to be taken care of when we integrate with the imaging dataset.
\item Data wrangling: handle missing values (remove rows, handle missing values), formatting into csv, json etc.
% 4. Modelling
% 5. Evaluation
% 6. Deployment

 \end{enumerate}
In the next section we describe the work done in point 3, \emph{Data preparation}

\subsection{Data preparation}
% Transformation including Scaling and discretize continuous variables (binning)
Most techniques are sensitive to scaling \cite{wu2012foundations} and this is because there used to be an implicit metric or definition of nearness in the dataset. The most common scaling is z-scaling which is easily performed as $\prime{x}_i=\frac{x_i -\mu}{\sigma_i}$, that is, the set of variables $X = x_i, i=1..n$ are scaled to be centered and have the same spread \footnote{Note that we may want to use a lognormal rather than normal as a reasonable distribution. The Benford law is pertinent here: in many naturally occurring collections of numbers, the leading significant digit is likely to be small. This law is reminiscent of Pareto or Zipf law of numbers. For such variables perform a log transform, $x'_i = \log x_i$}.
z-scale and log transform are example of linear transformations but we may need nonlinear transformations, for example discretize a continuous variable. For example we can discretize the age variable in bins of 3 or 5 years. Although machine learning techniques are perfectly able to deal with continuous variable the reason we still may be a good idea to discretize is that in doing so we will be helping the model to learn the relationship between inputs and output, binning can also provide statistical smoothing and robustness in the modeling process (The cutpoint of the variable can be calculated with Gini index, entropy, chi squared or KS criteria).

Praxis: always encode your data as best as possible to reduce the work of the model as much as possible.

% Variable Selection:
Variable Selection consists of filtering the most important features and remove those that are not needed - Feature selection and wrapper.
Note that it could be always possible to select all the features and let the model do the job of finding the best predictive model, but this is not a good idea in practical terms and the reason is \emph{stability}. Remove variables that have a spurious or non consistent relationship with the target, also the principle of parsimony applies: careful variable selection and elimination.
%Detection of Multicollinearity
Another important issue if multicollinearity, we need to identify and remove correlated variables (crucial for model building). We can do this in two steps, first remove variables that produce another one, for example if A = B + C + D we can remove B, C and D.
In a second step we deal with pair wise collinearity which is not exactly multicollinearity. For that we calculate the correlation between all pairs (build correlation matrix) and remove those with a large value (above some threshold). To do this systematically we can build a graph, in the node note the correlation with the target and in the edges the correlation value with the adjacent components. We will select variables with largest corr with target and remove its correlated variables. (page 211, \cite{wu2012foundations}).
%(Belsey's 80) approach is based on covariance matrix of the least squares coefficient vector. Decaying importance of the variables as ordered by the eigenvalues

% Missing data imputation

%CODE
%tensorflow/production/descriptive_stats.py
\subsection{Feature engineering}
Feature engineering is the process of transforming the gathered data into features that better represent the problem that we are trying to solve to the model, to improve its performance and accuracy. In feature engineering we create additional features for example by combining features to feed into the model. The rationale of feature engineering is to bring into the modeling process the domain expertise, it may also help with overfitting. 

\subsection{Data leakage}
Data leakage describes the situation in which data you are including to train the machine learning algorithm includes the very thing you are trying to predict, for example if I am trying to predict conversion and I include conversion in a given year as a feature, another example of data leakage is having test data in the training data set this will lead to overfitting. Data leakage can happen in ways more subtle than those just described, for example in time series using features from the future not available for the current prediction using. To eliminate leakage, before building the  model look for features highly correlated with the target, after building the model look for surprising feature behavior (large information gains). The proof that there is leakage is obtained by comparing the deployment performance versus the train and evaluation performance. 

\section{Methods}
\cite{patania2017topological}
In supervised learning a. odel refers to the mathematical object that malkes a prediction $y_i$ given $x_i$, one of the easiest incarnations is a linear combination of weighted input features, $y_i = \sum_k w_j x_{ij}$ prediction values have different interpretations depending on the task, i.e. classification, regression. For example, it can be logistic transformed to get the probability of positive class in logistic regression, and it can also be used as a ranking score when we want to rank the outputs.
%http://xgboost.readthedocs.io/en/latest/model.html
The parameters eg $w_j$ in linear model, are the undetermined part that we need to learn from data. To find the best parameters given the training data we need to measure the performance of the model given a certain set of parameters, this is the objective function. Objective functions have two terms, training loss and the regularization.
\begin{equation}
obj(w) = L(w) + R(w)
\end{equation}
where L is the training loss and R is the regularization. The training loss measures how predictive our model is on training data, for example if we use mean squared $L(w) = \sum_i \hat{y_i} - y_i)^2$ but there are many other possible loss functions for example log loss.
the regularization term, R, controls the complexity of the model which helps to avoid overfitting.


We exhaustively explore a number of models and evaluate their performance on the Vallecas Project dataset.
The appendix section provides a more in depth description.  of the models implemented, in this section we will
go through the models and give a layman's description of the models making emphasis in the applicability and pros and cons.

Learning happens in two steps, first we build an estimator and then we fit the model to the data, the model performance can be then studied. The estimator may need to specify parameters and hyperparameters, for example, decision tree needs to specify the maximum depth of the tree, other models for example Naive Bayes are less dependent on the choice of parameters. 
This easiness of parametrization does not come for free (no free lunch), Naive Bayes assumes that each feature is conditionally independent of every other feature given the target category, $p(x_i|C) == p(x_i|C), \forall i,j$ which is quite unrealistic in our dataset. Furthermore, when dealing with continuous data Naive Bayes assumes also a Gausssian or normal distribution. Thus, 
$p(x = a|C_k) = \frac{e^{-\frac{(a-\mu_k)^2}{2\sigma_k^{2}}}} {\sqrt{2\pi\sigma_k^{2}}}$. 


Once the estimator is being learned, we need to test it with the predictor. Predictors generate forecasts using the learned estimator fed with unknown  data, that is data that were not used to train the estimator.
% 0. Transformers : before learning, some are simple: replacing missing data with a contant, taking a log transform ..or some transforms are learning algorithms themselves like PCA
% The pipeline is: Read Data -> apply some simple or complex (PCA) transformation -> fit an appropriate model -> predict using the model for unseen data. (iteratively)
% For model tuning and selection we can use two meta estimators: GridSearchCV and RandomizedSearchCV to search the best parameters. Grid provides a grid of possible parameters and try each possible combination among them to arrive at the best one.. Randomized  optimizes this sampling the parameters to avoid combinatorial explosion of Grid. the also allow different x-validation schemes and score functions to measure performance.

\subsection{Model evaluation: quantifying the quality of predictions }
%http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
Whether we are doing classification both continuous and discrete or clustering we need to study the behavior of our model prediction, that is, model selection will rely on model evaluation criteria. Evaluation criteria consists in computing scoring objects that gives us information about model performance. It ought to be remarked that scoring is not dataset independent, for example, accuracy may be an optimal indicator of model performance in some situations but suboptimal in others. For example, in a binary classification problem -converter vs not converter- in which the converters are a small minority (e.g. $10\%$, accuracy is not the best measure to use because accuracy is just the number of correct prediction divided by the total number of instances. A dummy classifier that doesn't look at the features at all and always predict the most frequent class (i.e. non converter) will have an accuracy of $90\%$, that is, the dummy classifier will predict the right label for 90 out of 100 examples.
%https://www.coursera.org/learn/python-machine-learning/lecture/BE2l9/model-evaluation-selection
Dummy classifiers provide a null metric, in the results section we will compare the model prediction with dummy classifiers as a sanity check on the model's performance.
When we find that the model accuracy is close to the null accuracy baseline given by the dummy classifier where are in any of these situations:
\begin{itemize}
	\item Features are ineffective or missing
	\item Poor choice of hyperparameters or kernel in the model (eg in SVM) 
	\item Large class imbalance 
\end{itemize}
 %Scoring objects take care of that, as a rule,  higher return value better than lower return values.
 A very helpful mathematical object to study model performance is the confusion matrix. For example, for a bibary prediction task we have a $2 \times 2$ matrix where rows represent the true values and the columns the predicited values. Thus, the values in the diagonal are true predictions and off diagonal the predictions that the model got wrong, rather  than having a single number we have a matrix from which other metrics can be derived like recall, precision and $F_\beta$ metrics.
 %from sklearn.metrics import confusion_matrix
\[
M=
  \begin{bmatrix}
    TN & TF  \\
    FN & TP 
  \end{bmatrix}
\]
From the confusion matrix we can derive:
$\textit{Accuracy} = \frac{TN + TP}{TN+TP+FN+FP}$. 
Recall, also True Positive Rat, sensitivity and probability of detection  is the fraction of all positive instances does the classifier identifies correctly as positive, $\text{Recall} = \frac{TP}{TP+FN}$, in biomedical applications in which is crucial to do not miss the positives (converters) recall is a very important metric.
Precision is the fraction of positive predictions that are correct (finding sick when they are not) $\text{Recall} = \frac{TP}{TP+FP}$.
Specificity or False positive rate is the fraction of all negative instances that the classifier incorrectly identifies as positive (here the smaller the better)  $\text{specificity} = \frac{FP}{FP+TN}$

Precision and recall are complementary in the sense that you can increase the precision but at the expense of the recall and viceversa. Recall oriented machine learning tasks are biomedical (e.g. tumor detection) and precision oriented tasks are e.g. document classification. Precision (x-axis) recall (y-axis) curves, an ideal classifier would achieve ideal precision (1.0) and ideal recall (1.0) (the top right corner) 

The F1 score combines both precision and recall into a single number, mathematically based on the harmonic mean of precision and recall, $F_1 = 2\frac{\text{precision*recall}}{precision+recall} = \frac{2 * TP}{2TP + FN + FP}$. More generally we can modulate how much emphasis we do to precision versus recall using the parameter $\beta$. Thus, $F_\beta = (1+\beta^2)\frac{\text{precision*recall}}{(\beta^2 * precision)+recall} = \frac{2 * TP}{2TP + FN + FP}$ (precision oriented $\beta=0.5$, false positives hurts performance more than false negatives, recall oriented $\beta=2$, false negatives hurts performance more than false positives).
%from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
%print('Accuracy:{:.2f}'.format(accuracy_score(test, predicted)));precision_score, recall_score;f1_score
%print(classification_report(test,pred,target_names=['not converter 0', 'converter 1']))
%varying the decision threshold for predict_proba https://www.coursera.org/learn/python-machine-learning/lecture/0YPe1/classifier-decision-functions

Receiving Operating Characteristic ROC curves, false positive rate (x-axis), true positive rate (y-axis) (top left corner is the ideal point, false positive 0 and true positive 1), area under the curve AUC is a way to summarize the classifier performance in one single number.

Other metrics that will be shown in the Results section are:
\begin{itemize}
	\item Hamming loss: Calculates the Hamming (distance) between two sets
	\item Jaccard similarity: average of Jaccard similarity coefficients (Jaccard index)
	\item Hinge loss: computes the average distance between the predictions and the data using hinge loss (only prediction errors) Used in SVM
	\item Matthews correlation coefficient (phi coefficient): it is regarded as a balance measure useful even if the classes are of very different sizes. [-1,1] +1 is perfect prediction , 0 average random prediction and -1 inverse prediction. It is also called phi coefficient.
	%from sklearn import matthews_corrcoef, matthews_corrcoef(y_pred, y_true) # tp*tn - fp*fn/ sqrt()
	\item Zero one loss: computes the sum or average of the 0-1 classification loss $L_{0-1}$ over n samples. By default normalizes over the sample
\end{itemize}

\subsection{Model selection}
In the previous section we sketched different evaluation metrics now we will see how to apply them in order to select the best model for our application. 
Model selection is the phase in the modeling process in which we opt for one model or other based oin the model's performance which is being previously evaluated.

There are three main methods of model selection: Train-Test split, K-fold Cross Validation and Grid Search.
The train/test method split the data set into two portions testing set and test test. The training set is used to train the model and the testing set is used to test the model. The pros are flexibility and speed, the cons provides a high  variance estimate of out of sample. 

This method splits the data set into K equal partitions (“folds”), then use 1 fold as the testing set and the union of the other folds as the training set. Then the model is tested for accuracy. The process will follow the above steps K times, using different fold as the testing set each time. The average testing accuracy of the process is the testing accuracy.
%https://towardsdatascience.com/machine-learning-workflow-on-diabetes-data-part-01-573864fcc6b8
In K-fold Cross Validation the data set is divided into K equal partitions or “folds”, 1 fold is used as the testing set and the union of the other sets are the training set. The process is performed K times using different fol as the testing set each time. the average testing accuracy of the K iterations is the testing accuracy.
In Cross Validation we do not to set any parameter, apart from the number of folds (cv) from which we calculate the average performance across folds. The pros are more accurate estimate of out-of-sample accuracy, more “efficient” use of data in the sense that every observation is used for both training and testing. The cons are slow compared to Train/Test split.

In GridSearch we need a gamma parameter (for example in SVC with rbf kernel set the radius), GridSearchCV finds the gamma that optimizes the given evaluation metric (accuracy, precision etc).
%cross validation, no parameters only cv (number of folds)
%from sklearn.model_selection import cross_val_score; from sklearn.svm import SVC
%print('cross validation (accuracy)', cross_val_accuracy(clf, X,y, cv=5))
%print('cross validation (AUC)', cross_val_accuracy(clf, X,y, cv=5, scoring = 'roc_auc'))
%print('cross validation (recall)', cross_val_accuracy(clf, X,y, cv=5, scoring = 'recall'))
%not that we do not any parameter tunning just 5 folds and examine performance

%% Grid Search, we need gamma parameter
%from sklearn.model_selection import GridSearchCV
%grid_values = {'gamma': [0.001, 0.01, 0.1, 1 10]}
%grid_clf_acc = GridSearchCV(clf, param_grid = grid_values); grid_clf_acc.fit(X_train, y_train)
%#optimize for acc
% y_decision_fn_scores_acc = grid_clf_acc.decision_function(X_test); print('Grid best parameter (max. accuracy):', grid_clf_acc.best)
%print('Grid best score (accuracy):', grid_clf_acc.best_score_) 
%#optimize for auc
%grid_clf_auc = GridSearchCV(clf, param_grid = grid_values, scoring='roc_auc'); grid_clf_auc.fit(X_train, y_train)
%print('Grid best score (accuracy):', grid_clf_auc.best_score_) 
%the gamma parameter can be equal but also different depending on the metric used

The evaluation metric supported for model selection are
\begin{lstlisting}
%from sklearn.metrics.scorer import SCORERS
%sorted(list(SCORERS.keys()))
%['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'log_loss', 'mean_absolute_error', 'mean_squared_error', 'median_absolute_error', 'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']
\end{lstlisting}
The decision boundaries change when it is optimize for different metrics

Three data splits:
\begin{itemize}
	\item Training set: model building
	\item Evaluation set: model selection 
	\item Test set: final evaluation
\end{itemize}

\sectoion{Results}
Results for each classifier.
\subsection{K-neighbors}
K-Neighbors (KNN) classifier is non-parametric \footneote{KNN makes no explicit assumptions about the functional form of the mapping $h:X->y$ this is contrary to what happens in naive Bayes where it is assumed that data follow a Gaussian distribution}, instance-based supervised learning algorithm. KNN doesn't assume normality of data (non-parametric) a¡not it explicitly learn a model (instance-based) the algorithm rather memorizes the entire training set which are subsequently used as “knowledge” for the prediction phase, that is, KNN works like an on-demand process only when a query to our database is made will the algorithm use the training instances to spit out an answer. 
k-NN works as follows, given a training set $X_train$ with labels $y_train$ and an instance $x_test$ to be classified, first, finds the set of most similar instances to $x_test$ that are in $X_train$ and we call then $X_NN$. Next, it predicts the labels $y_NN$ for $X_NN$ and finally predicts the label for $x_test$ by combining the labels $y_NN$ using simply majority vote. 
It requires minimal training but relies upon expensive testing. KNN is used as a benchmark for more complex classifiers such as SVM and artifical network which contrary to KNN have lengthy training phase albeit a very fast testing phase. KNN can suffer from skewed class distributions, if a certain class is very frequent in the training set it might perform poorly because the majority voting 
it will tend to dominate the response. Finally, the accuracy of KNN can be severely degraded with high-dimension data because there is little difference between the nearest and farthest neighbor
%https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/
The algorithm requires the computation of the number of neighbors k, once this parameter is estimated performing cross validation on our dataset using a generated list of an odd number k neighbors (by default 5). For low k neighbor parameter, eg $k=1$ the decision boundaries will be very convoluted (high complexity, over fitting), for large k now the classifier needs to weigh the votes of many neighbors, not juts one, so single training data points will not have the dramatic influences in the prediction as in k=1 and the result is a smoother boundary (model with lower complexity, less variance), so for k equal all the number the prediction will be always the most frequent class (model underfit). The metric distance is the Minkowski distance ($p=1$ Manhattan, $p=2$ Euclidean). 
%https://www.coursera.org/learn/python-machine-learning/lecture/I1cfu/k-nearest-neighbors-classification-and-regression

\subsection{Naive Bayes Classifier}
%https://www.evernote.com/shard/s263/nl/33458921/18793f59-d0f8-40c0-b1a5-6f51bab38c60?title=Naive%20Bayes%20Classifier
The naive Bayes \footnote{Bayes hardly deserves the glory, Laplace had not only the intuition but provided the mathematical formulation, this is not rare in science and human affair see Amerigo Vespucci giving the name of America, absolutely disproportionate} classifier determines to which class a sample belongs to by calculating the posterior probability using the Bayes formula $p(C|X)$ where X is the sample and C the target class, for example converter.
The Bayes rules combines two sources of information -prior and likelihood- to calculate the posterior which is the final classification criterion.
For example if the posterior (posterior = prior x likelihood) of X being a converter is larger than the posterior of X being a non converter then we classify x as converter ($p(C|X) > p(~C|X)$). Thus, given a subject X with the features values $f1_x, f2_x and f3_x$ (eg $f1_x=0$ (apoe), $f2_x=30$ (mmse) and $f3_x=12$ (school years)) we just need to calculate 
the posterior probability $P(converter|f1_x, f2_x, f3_x) = P(f1_x|converter) * P(f2_x|converter) * P(f3_x|converter) * P(converter)$.

The assumption is that the features (predictor, inputs) are independent, and this is why we can easily calculate the Likelihood (e.g. $P(fi_x|converter)$) as a product of conditional probabilities. Note that this assumption is quite strong it means that the presence of a feature in a class is unrelated to the presence of other feature, for example in the previous example having large mmse and the years of school are uncorrelated.
When the assumption of independence holds, a Naive Bayes classifier tends to perform better than other models such as logistic regression with the advantage that naive Bayes needs less training data to perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).
Bayes tend to be efficient in learning and prediction but poor in generalization compared with more sophisticated methods.

\subsection{Decision trees}
Decision trees are a popular supervised learning method can be used for both regression and classification  a very good exploratory method to understand which are the features more relevant to predict the target.
In essence, decision trees learn a set of conditional rules (if-then) on features values that result in the prediction, so the idea is to find a set of rules useful to categorize an object (reminiscent to expert systems in the 70s and 80s) but rather than figure out manually the rules for every task the algorithm will learn it for us.
% if accuracy on training > acc test we are doing overfitting, one strategy is to keep addin
When the tree keeps adding rules it may become very complex we will memorize the data and will overfit, to avoid this we can prevent the growth of the tree using the max depth parameter which controls the maximum number of splitting points (eg 3) and minimum-maximum samples leaf which is the number of data instances that a leaf can have to avoid further splitting.  In practice, the max depth parameter is enough to deal with overfitting.
the pros are that are easy to understand via visualization, no need of feature normalization or scaling and work well with databases with heterogeneous data types. the cons is that they overfit (complex trees). In the next section we show ensemble methods such as Random Forests that might help to alleviate the overfitting problem found in Decision Trees.


\subsection{Gradient boosted decision trees}
Random forest and gradient boosted are both tree based ensemble methods. Like Random forest Gradient boosted creates an ensemble of decision tree to create more powerful predictive models for classification and regression. Random forest is a bagging algorithm Gradient boosted is a boosted algorithm, bagging reduces variability (stability of the classifier) and boosting reduces both variance and bias. It reduces variance because you are using multiple models as we do in bagging and it reduces bias by training the subsequent model by telling him what errors the previous models.
Boosting was originally a theoretical invention motivated by the question "can we build a stronger model using weaker models", weak learner here mean shallow trees built in a non random way (different from RF) to create a model that makes fewer and fewer mistakes as these are added (see Schapire The Strength of Weak Learnability). An interesting property of GBDT is that it works with different loss functions, even when the derivative is not convex (eg pinball loss function). %https://www.lokad.com/pinball-loss-function-definition
 There are tow main algorithms: Adaboost and Gradient boosting. Adaboost is the original algorithm and what it does is to you tell subsequent models to punish more heavily observations mistaken by the previous models. In Gradient boosting you train each subsequent model using the residuals (the difference between the predicted and true values). The learning rate parameter controls how hard each tree tries to correct the mistakes from previous rounds (high learning rate compelx trees, low learning rate simpler trees). 
What do they work? We don't really know, note that Data science is an empirical science, "because it works" is good enough \footnote{The 'As if' argument in economics (Milton Freedman) engineering view of scientific, see the the analogy of the billiard player plays “as if he knew ... complicated mathematical formulas”. For Friedman scientific theory (hypothesis or formula) cannot be tested by testing the realism of its assumptions.  All that matters is the accuracy of a theory’s predictions, not whether or not its assumptions are true. }. %http://www.rweconomics.com/BPA.htm

GBDT are often best off-the-shelf accuracy on many problems, require not much memory and are fast but like random forests the results are hard for humans to interpret, require careful tuning of the learning rate.

% Extreme gradient boosting
%http://xgboost.readthedocs.io/en/latest/model.html
%https://github.com/dmlc/xgboost
The optimization problem of decision trees (forest) is much harder than than traditional optimization problem like back propagation which just do gradient descent. Since it is not easy to train all the trees at once we can use an additive strategy: fix what we have learned, and add one new tree at a time. 

\subsection{Random forests}

Random forest is a bagging algorithm because it tries to "fix" the reliability problem existing in the decision tree model. DT are unreliable in the sense that small changes in your data produce may produce large changes very different decision trees\footnote{This is reminiscent to the the butterfly effect in chaos theory coined by Edward Lorenz}. This is where bagging comes from, we can create a robust model through bagging, that is, create different models by resampling your data. Each tree will deal with a different set of the data called the bootstrap sample, chooses at random with replacement this means that a bootstrap sample may have missing instances and also repeated instances doing this N times (one for each Tree) importantly there is not only randomness in picking the dataset for each tree but also there is randomness on the features from which to decide to split or not, if the max features is 1 we have forests with diverse complex trees and with max features closed to the number of features will lead to similar forests with simpler trees. Once the random forest is train the overall prediction is a a weighted vote.
Random forest is just the application of bagging to decision trees, and we want to do this if we care about stability.




Random forest inherit many of the benefits of decision trees. Pros widely used excellent performance on a variety of problems, easily parallelized, does not require careful normalization of features it may not be good for very highly dimensional problems (text classifiers). some of the k parameters are n estimators (default 10) is the number of trees, the max features parameters has a large effect on performance influence the diversity of trees in the forest, the max depth by default is None which means that the tree will continue to split until all nodes (or the min samples which is 2) in the leaf belong to the same class.









\section{Appendix}



%-------------------------------------------------------------------------------
% REFERENCES
%-------------------------------------------------------------------------------
\newpage
\section*{References}
\addcontentsline{toc}{section}{References}


% BibTeX users please use
\bibliographystyle{spmpsci}
\bibliography{../bibliography-jgr/bibliojgr}

\end{document}

%-------------------------------------------------------------------------------
% SNIPPETS
%-------------------------------------------------------------------------------

%\begin{figure}[!ht]
%	\centering
%	\includegraphics[width=0.8\textwidth]{file_name}
%	\caption{}
%	\centering
%	\label{label:file_name}
%\end{figure}

%\begin{figure}[!ht]
%	\centering
%	\includegraphics[width=0.8\textwidth]{graph}
%	\caption{Blood pressure ranges and associated level of hypertension (American Heart Association, 2013).}
%	\centering
%	\label{label:graph}
%\end{figure}

%\begin{wrapfigure}{r}{0.30\textwidth}
%	\vspace{-40pt}
%	\begin{center}
%		\includegraphics[width=0.29\textwidth]{file_name}
%	\end{center}
%	\vspace{-20pt}
%	\caption{}
%	\label{label:file_name}
%\end{wrapfigure}

%\begin{wrapfigure}{r}{0.45\textwidth}
%	\begin{center}
%		\includegraphics[width=0.29\textwidth]{manometer}
%	\end{center}
%	\caption{Aneroid sphygmomanometer with stethoscope (Medicalexpo, 2012).}
%	\label{label:manometer}
%\end{wrapfigure}

%\begin{table}[!ht]\footnotesize
%	\centering
%	\begin{tabular}{cccccc}
%	\toprule
%	\multicolumn{2}{c} {Pearson's correlation test} & \multicolumn{4}{c} {Independent t-test} \\
%	\midrule
%	\multicolumn{2}{c} {Gender} & \multicolumn{2}{c} {Activity level} & \multicolumn{2}{c} {Gender} \\
%	\midrule
%	Males & Females & 1st level & 6th level & Males & Females \\
%	\midrule
%	\multicolumn{2}{c} {BMI vs. SP} & \multicolumn{2}{c} {Systolic pressure} & \multicolumn{2}{c} {Systolic Pressure} \\
%	\multicolumn{2}{c} {BMI vs. DP} & \multicolumn{2}{c} {Diastolic pressure} & \multicolumn{2}{c} {Diastolic pressure} \\
%	\multicolumn{2}{c} {BMI vs. MAP} & \multicolumn{2}{c} {MAP} & \multicolumn{2}{c} {MAP} \\
%	\multicolumn{2}{c} {W:H ratio vs. SP} & \multicolumn{2}{c} {BMI} & \multicolumn{2}{c} {BMI} \\
%	\multicolumn{2}{c} {W:H ratio vs. DP} & \multicolumn{2}{c} {W:H ratio} & \multicolumn{2}{c} {W:H ratio} \\
%	\multicolumn{2}{c} {W:H ratio vs. MAP} & \multicolumn{2}{c} {\% Body fat} & \multicolumn{2}{c} {\% Body fat} \\
%	\multicolumn{2}{c} {} & \multicolumn{2}{c} {Height} & \multicolumn{2}{c} {Height} \\
%	\multicolumn{2}{c} {} & \multicolumn{2}{c} {Weight} & \multicolumn{2}{c} {Weight} \\
%	\multicolumn{2}{c} {} & \multicolumn{2}{c} {Heart rate} & \multicolumn{2}{c} {Heart rate} \\
%	\bottomrule
%	\end{tabular}
%	\caption{Parameters that were analysed and related statistical test performed for current study. BMI - body mass index; SP - systolic pressure; DP - diastolic pressure; MAP - mean arterial pressure; W:H ratio - waist to hip ratio.}
%	\label{label:tests}
%\end{table}
