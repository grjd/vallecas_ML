%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{report}
\usepackage[a4paper]{geometry}
\usepackage[myheadings]{fullpage}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{amsmath}
\usepackage{graphicx, wrapfig, subcaption, setspace, booktabs}
\usepackage[T1]{fontenc}
\usepackage[font=small, labelfont=bf]{caption}
\usepackage{fourier}
\usepackage[protrusion=true, expansion=true]{microtype}
\usepackage[english]{babel}
\usepackage{sectsty}
\usepackage{url, lipsum}
\usepackage{listings}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}
\onehalfspacing
\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}

%-------------------------------------------------------------------------------
% HEADER & FOOTER
%-------------------------------------------------------------------------------
\pagestyle{fancy}
\fancyhf{}
\setlength\headheight{15pt}
\fancyhead[L]{Five years of the Vallecas Project}
\fancyhead[R]{Fundaci\'on Reina Sof\'ia}
\fancyfoot[R]{Page \thepage\ of \pageref{LastPage}}
%-------------------------------------------------------------------------------
% TITLE PAGE
%-------------------------------------------------------------------------------

\begin{document}

\title{ \normalsize \textsc{Five years of the Vallecas Project}
		\\ [2.0cm]
		\HRule{0.5pt} \\
		\LARGE \textbf{\uppercase{Machine Learning in the Vallecas Project}}
		\HRule{2pt} \\ [0.5cm]
		\normalsize \today \vspace*{5\baselineskip}}

\date{}

\author{
		Jaime G\'omez-Ram\'irez, Marina \'Avila and Miguel \'Angel Fern\'andez-Bl\'azquez   \\
		Fundaci\'on Reina Sof\'ia \\
		Centre for Research in Neurodegenarative Diseases }

\maketitle
\tableofcontents
\newpage

%-------------------------------------------------------------------------------
% Section title formatting
\sectionfont{\scshape}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
% BODY
%-------------------------------------------------------------------------------

\section*{Abstract}

We utilize Machine Learning techniques, in particular, Deep Networks to perform predictive analytics on The Vallecas Project dataset that covers five years of data collection.


\section{Introduction}
The Vallecas Project for early detection of AD is the most ambitious population-based study in Spain. The project is carried out in the Queen Sofia Foundation Alzheimer Center by a multidisciplinary team of researchers from the CIEN Foundation. The main objective of the Vallecas Project is to elucidate, through tracking of progression of the cohort, the best combination of  parameters, clinical and others \footnote{Note in the inception of the project only clinical parameters were considered, here we advocate for a more inclusive set of parameters including neuropsychological, and also features related to the lifestyle such as nutrition and physical exercise} , that are informative about the medium and long-term features that distinguish individuals who will develop future cognitive impairment from those who will not. Thus, it intends to identify various markers to eventually determine the potential risk that each individual could have to develop the disease in the future.
There are four branches in Machine Learning:
\begin{itemize}
\item Supervised learning: map between input data to known targets (labels, annotations) 
\item Unsupervised learning: finding interesting transformations of input data (without the help of any target) for the purpose of compression, denoising, visualization or to better understand the correlations in the input data. UL is the bread and butter of Data analytics and it is often advisable to perform UL (eg. clustering, dimensionality reduction PCA) before doing SL. 
\item Self supervised learning: SL without humans in the loop, labels are generated from the input data (eg. autoencoders)
\item Reinforcement learning: model agent-environment interactions the goal is to learn actions that maximize some reward. for example,a network that looks at a video game screen and outputs the game actions that will maximize the score (Atari games)
\end{itemize}
In machine learning, the goal is to achieve models that generalize —that perform well on never-before-seen data—and overfitting is the central obstacle. Evaluating a model always boils down to splitting the available data into three sets: training, validation, and test. You train on the training data and evaluate your model on the validation data. Once your model is ready for prime time, you test it one final time on the test data. 
But why not just two sets: training and testing? Because adjusting the optimal configuration of the model (parameter and hyperparameters) is a feedback loop, the performance of the model on the validation set is the feedback signal.

%chollet book pg216
% why machine learning
Machine learning is in essence a form of applied statistics where the emphasis is placed on estimate complicated functions rather than providing confidence intervals around those functions \footnote{the p-hacking crisis is absolutely irrelevant for scientists working on ML and a symptom of the obsolescence of that approach}. Machine learning is not only interesting from an engineering point of view (fabricating a system with a desired behavior) but also from the standpoint of psychology because principle-based approach of machine learning necessarily informs and is informed by the principles that underlie human intelligence.
The classification problem we are trying to solve here is to build a function $f:R^n \to \{0,1\}$, y=f(x), the input x has n dimensions it follows that we can obtain $2^n$ different classification functions  but we only need to learn one, the optimal describing the joint probability distribution.

What separates machine learning from optimization is that optimization business is about reducing the training error  while in machine business we do that and also reduce the generalization or test error (expected error on a new input) as well. For example, in the simplest case of machine learning, linear regression, we train the model by minimizing the training error -train and test tests are identically distributed, that is, drawn from the same probability distribution. Thus, if we have a probability distribution p(X,y) and we sample it repeatedly to obtain the training set and the test set, the expected error for either sets should be the same because both expectations are built using identical sampling process.

No free lunch theorem in search an optimization \cite{wolpert1997no} (No ML algorithm is universally any better than any other) it follows that the goal of ML is not to seek an universal earning algorithm, instead the goal must be to understand what kinds of distributions are relevant to the real world that an AI agent is experiencing. Thus, we must develop an algorithm to perform well in a particular task and we do by adding preference (regularization) in the algorithm which if they are aligned with the problem at hand will perform optimally.


\begin{equation}
min \frac{1}{m^{(train)}} ||X^{(train)}w - y^{(train)}||
\end{equation}

but we actually care about minimizing the test error:
\begin{equation}
min \frac{1}{m^{(test)}} ||X^{(test)}w - y^{(test)}||
\end{equation}
But how is it possible to affect performance in the test set when we get only observations on the training set? If training and test sets were collected arbitrarily the whole enterprise of statistical learning would not stand, the reason why the field exists is because exists the assumption that both the trainign and test sets are generated by the same data generative process.


\section{CRISP-DM model}
%James Wu, Foundations of predictive analytics
The voluminous and complex (heterogeneity and dimensionality) dataset collected in the Vallecas Project make it particularly to build analytics of any kind, let alone predictive analytics.
In order to try to cope with this difficulty, we use a methodology well suited for machine learning analytics called CRISP-DM model (CRoss Industry Standard Process for Data Mining).
The CRISP-DM model consists of 5 phases:
\begin{enumerate}
%1. Problem understanding
\item
\item Definition of the problem. The data set is contains a number of features and we want to clarify the relationships between those inputs and most importantly which if any and in which measure have predictive power about conversion to MCI and dementia.
\item Assessment of scenarios for analysis. The main resource available are spreadsheets that contain demographic e.g age, school years etc. genetic: APOE, cognitive performance metrics from neuropsychological tests together with features related to life style  including nutrition and physical exercise among others. MRI and fMRI are also available but need ti be integrated with the above mentioned features, in this we will not deal with neuroimaging data.
%2. Data Understanding
\item Data understanding:
\item Data understanding
\item Data collection
\item Data description: format, volume, description of attributes \footnote{Here we will refer indistinctly features, attributes, inputs and dimensions}
\item Exploratory data analysis (EDA): charts, plots to visualize data features find associations and correlations. This task comprises: explore and visualize data, select attributes (most important, remove redundant or dependent attributes), test hypothesis about correlations and associations.
\item Data quality analysis: Deal with missing values, inconsistent values
%3. Data preparation
\item Clean, wrangle and curate the dataset before the learning machinery is launched to build models. The most time consuming $(60\%) of the time$
\item Data integration : multiple. datasets, this will have to be taken care of when we integrate with the imaging dataset.
\item Data wrangling: handle missing values (remove rows, handle missing values), formatting into csv, json etc.
% 4. Modelling
% 5. Evaluation
% 6. Deployment

 \end{enumerate}
In the next section we describe the work done in point 3, \emph{Data preparation}

\subsection{Data preparation}
% Transformation including Scaling and discretize continuous variables (binning)
Most techniques are sensitive to scaling \cite{wu2012foundations} and this is because there used to be an implicit metric or definition of nearness in the dataset. The most common scaling is z-scaling which is easily performed as $\prime{x}_i=\frac{x_i -\mu}{\sigma_i}$, that is, the set of variables $X = x_i, i=1..n$ are scaled to be centered and have the same spread \footnote{Note that we may want to use a lognormal rather than normal as a reasonable distribution. The Benford law is pertinent here: in many naturally occurring collections of numbers, the leading significant digit is likely to be small. This law is reminiscent of Pareto or Zipf law of numbers. For such variables perform a log transform, $x'_i = \log x_i$}.
z-scale and log transform are example of linear transformations but we may need nonlinear transformations, for example discretize a continuous variable. For example we can discretize the age variable in bins of 3 or 5 years. Although machine learning techniques are perfectly able to deal with continuous variable the reason we still may be a good idea to discretize is that in doing so we will be helping the model to learn the relationship between inputs and output, binning can also provide statistical smoothing and robustness in the modeling process (The cutpoint of the variable can be calculated with Gini index, entropy, chi squared or KS criteria).

Praxis: always encode your data as best as possible to reduce the work of the model as much as possible.

% Variable Selection:
Variable Selection consists of filtering the most important features and remove those that are not needed - Feature selection and wrapper.
Note that it could be always possible to select all the features and let the model do the job of finding the best predictive model, but this is not a good idea in practical terms and the reason is \emph{stability}. Remove variables that have a spurious or non consistent relationship with the target, also the principle of parsimony applies: careful variable selection and elimination.
%Detection of Multicollinearity
Another important issue if multicollinearity, we need to identify and remove correlated variables (crucial for model building). We can do this in two steps, first remove variables that produce another one, for example if A = B + C + D we can remove B, C and D.
In a second step we deal with pair wise collinearity which is not exactly multicollinearity. For that we calculate the correlation between all pairs (build correlation matrix) and remove those with a large value (above some threshold). To do this systematically we can build a graph, in the node note the correlation with the target and in the edges the correlation value with the adjacent components. We will select variables with largest corr with target and remove its correlated variables. (page 211, \cite{wu2012foundations}).
%(Belsey's 80) approach is based on covariance matrix of the least squares coefficient vector. Decaying importance of the variables as ordered by the eigenvalues

% Missing data imputation

%CODE
%tensorflow/production/descriptive_stats.py
\subsection{Feature engineering}
Feature engineering is the process of transforming the gathered data into features that better represent the problem that we are trying to solve to the model, to improve its performance and accuracy. In feature engineering we create additional features for example by combining features to feed into the model. The rationale of feature engineering is to bring into the modeling process the domain expertise, it may also help with overfitting. 
There is no a formal method to deal with feature engineering but it is one of the most important processes and if not done properly it will hamper success. It is worth noting that success in the predictive model built depends on three things: the model selection, , the dataset available and the engineered features. With well engineered features, it is possible to choose suboptimal models, and or suboptimal parameters and still obtain good results. Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data. Feature engineering is manually designing what the input x’s should be. Feature engineering asks: what is the best representation of the sample data to learn a solution to your problem?
%It is an art like engineering is an art, like programming is an art, like medicine is an art.

Feature engineering is what makes us distinguish attributes from features, attributes is just any column in our tabular dataset,a feature on the other hand is an attribute that is useful or meaningful to your problem
%Feature importance scores:A feature may be important if it is highly correlated with the dependent variable (the thing being predicted). Correlation coefficients and other univariate (each attribute is considered independently) methods are common methods
Complex predictive modeling algorithms perform feature importance and selection internally while constructing their mode Random Forest and Gradient Boosted Machines.

Feature selection addresses automatically selecting a subset that are most useful to the problem. Feature selection algorithms may use a scoring method to rank and choose features, such as correlation or other feature importance methods.Regularization methods like LASSO and ridge regression may also be considered algorithms with feature selection baked in, as they actively seek to remove or discount the contribution of features as part of the model building process

The group the features that belong to the same category reducing the dimensionality from 115 features to 20 features.
the resulting set of features are : 
\begin{center}
\textit{sex, education (nivel educativo),apoe, edad visita, scd (aggregate of 9 features related to subjective cognitive complaints), cognitive complaints (aggregate of 15 features), psychiatric syndromes (aggregate of 5 features), cognitive performance (aggregate of 11 features), quality of life (aggregate of 10 features), social engagement (aggregate of 4 features), physical exercise (aggregate of 2 features), diet(aggregate of 14 features), intellectual activity (aggregate of 14 features), demographics (aggregate of 6 features: married/singles, sons, perceived socioeconomic status), professional life(aggregate of 3 features), health (aggregate of 15 features: smoker, cardiac,lipids, glucose,diabetes, ictus, heart.. ), psychiatric history(aggregate of 15 features: depression and anxiety), sleep (aggregate of 12 features), family dementia history (aggregate of 4 features), sensory disturbances (auditive and visual disturbance) } 

\end{center}

We can also "test" using clustering (K-means, hierarchical clustering) our domain specific partition. Clustering is associated with unsupervised learning but it could be useful to exploit out domain specific knowledge tin order to obtain more discriminant features (distributional clustering) \cite{guyon2003introduction}. Distributional clustering is related to information bottleneck \cite{tishby2015deep} searches for the solution that achieves the largest possible compression while retaining the essential information about the target.
Another method of feature construction is single value decomposition (SVD) an unsupervised method of feature construction, the goal of SVD is to form a set of features that are liner combinations of the original variables which provide the best reconstruction in the least square sense \cite{duda2012pattern}.  

Supervised methods: Filters: maximize the mutual information between the features and the target

\subsection{Data leakage}
Data leakage describes the situation in which data you are including to train the machine learning algorithm includes the very thing you are trying to predict, for example if I am trying to predict conversion and I include conversion in a given year as a feature, another example of data leakage is having test data in the training data set this will lead to overfitting. Data leakage can happen in ways more subtle than those just described, for example in time series using features from the future not available for the current prediction using. To eliminate leakage, before building the  model look for features highly correlated with the target, after building the model look for surprising feature behavior (large information gains). The proof that there is leakage is obtained by comparing the deployment performance versus the train and evaluation performance. 

The features to be removed are obviously the target variable conversion, tiempo (time to convert), tpo1.1..5 (time from year 1 to conversion), dx visita1
%Dummy features to remove: id, fecha nacimiento, fecha_visita

\section{Descriptive statistics}

\section{Data visualization (dimensionality reduction-unsupervised learning-manifold learning)}
%https://towardsdatascience.com/reducing-dimensionality-from-dimensionality-reduction-techniques-f658aec24dfe
\subsection{PCA}
\subsection{Singular Value Decomposition}
TruncatedSVD is very similar to PCA, but differs in that it works on sample matrices X directly instead of their covariance matrices. Note that when the columnwise (per-feature) means of X are subtracted from the feature values, truncated SVD on the resulting matrix is equivalent to PCASVD deals more efficiently with sparse matrices than PCA (this is because it does not center the data before doing SVD) \cite{halko2009finding}. When truncated SVD is applied to term-document matrices, this transformation is known as latent semantic analysis (LSA), because it transforms such matrices to a “semantic” space of low dimensionality (combat the effects of synonymy and polysemy which cause term-document matrices to be overly sparse and exhibit poor similarity under measures such as cosine similarity).
Mathematically, truncated \footnote{Truncated means just that the algorithm will return matrices with the specified number of columns (k). This is precisely how the dimensionality is reduced.} SVD applied to training samples X produces a low-rank approximation:
\begin{equation}
X \sim X_k = U \sum V
\end{equation}


\subsection{tSNE: stochastic neighbor embedding}
computers can deal with as many dimensions as they like, humans on the other hand we are limited to 3. To visualize large (larger than 3) dimensionality datasets we need to reduce the dimensionality. The rationale is that despite the apparent large dimensionality the intrinsic dimensionality is low, or at least lower. For example a 10 mega pixels video camera rotating to film some scene build a 10 million dimensional space and yet the images approximately lie in a 3D space (yaw, pitch, roll), this embedding is certainly complex and nonlinear.
Manifold learning also called nonlinear dimensional reduction is an unsupervised method that deals with discovering the hidden simpler structure in a high dimensional dataset.
A popular algorithm is t-distributed stochastic neighbor embedding (t-SNE) \cite{maaten2008visualizing}.
Let us briefly explain the algorithm, first some definitions:
A \textbf{data point} is a point $x_i$ in the original space $\mathbf{R}^D$, where D is the number of dimension (features). A map point is a point $y_i$ in the map space $y \in \mathbf{R}^2$ (if we want to map the original space in 2D). The map space will contain the final representation of the original data space (b:X -> Y , where b is a isomorphic from the original space (X) to the Map space (Y), that is, for every map point y there is one point x. 
Now, how this map is built? we want to preserve the structure of the original data, that is, if two points are close together in X their images in Y must be closed together as well. We need to define a distance that quantifies closeness, for example, the Euclidean distance. Thus, 
\begin{equation}
p_{j|i} = \frac{\exp\left(-\left| x_i - x_j\right|^2 \big/ 2\sigma_i^2\right)}{\displaystyle\sum_{k \neq i} \exp\left(-\left| x_i - x_k\right|^2 \big/ 2\sigma_i^2\right)}
\end{equation}
which measures how close $x_i$ is from $x_j$ if we assume a Gaussian distribution around $x_i$. Note that the variance $\sigma$ is different for every point, it is chosen such as in dense areas is smaller than in sparse areas (closed by need to be really close by to be friends). 
Finally, the similarity $p_{ij}$ needs to be a symmetric measure of the conditional probability, then:
\(p_{ij} = \frac{p_{j|i} + p_{i|j}}{2N}\)
with the similarty we obrain the similarity matrix for the original dataset (the similarity matrix can be calcualted for siumplicity with constant $\sigma$ or using a different $\sigma_i$ per point i).
Bellow we show the distance (Euclidean) matrix and the similarity matrix both for constant and variable $\sigma$.

%images here: https://github.com/oreillymedia/t-SNE-tutorial/blob/master/images/similarity.png
Next, we need to compute the similarity matrix for the map points,
\begin{equation}
q_{ij} = \frac{f(\left| x_i - x_j\right|)}{\displaystyle\sum_{k \neq i} f(\left| x_i - x_k\right|)} \quad \textrm{with} \quad f(z) = \frac{1}{1+z^2}
\end{equation}
note that q is built using a Cauchy distribution\footnote{Cauchy is "pathological" distribution since both its expected value and its variance are undefined. This choice of the distribution for the map points obeys to the fact that the volume of the N dimensional ball of radius r scales at $r^N$ which means that when N is large if we pick random points uniformly in the ball, most points will be closed to the surface, that is, a N dimensional orange will have most of the points in the skin rather than in the pulp} 
%https://github.com/oreillymedia/t-SNE-tutorial see similation of distance from origin 
(t-student with one degree of freedom) rather than a Gaussian distribution as in p, another difference is that the similarity matrix p is fixed and the similarity matrix q depends on the map points. 



When reducing the dimensionality of a dataset, if we used the same Gaussian distribution for the data points and the map points, we would get an imbalance in the distribution of the distances of a point's neighbors. This is because the distribution of the distances is so different between a high-dimensional space and a low-dimensional space. Yet, the algorithm tries to reproduce the same distances in the two spaces. This imbalance is actually what happens in the original SNE algorithm \cite{hinton2003stochastic}.
The t-SNE algorithm works around this problem by using a Cauchy(0,1) distribution for the map points, since this distribution has a much heavier tail than the Gaussian distribution it compensates the original imbalance. For a given similarity between two data points, the two corresponding map points will need to be much further apart in order for their similarity to match the data similarity. So Cauchy distribution leads to more effective data visualizations, where clusters of points are more distinctly separated.

What we want is that the matrices p and q be as similar as possible, that is, similar data points yield similar map points.
The Kullback-Leiber divergence measures the distance between the two matrices p and q. 
\begin{equation}
KL(P||Q) = \sum_{i, j} p_{ij} , \log \frac{p_{ij}}{q_{ij}}
\end{equation}
to minimize the score we perform gradient descent:
\(\frac{\partial , KL(P || Q)}{\partial y_i} = 4 \sum_j (p_{ij} - q_{ij}) g\left( \left| x_i - x_j\right| \right) u_{ij} \quad \textrm{where} , g(z) = \frac{z}{1+z^2}.\)
we try to approximate the true distribution P using Q, KL(P|Q) = H(P,Q) - H(P), the cross entropy minus the entropy.
So, KL-divergence is better not to be interpreted as a "distance measure" between distributions, but rather as a measure of entropy increase due to the use of an approximation to the true distribution rather than the true distribution itself. (if we knew the true distribution P of the random variable, we could construct a code with average description length H(P). If, instead, we used the code for a distribution Q, we would need H(P)+K(P||Q) bits on the average to describe the random variable) so you need more bits to describe the situation if youa re going to use Q while the true distribution is P, so what KL measures is the inefficiency caused by the approximation of Q to P.
%https://stats.stackexchange.com/questions/111445/analysis-of-kullback-leibler-divergence

\subsection{Topological Data Analysis}
%https://www.quora.com/Is-there-a-relationship-between-manifold-learning-and-topological-data-analysis
TDA focuses on identifying global features within the data.
Manifold learning focuses on reducing dimensionality by finding a good map from the data to a low-dimensional manifold. Topological data analysis could be helpful to analyse (manifold) data before and after dimensionality reduction (i.e. manifold learning). The topology of the manifold should be the same and independent of the dimensionality of the ambient space.

Manifold learning assumes that the data lies on a (usually low) dimensional submanifold and has as a goal to create a representation that faithfully records this local neighborhood structure given in the manifold. 
TDA, on the other hand, makes very few assumptions about the data and the goal is not to faithfully reconstruct the data - or to fit the data to a model - but to provide (unbiased) summaries of the geometric/topological structure in the data.
Persistent homology and mapper both take as inputs some kind of "space" (e.g simplicial complex) and both use a function to create a summary. For Mapper the summary is (loosely speaking) a recording of the connectivity of the fibers of the  function while for persistent homology it's the relationship between the homology of sublevel sets of the function. The output is a "barcode" for persistent homology or a simplicial complex in the case of mapper.The main point here is that to create such a summary very little is needed beyond some notion of similarity or proximity in your data.

As we saw in the results (no separation of data points!), the manifold assumption (locally uniform and smooth) is quite strong and outside of scientific contexts have not found data sets that look like manifolds.

%https://jsseely.github.io/notes/TDA/
TDA involves ‘fitting’ a topological space to data, then perhaps computing topological invariants of that space. TDA is related to two familiar problems: clustering and manifold learning. In some sense, TDA is a generalization of both problems.
We can view many data analysis problems as ‘fitting a space to data’. E.g. both PCA and linear regression involve fitting a linear subspace to data; the space perspective complements the statistical (least-squares) and algebraic (SVD, pseudoinverse) perspectives of these techniques.
How do we generalize these linear techniques? A stock answer: manifold learning. But, manifolds are restrictive objects. A consequence of being locally Euclidean is that they cannot contain singularities and must have the same dimension everywhere. A mathematician’s motivation for working with manifolds is that one can do calculus on them. But if this is not the goal, then there may be little reason to assume a restrictive type of space in a data analysis context. Indeed, the singularities of a space are often the interesting points of study (e.g. bifurcation points), and one wants tools to capture these.
%Simplicial complexes
Topological spaces are a natural thing to turn to whenever one wants to ‘fit a space to data’. Yet topological spaces, without restriction, are too general. Simplicial complexes, which can be viewed as a convenient middle ground of specificity and generality in modeling spaces. Simplicial complexes lie somewhere between graphs and hypergraphs. All graphs are special kinds of simplicial complexes, and all simplicial complexes are special kinds of hypergraphs. “Most” topological spaces of interest can be discretized (triangulated) and represented as a simplicial complex.
%Persistent homology
The workhorse technique of TDA is persistent homology
%Mapper
An especially underused TDA technique is “mapper”. I found this particularly useful for visualization, and I wonder why it isn’t as widely applied as, say, t-SNE. Mapper fits a simplicial complex to data (often, just a graph), but in a very flexible way. The goal of mapper is either data visualization or clustering. The key insight offered by this technique is that many interesting “clusters” in real data are not clusters in the classical sense (as disconnected components), but are the branches of some single connected component. 
We didnt find clusters with SVD nor tSNE, can we find some with Mapper??


\section{Methods}
\label{se:me}
The most common learning method is called regression, it is then worth spend some time to explain this technique prior to get into more brave waters. 
%https://www.evernote.com/shard/s263/sh/574750b2-7b4a-4300-8c41-7e8e5150de18/f2c4ffea344f3dd11b3e2f5dcdd25633
There are two types of regression, linear regression and logistic regression. Logistic regression gives you a discrte ouput e.g.0,1 while linear regression gives a continuous output. Logistic and Linear Regression can be used as a benchmark (baseline performance) against which more complex techniques can be compared. 
Both linear regression and logistic regression does work better when you remove attributes that are unrelated to the output variable as well as attributes that are very similar (correlated) to each other. Thus, feature engineering plays an important role in regards to the performance of this technique and it is not exception with other methodologies explored here. 
Linear regression predicts a continuous output, $\hat{y}$ as a function of the input variables $x$ each weighted by a coefficient $\hat{w}$ plus a bias term $\hat{w}$. Note that the terms $\hat{y}$, $\hat{w}$ and $\hat{w}$ are estimates that can be estimated via different methods.

The equation of a linear regression is
\begin{equation}
\hat{y} = \sum_i \hat{w_i}x_i
\end{equation}
Logistic regression takes this a step further by running the output  of the linear combination of w and x through a non linear function (the sigmoid or logistic function). The logistic function range is $[0,1]$ so it can be used to estimate the probability that a given instance belong to one class or another.
\begin{equation}
\hat{y} = \sigma(\sum_i \hat{w_i}x_i)
\end{equation}

Maximum Likelihood Estimate (MLE) is a general method to estimate parameters in statistical models including regression. In MLE we maximize the probability that a random input point will be classified correctly. In order to maximize the likelihood we use optimization processes\footnote{Formally, any operation in which you are solving for the minimum or maximum of some function can be interpreted as an optimization. What optimization technique does is to looking for the linear model that provides the "best" fit to the data. For example, OLS minimizes the model error, in this case the residuals which are the part of the data that aren't explained by the model. OLS seeks to give the best description of the data, by minimizing the "total amount" of unexplained variation in the data.} e.g. ordinary least squares (OLS), Ridge regression or Lasso regression, Newton, gradient descent etc. 
%OLS works by defining the "best" model as the one that minimizes a certain measure of model error -- in this case, the sum of the squares of the model residuals. The residuals are the part of the data that aren't explained by the model: OLS seeks to give the best description of the data, by minimizing the "total amount" of unexplained variation in the data.

\subsection{Linear regression}
Conversion and psychological test scores are not continuous but discrete variables so it does not make sense of performing a linear regression.

There are teo main ways to train a linear regression model (the simplest machine learning method): "closed-form" directly computes the model parameters that best fit the model to the training set \textbf{Normal Equation} and using "gradient-descent" an iterative optimization approach that gradually tweaks the parameters to minimize the cost function over the training set. (eventually giving the same results as in the closed form). Variations of Gradient Descent are Batch GD, Mini batch GD and Stochastic GD.

Polynomial regression, contrary to linear regression can fit nonlinear datasets. It has more parameters than linear regression and it is therefore more prone to overfitting the training data (Learning curve is the graphical device we have to detect if the model if overfiting), regularization s what deals with overfitting phenomenon.

A linear regression model is something like this
\begin{equation}
y = \theta_0 + \theta_{1}x
\end{equation}
for example, $happiness = \theta_0 + \theta_1 wealth$., that is, the model is just a lienar fucntion of the input feature $wealth$ amd $\theta_0, \theta_{1}$ are the model parameters. A linear makes a prediction $\hat{y}$ based on the weighted sum of the input features pus a bias term(or intercept), $\hat{y} = \theta_0 + \theta_1 x_1 + ...+ \theta_n x_n$. This can be written in a more compact form as
\begin{equation}
y = \Theta^{T} X 
\end{equation}
where $\Theta$ is the model's parameter vector ( a column so we need to transpose y have a row ($\Theta^{T} (1 \times n), X (n \times 1)$ to get an scalar $(1 \times 1)$ which is the actual prediction for that input instance ), X is the instance vector (note that $x_0$ is always 1).
Once we have built the model we need to train it which in essence consists in setting its parameters so that the model best fits the training set. Thus, we need to find a measure that tells us whether the model fits well the data, the most common measure is the root mean square error (RSME) but we use the mean square error (MSE) because the value that minimizes a function also minimizes square root.
The MSE os a linear regression hypothesis on a training set X is

\begin{equation}
MSE(X,\Theta) = \frac{1}{m} \sum_{i=1}^{m}(\hat{y}^{i} - y^{i})^2 = \frac{1}{m} \sum_{i=1}^{m}(\Theta^Tx^{i} - y^{i})^2
\end{equation}
T find the value of the vector $\Theta$ we can use the closed-form also called normal equation
\begin{equation}
\hat{\Theta} = (X^{T}X)^{-1}X^{T}y
\end{equation}
where $\hat{\Theta}$ is the value that minimizes the cost function and y is the vector of target values.

Fortunately the MSE cost function for a LR happens to be convex (choosing two points, the line segment never crosses the curve) that is, no local minima just one global minimum and it is also continuous and smooth function, thats to this: “Gradient Descent is guaranteed to approach arbitrarily close the global minimum (if you wait long enough and if the learning rate is not too high).”

Importantly, when using Gradient Descent, you should ensure that all features have a similar scale (e.g., using Scikit-Learn’s StandardScaler class), or else it will take much longer to converge. (see pag 202 Aurelien book). The search is in the parameter space, so the more parameters the longer (finding a needle in a 300 dimensional haystack s much harder than in a 3 dimensional one, but the haystack is convex, so we know the needle sis always at the bottom)!


To calculate the gradient descent we need to do the partial derivative of the cost function for each model parameter $\Theta$ in order to see how the cost function will change if a I change the parameter just a little bit(what is the slope of the mountain if I change $\Theta_i$ eg direction east and so on for each dimension ($\frac{\partial}{\partial \theta_j} MSE(\Theta$). Once we have the gradient vector (which points uphill) just subtract the gradient (weighted by the learning rate) from $\theta$ to go downhill

The main problem with Batch GD is that it uses the entire training set to compute the gradient at every step, which makes it slow when the training set is large. Stochastic GD picks a subset (a random instance) in the training set at every step and computes the gradient for that instance, this makes the algorithm faster, with the inconvenient that its randomness (stochasticity) makes it less regular than GD, the cost function rather than slowly going to the minimum it may bounce up and down, but still decreasing on the average. However, randomness is actually good to escape from local optima but it could never settle in the global minimum.
\textbf{simulated annealing} can solve this problem playing with the learning rate, it starts with a large one, to avoid local minimum, to make shorter steps as the algorithm approaches the global minimum (learning schedule).
Each round of iterations is called epoch so in SGD the algorithm goes through the training set epochs times. Note that since instances are picked randomly per each epoch some instances can be chosen more than once while others none.
%SGDRegressor “To perform Linear Regression using SGD with Scikit-Lear”

Mini batch GD: “at each step, instead of computing the gradients based on the full training set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-batch GD computes the gradients on small random sets of instances called mini-batches”. Better than SGD is we use GPU, the algorithm is less erratic than SGD especially with fairly large mini-batches

Importantly these 3 algorithms will yield the same predictions choosing one or another relies upon the use computational resources we wish.


\subsection{Over and underfitting}
If a model performs well on the training data but generalizes poorly according to the cross-validation metrics, then your model is overfitting. It it performs badly in both is underfitting or what is the same the model is too complex or too simple. Another way to look at model's behavior is using learning curves: plots performance in training and validation set as a function of the training data size. To generate the plots, simply train the model several times on different sized subsets of the training set.
Undefitting: both training and validation set reach a plateau, i.e. throwing more data doesn't translate into better performance.
Thus, underfitting is easily detected when adding more training example doesn't improve the performance, what is needed is a more complex model or come up with better features.

The hallmark of overfitting is the gap between curves -the model performs better in the training set- however if we use a largerr dataset the curves can eventually meet.

The hallmark of undefiting is plateau.


\subsection{Regularization}
The bias/variance tradeoff: Increasing a model’s complexity will typically increase its variance and reduce its bias. Conversely, reducing a model’s complexity increases its bias and reduces its variance. To reduce the overfitting we can use regularization, for example, in a polynomial model just reduce the number of polynomial degrees.
For a linear model, regularization is typically achieved by constraining the weights of the model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net, which implement three different ways to constrain the weights.”
Always scale the inputs if using regularization.


Ridge or Tikhonov regularization is a regularized version of linear regression, a term $\alpha \sum_{i=1}^{n}(\theta_i)^2$ (the hyperparameter $\alpha$ tells how much regularization we want, if 0 then is identical to linear regression. Note also that the bias term $\theta_0$ is not regularized) is added to the cost function, the goal is not only to fit the data but keeping the model weights as small as possible. Note that the regularization term is only used during training, once the model is trained the model's performance is done using unregularized cost measure.
As with linear regression we can compute Ridge using the closed form or gradient descent.
\begin{equation}
\hat{\Theta} = (X^{T}X + \alpha A)^{-1}X^{T}y
\end{equation}

Lasso uses $l_1$ instead of the square of the $l_2$ norm as in ridge as a regularization term in the cost function. Lasso tends to eliminate entirely the coefficients of the least important features, that is, Lasso Regression automatically performs feature selection and outputs a sparse model (i.e., with few nonzero feature weights).
%Scikit-Learn example using the Lasso class. Note that you could instead use an SGDRegressor(penalty="l1")”

elastic net is a middle ground between Ridge $(r=0)$ and Lasso $(r=1)$. “It is almost always preferable to have at least a little bit of regularization, so generally you should avoid plain Linear Regression. Ridge is a good default, but if you suspect that only a few features are actually useful, you should prefer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to zero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated.”
%“from sklearn.linear_model import ElasticNet”

\subsection{Early stopping}
An alternative approach to deal with overfitting is early stopping: stops trainign as soon as the validation error reaches a minimum. In the learning curve is the validation set reaches a minimum and then rises , this means that the model has started to overfit the data. With early stopping you just stop training as soon as the validation error reaches the minimum. It is such a simple and efficient regularization technique that Geoffrey Hinton called it a “beautiful free lunch.”
%“ warm_start=True, when the fit() method is called, it just continues training where it left off instead of restarting from scratch”

\subsection{Logistic regression}
Logit just like Linear Regression computes a weighted sum of the input features (plus a bias term) but instead of outputting directly the results it returns the logistic or the sigmoid of the result.
\begin{equation}
\hat{p} = h_{\theta}(x) = \sigma(\Theta^{T} X)
\end{equation}
once the logit has estimated the probability $\hat{p}$ that the input x belongs to the positive class it can easily make the prediction $\hat{y} = 0 if \hat{p} < 0.5, \hat{y} = 1 otherwise$. We train a logit model with the log loss function, however there is no known closed function for the log loss, that is, there is no Normal Equation as it is the case of the MSE and the regularizations cost functions in which the $\Theta$ that minimizes the cost can be calculated. However, the good news is that the log loss fucntion is convex, so it is guarantee that stichastic descent (or any other optimization algorithm it will find a global minimum given that the learning rate is not too large is we have time to wait.
“Just like the other linear models, Logistic Regression models can be regularized using $l_1$ or $l_2$ penalties. Scitkit-Learn actually adds an $l_2$ penalty by default”


\subsection{Multinomial Logistic regression or Softmax}

Logit can be generalized to support multiple classes. When given an instance x, the Softmax Regression model first computes a score $s_{k}(x)$ for each class k, then estimates the probability of each class by applying the softmax function (also called the normalized exponential) to the scores. $\hat{y}= argmax_{k} \sigma(s(x))_k$. The cost function in Softmax is cross entropy (for k=2 the cross entropy is the same as the log loss). Cross entropy measures the average number of bits you actually send per option. If your assumption is perfect, cross entropy will just be equal to the entropy (i.e., its intrinsic unpredictability). But if your assumptions are wrong , cross entropy will be greater by an amount called the Kullback–Leibler divergence. The cross entropy between two probability distributions is $ H(p,q)= -p(x)\log q(x)$

%“Scikit-Learn’s LogisticRegression uses one-versus-all by default when you train it on more than two classes, but you can set the multi_class hyperparameter to "multinomial" to switch it to Softmax Regression instead. You must also specify a solver that supports Softmax Regression, such as the "lbfgs" solver (see Scikit-Learn’s documentation for more details). It also applies ℓ2 regularization by default, which you can control using the hyperparameter C.”





Logistic Regression is a Machine Learning classification algorithm, the most basic, that is used to predict the probability of a categorical dependent variable. 
In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (e.g. converter) or 0 (e.g. nono converter).
Some of the assumptions of this kind of modeling are: the dependent variable needs to be binary, only "meaningful" variables should be included in the model, the independent variables should be independent from each other (this happens very rarely certainly it does not in out dataset) 

A disadvantage of logistic regression is that we can’t solve non-linear problems with logistic regression since it’s decision surface is linear. 
Logistic Regression separates the input into two "regions" by a linear boundary, one for each class. Therefore it is required that data is linearly separable. Another disadvantage of logistic regression is that it has a high reliance on a proper presentation of your data, that is, the important independent variables need to be known. Another drawback of logistic regression is its vulnerability to overfitting.

YS: Draw figure with impossibility of classifying non linear datasets. XOR


In section \ref{} we expand on these ideas defining multi layer perceptron model (MLP) and other neural network models including Deep learning.


\cite{patania2017topological}
In supervised learning model refers to the mathematical object that makes a prediction $y_i$ given $x_i$, one of the easiest incarnations is a linear combination of weighted input features, $y_i = \sum_k w_j x_{ij}$ prediction values have different interpretations depending on the task, i.e. classification, regression. For example, it can be logistic transformed to get the probability of positive class in logistic regression, and it can also be used as a ranking score when we want to rank the outputs.
%http://xgboost.readthedocs.io/en/latest/model.html
The parameters eg $w_j$ in linear model, are the undetermined part that we need to learn from data. To find the best parameters given the training data we need to measure the performance of the model given a certain set of parameters, this is the objective function. Objective functions have two terms, training loss and the regularization.
\begin{equation}
obj(w) = L(w) + R(w)
\end{equation}
where L is the training loss and R is the regularization. The training loss measures how predictive our model is on training data, for example if we use mean squared $L(w) = \sum_i \hat{y_i} - y_i)^2$ but there are many other possible loss functions for example log loss.
the regularization term, R, controls the complexity of the model which helps to avoid overfitting.


We exhaustively explore a number of models and evaluate their performance on the Vallecas Project dataset.
The appendix section provides a more in depth description.  of the models implemented, in this section we will
go through the models and give a layman's description of the models making emphasis in the applicability and pros and cons.

Learning happens in two steps, first we build an estimator and then we fit the model to the data, the model performance can be then studied. The estimator may need to specify parameters and hyperparameters, for example, decision tree needs to specify the maximum depth of the tree, other models for example Naive Bayes are less dependent on the choice of parameters. 
This easiness of parametrization does not come for free (no free lunch), Naive Bayes assumes that each feature is conditionally independent of every other feature given the target category, $p(x_i|C) == p(x_i|C), \forall i,j$ which is quite unrealistic in our dataset. Furthermore, when dealing with continuous data Naive Bayes assumes also a Gaussian or normal distribution. Thus, 
$p(x = a|C_k) = \frac{e^{-\frac{(a-\mu_k)^2}{2\sigma_k^{2}}}} {\sqrt{2\pi\sigma_k^{2}}}$. 

Once the estimator is being learned, we need to test it with the predictor. Predictors generate forecasts using the learned estimator fed with unknown  data, that is data that were not used to train the estimator.
% 0. Transformers : before learning, some are simple: replacing missing data with a contant, taking a log transform ..or some transforms are learning algorithms themselves like PCA
% The pipeline is: Read Data -> apply some simple or complex (PCA) transformation -> fit an appropriate model -> predict using the model for unseen data. (iteratively)
% For model tuning and selection we can use two meta estimators: GridSearchCV and RandomizedSearchCV to search the best parameters. Grid provides a grid of possible parameters and try each possible combination among them to arrive at the best one.. Randomized  optimizes this sampling the parameters to avoid combinatorial explosion of Grid. the also allow different x-validation schemes and score functions to measure performance.


\subsection{Model evaluation: quantifying the quality of predictions }
The two factors that tell us how well our machine learning is doing are:
\begin{itemize}
\item Underfitting: the error on the training set is too large.
\item Overfitting: the gap between the error in the training and test sets is too large. 
\end{itemize}

%http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
Whether we are doing classification both continuous and discrete or clustering we need to study the behavior of our model prediction, that is, model selection will rely on model evaluation criteria. Evaluation criteria consists in computing scoring objects that gives us information about model performance. It ought to be remarked that scoring is not dataset independent, for example, accuracy may be an optimal indicator of model performance in some situations but suboptimal in others. For example, in a binary classification problem -converter vs not converter- in which the converters are a small minority (e.g. $10\%$, accuracy is not the best measure to use because accuracy is just the number of correct prediction divided by the total number of instances. A dummy classifier that doesn't look at the features at all and always predict the most frequent class (i.e. non converter) will have an accuracy of $90\%$, that is, the dummy classifier will predict the right label for 90 out of 100 examples.
%https://www.coursera.org/learn/python-machine-learning/lecture/BE2l9/model-evaluation-selection
Dummy classifiers provide a null metric, in the results section we will compare the model prediction with dummy classifiers as a sanity check on the model's performance.
When we find that the model accuracy is close to the null accuracy baseline given by the dummy classifier where are in any of these situations:
\begin{itemize}
	\item Features are ineffective or missing
	\item Poor choice of hyperparameters or kernel in the model (eg in SVM) 
	\item Large class imbalance 
\end{itemize}
 %Scoring objects take care of that, as a rule,  higher return value better than lower return values.
 A very helpful mathematical object to study model performance is the confusion matrix. For example, for a bibary prediction task we have a $2 \times 2$ matrix where rows represent the true values and the columns the predicited values. Thus, the values in the diagonal are true predictions and off diagonal the predictions that the model got wrong, rather  than having a single number we have a matrix from which other metrics can be derived like recall, precision and $F_\beta$ metrics.
 %from sklearn.metrics import confusion_matrix
\[
M=
  \begin{bmatrix}
    TN & TF  \\
    FN & TP 
  \end{bmatrix}
\]
From the confusion matrix we can derive:
$\textit{Accuracy} = \frac{TN + TP}{TN+TP+FN+FP}$. 
Recall, also True Positive Rat, sensitivity and probability of detection  is the fraction of all positive instances does the classifier identifies correctly as positive, $\text{Recall} = \frac{TP}{TP+FN}$, in biomedical applications in which is crucial to do not miss the positives (converters) recall is a very important metric.
Precision is the fraction of positive predictions that are correct (finding sick when they are not) $\text{Recall} = \frac{TP}{TP+FP}$.
Specificity or False positive rate is the fraction of all negative instances that the classifier incorrectly identifies as positive (here the smaller the better)  $\text{specificity} = \frac{FP}{FP+TN}$

Precision and recall are complementary in the sense that you can increase the precision but at the expense of the recall and viceversa. Recall oriented machine learning tasks are biomedical (e.g. tumor detection) and precision oriented tasks are e.g. document classification. Precision (x-axis) recall (y-axis) curves, an ideal classifier would achieve ideal precision (1.0) and ideal recall (1.0) (the top right corner) 

The F1 score combines both precision and recall into a single number, mathematically based on the harmonic mean of precision and recall, $F_1 = 2\frac{\text{precision*recall}}{precision+recall} = \frac{2 * TP}{2TP + FN + FP}$. More generally we can modulate how much emphasis we do to precision versus recall using the parameter $\beta$. Thus, $F_\beta = (1+\beta^2)\frac{\text{precision*recall}}{(\beta^2 * precision)+recall} = \frac{2 * TP}{2TP + FN + FP}$ (precision oriented $\beta=0.5$, false positives hurts performance more than false negatives, recall oriented $\beta=2$, false negatives hurts performance more than false positives).
%from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
%print('Accuracy:{:.2f}'.format(accuracy_score(test, predicted)));precision_score, recall_score;f1_score
%print(classification_report(test,pred,target_names=['not converter 0', 'converter 1']))
%varying the decision threshold for predict_proba https://www.coursera.org/learn/python-machine-learning/lecture/0YPe1/classifier-decision-functions

Receiving Operating Characteristic ROC curves, false positive rate (x-axis), true positive rate (y-axis) (top left corner is the ideal point, false positive 0 and true positive 1), area under the curve AUC is a way to summarize the classifier performance in one single number. ROC AUC is a good metric for balanced-classification problems only.

Other metrics that will be shown in the Results section are:
\begin{itemize}
	\item Hamming loss: Calculates the Hamming (distance) between two sets
	\item Jaccard similarity: average of Jaccard similarity coefficients (Jaccard index)
	\item Hinge loss: computes the average distance between the predictions and the data using hinge loss (only prediction errors) Used in SVM
	\item Matthews correlation coefficient (phi coefficient): it is regarded as a balance measure useful even if the classes are of very different sizes. [-1,1] +1 is perfect prediction , 0 average random prediction and -1 inverse prediction. It is also called phi coefficient.
	%from sklearn import matthews_corrcoef, matthews_corrcoef(y_pred, y_true) # tp*tn - fp*fn/ sqrt()
	\item Zero one loss: computes the sum or average of the 0-1 classification loss $L_{0-1}$ over n samples. By default normalizes over the sample
\end{itemize}

\subsubsection{Class imbalance}
The class imbalance problem -the number of samples of one class being under represented compared to another class- is predominant in many scenarios, in particular when dealing with anomaly detection, for example, fraudulent transaction, electrical grid fault, electrical theft, conversion to disease  etc. In this situation, a predictive model that does not take into account this could be very unreliable.
The imbalance problem needs to be addressed in the design phase with an emphasis on improving the identification of the minority class as opposed to achieving higher overall accuracy.
How rare is our data set can be easily quantified with the balancing ratio, r. Let X be an imbalanced set with $X_{min}$ the subset of the undersampled class and $X_{maj}$ the subset of the majority class. Te balancing ratio r is
\begin{equation}
r_X = \frac{X_{min}}{X_{maj}}
\end{equation}

%https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/
The balancing process is equivalent to resample X in such a way that the ratio increases, $r_{X_{resampled}} > r_X$, we can do so undersampling the majority class and or over sampling the minority class. Udersampling refers to reducing the number of samples in $X_{maj}$ for example cleaning the majority space by eliminating redundant data points. Oversampling tries to achieve data balancing by generating new points in $X_{min}$.

Standard classifier algorithms e.g. Logistic Regression, Decision tres etc. have a bias towards the majority class, that is, they tend to only predict the majority class data and the features of the minority class are treated as noise and are often ignored. 


Random under sampling: eliminate randomly majority class examples, for example taking out $10\%$ samples without replacement from the majority class to combining them with the minority class. It reduces the training set discarding potentially useful information, this approach is only advisable for really large datasets.

Random over-sampling increases the number of elements in the minority class by randomly replicating. Here, contrary to udersampling, there is no information loss but it may increase the likelihood of overfitting since it replicates the minority class events.

Cluster-Based Over Sampling k-NN algorithm is independently applied to minority and majority class instances to identify clusters in the dataset. Subsequently, each cluster is oversampled such that all clusters of the same class have an equal number of instances and all classes have the same size. The disadvantage of this algorithm, like most oversampling techniques is the possibility of over-fitting the training data.  

Informed Over Sampling: Synthetic Minority Over-sampling Technique (SMOTE): This technique is good at avoiding overfitting which naturally occurs when replicas of the minority set are added. In SMOTE rather than replicas we generate a synthetic subset from the minority class which is used to train the classifier. For example we get 20 instances from $X_{min}$ and similar synthetic instances are generated 20 times for a total of 400 instances. SMOTE mitigates the problem of overfitting because it does not mimic minority instances it rather generates new ones, another advantage is that the there is no information loss.
The disadvantages of SMOTE are that the algorithm does not take into account neighboring examples across classes which could introduce noise, SMOTE is not very effective for high dimensional data (YS:how ineffective and ho high??)
SMOTE creates artificial data based on the feature similarities between existing minority samples. To create a synthetic sample, randomly select one of the K-nearest neighbors (eg under Euclidean distance), then multiiply the feature vector with a random number between 0 and 1 and finally add this vector to x
\begin{equation}
x_{new} =  x_i + \delta * (\hat{x_i} - x_i)
\end{equation}
where $x_{new}$ is the synthetic new point, $x_i$ is the minority instance and $\hat{x_i}$ is one of the k nearest neighbors. Thus, the resulting new point is along the line between $x_i$ and the randomly selected neighbor $\hat{x_i}$. The drawbacks of SMOTE include over generalization and variance \cite{wang2004imbalanced}.

YS: Tomek links
SMOTE can be complemented with data cleaning techniques, notably Tomek links \cite{tomek1976two} to reduce the overlapping (new instances become 'tied' together leading to overfitting \cite{mease2007boosted}) that is introduced in the sampling methods.
A Tomek link is a pair if minimally distanced nearest neighbors that belong to different classes. By eliminating the Tomek links (pairs of points that are two close or noise) we cleanup the undesired overlapping that the synthetic sampling may introduce. After the Tomek links are removed all the minimally distanced nearest neighbors belong to the same class. Then, if we remove the overlapping points it could be possible to establish well-defined clusters. 

We can also integrate sampling techniques with ensemble learning techniques,  (SMOTE + Adaboost), SMOTE can introduce new synthetic sampling at each boosting iteration and the successive classifier ensemble focus more on the minority class.


Modified synthetic minority oversampling technique (MSMOTE): Two approaches: Bagging (Bootstrap Aggregating) and Boosting. In bagging we generate ‘n’ different bootstrap training samples with replacement. And training the algorithm on each bootstrapped algorithm separately and then aggregating the predictions at the end. 
Boosting is an ensemble technique to combine weak learners (small changes in data induce big changes in the classification model.) to create a strong learner. 
AdaBoost (Adaptive Boosting) is the first original boosting technique which creates a highly accurate prediction rule by combining many weak and inaccurate rules. 
Gradient Tree boosting: Decision Trees are used as weak learners. Gradient Boosted trees are harder to fit than random forests
XGBoost (Extreme Gradient Boosting) is an advanced and more efficient implementation of Gradient Boosting Algorithm. It is 10 times faster than the normal Gradient Boosting as it implements parallel processing. Unlike gradient boosting which stops splitting a node as soon as it encounters a negative loss, XG Boost splits up to the maximum depth specified and prunes the tree backward and removes splits beyond which there is an only negative loss.

The pipeline we will use is bagging plus boosting. First, bagging, balance the unbalanced dataset using Synthetic Minority oversampling technique (SMOTE) by creating synthetic instances. And train the newly balanced data set using a Gradient Boosting Algorithm. Note that ensemble based methods (eg. gradient tree boosting) are not an alternative to sampling techniques per se – so it is better to use SMOTE first and then  the ensemble algorithm SMOTE+Gradient boosting. However, XG boosting can be applied directly on the imbalanced data, XG Boost is a more advanced form of Boosting and takes care of imbalanced data set by balancing it in itself- so use of sampling techniques may not be necessary.

In our dataset the imbalance is around 12:1 which is far from what is more often considered as imbalance with ratios as skewed as 100:1, 1,000:1 or 10,000:1\cite{he2009learning}.
The goal is to have high accuracy i the minority class without jeopardizing  the accuracy of the majority class.

The two main classes to deal with the imbalance problem are Sampling (random, synthetic and cluster based)and Cost Sensitive (Adaptive boosting, cost sensitive decision trees) 
Note that classifiers can also learn from imbalanced data sets, some studies have found comparable result for imbalanced datasets and the same balanced using sampling techniques \cite{japkowicz2002class}.
Cost-sensitive methods for imbalanced consists in including the cost associated with missclassifying examples, that is, instead of creating a balanced data distribution with sampling , cost sensitive penalizes learning that missclassifies wityh the cost matrix. AdaBoost introduces a cost via assigning a weight for the data instances as an updating strategy. Thresholding is another valid forms of cost-sensitive learning (adjust the threshold of predicted probabilities return by the classifier to reduce the missclassified examples).

Weighting is based on the idea that the ‘cost’ of misclassifying the minority class is worse than misclassifying the majority  class. This is applied at the algorithmic level in such algorithms as SVM, ANN, and Random Forest, for example with the weights parameter. So, the limitations here consist of whether the algorithm can deal with weights.
%https://stackoverflow.com/questions/20082674/unbalanced-classification-using-randomforestclassifier-in-sklearn
%sample_weight parameter is to balance the target classes in training dataset
%len(X) == len(y) == len(sample_wight), each element of sample 1-d array represent weight for a corresponding (observation, label) pair. For 5:1 imbalance, if 1 class is represented 5 times as 0 class is, do: sample_weight = np.array([5 if i == 0 else 1 for i in y])
% scikit-learn 0.17, there is class_weight='balanced' option which you can pass at least to some classifiers:
%The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)).


Thresholding can be used if the algorithm return probabilities, for example \textit{predict proba} adopting an appropriate trade-off level, this level can be optimized by generated a curve of the evaluation metric (e.g. F-measure). The limitation here is that you are making absolute trade-offs. Any modification in the cutoff will in turn decrease the accuracy of predicting the other class. If you have exceedingly high probabilities for the majority of your common classes (e.g. most above 0.85) you are more likely to have success with this method. It is also algorithm independent (provided the algorithm returns probabilities).
Note that the F-measures which combine precision (exactness) and recall (completeness) don’t take into account the true negative rate. Therefore, it is often recommended that in imbalanced settings to use metrics such as Cohen’s kappa metric.

Sampling (synthetic data, algorithms are SMOTE, SMOTEBoost and EasyEnsemble)



\subsection{Support Vector Machine (SVM)}
%

The idea behind SVM is to fit the widest margin street between the classes. Scaling is required because otherwise it will neglect the small features.
A support vector is an instance in the street and the decision function is entirely determined by the support vectors. Computing the predictions only involves the support vectors, you could remove the other training points they will not matter for prediction.
SVM does not return probabilities (contrary to logistic regression) but it can give you a confidence score which is the distance between the test instance and the decision boundary.



It can perform both linear and non linear classification, regression and outlier detection. What an SVM classifier does is fitting the widest possible street (represented by the parallel dashed lines) between the classes. This is called large margin classification.
Note that adding more training instances “off the street” will not affect the decision boundary at all: it is fully determined (or “supported”) by the instances located on the edge of the street. These instances (those that touch the street boundaries) are called the support vectors. Always standarize data because SVMs are sensitive to the feature scales.
“If your SVM model is overfitting, you can try regularizing it by reducing C.” large C margins are narrow and we may overfit C small (C=1) larger margins. “Unlike Logistic Regression classifiers, SVM classifiers do not output probabilities for each class”
%LinearSVC same as SVC(kernel='linear') but SVC is musch slower specially for large datasets  also possible use 
% GDClassifier(loss="hinge", alpha=1/(m*C)). This applies regular Stochastic Gradient Descent to train a SVM classifier but it doesnt converge as fast as the LinearSVC class


When the datasets are not linearly separable we can use nonlinear SVM classifiers, one tactic is to add more feartures so that it result in a linearly separable dataset (pg 259).
\textbf{Kernel trick} “ It makes it possible to get the same result as if you added many polynomial features, even with very high-degree polynomials, without actually having to add them. ”

another 'trick' to deal with nonlinearity in the dataset is adding similarity features, that is achieved using a similarity function that measures how much each instance matches a particular landmark. The Gaussian Radial function (GRF) is a similarity, it is a bell-shape function with 0 for very far from the landmark to 1 very close to it. We transform the dataset using the function $\phi_{\gamma}(x,d) = e(-\gamma ||x-d||^2)$, where $\gamma$ a hyperparameter (approximated using grid ) and d is the distance from x to the landmark (with this transform the dataset may become linearly separable). Increasing $\gamma$ makes the Bell curve narrower, which means that the decision boundary is more wiggling around individual instances. Conversely a large $$ makes the Bell curve wider and the decision boundary is smoother (instances have more influence), so $$ acts like a regularization parameter, if the model is overfitting, reduce $\gamma$ (wider) and if you underfit increase (too much regularization) $\gamma$ (narrow) or C or both. The C parameter behaves in a similar way.
%see page 263 for 1 dimensional to 2dimensional

How to create a landmark? the easiest is one landmark at each point, the problem s that we make the dateset bigger, from m points and n features to $m \times m$. Thus, compute the additional features (m-n) can become computationally expensive, however the kernel trick does the magic 
%“SVC(kernel="rbf", gamma=5, C=0.001)”

GRF is not he only kernel there are vene specialized kermnels, for example  string kernels for classifying texts or DNA sequences (Levenshtein distance). Which kernel to use? Asa rule of thumb start with linear kernel \emph{LinearSVC}(\emph{LinearSVC} is faster than \emph{SVC(kernel=linear)}, especially isf the training set is very large or has many features, if this is not the case use Gaussian Radial basis function (RBF).

In terms of complexity, \emph{LinearSVC} (\emph{liblinear}) does not support the kernel trick, but it scales almost linearly with the number of training instances and features, so its training time complexity is $O(m \times n$).
The \emph{SVC} (\emph{libsvm}) class supports the kernel trick and the training time is cuaratic or cube witht he number of  $O(m^{2} \times n$) instances. This is a great algorithm for small-medium size d¡datasets. Note also that it scales well with the number of features, especially sparse features. 

Linear SVM works by simply calculating a decision function $w^Tx +b $ (where $w$ is the parameeters vector we call to this $\Theta$ and b is the bias ($b_0$)). Thus if the result of $w_{1} x_{1} + ....+ w_{n}x_{n} + b$ is positive the predicted class $\hat{y}$ is the positive class or else is the negative class. Thus, in essence, training a linear SVM classifier means finding the value of w and b that make this margin as wide as possible while avoiding margin violations (hard margin) or limiting them (soft margin).




\subsection{Model selection}
In the previous section we sketched different evaluation metrics now we will see how to apply them in order to select the best model for our application. 
Model selection is the phase in the modeling process in which we opt for one model or other based on the model's performance which is being previously evaluated.

There are three main methods of model selection: Train-Test split, K-fold Cross Validation and Grid Search.
The train/test method split the data set into two portions testing set and test test. The training set is used to train the model and the testing set is used to test the model. The pros are flexibility and speed, the cons provides a high  variance estimate of out of sample. 

This method splits the data set into K equal partitions (“folds”), then use 1 fold as the testing set and the union of the other folds as the training set. Then the model is tested for accuracy. The process will follow the above steps K times, using different fold as the testing set each time. The average testing accuracy of the process is the testing accuracy.
%https://towardsdatascience.com/machine-learning-workflow-on-diabetes-data-part-01-573864fcc6b8
In K-fold Cross Validation the data set is divided into K equal partitions or “folds”, 1 fold is used as the testing set and the union of the other sets are the training set. The process is performed K times using different fol as the testing set each time. the average testing accuracy of the K iterations is the testing accuracy.
In Cross Validation we do not to set any parameter, apart from the number of folds (cv) from which we calculate the average performance across folds. The pros are more accurate estimate of out-of-sample accuracy, more “efficient” use of data in the sense that every observation is used for both training and testing. The cons are slow compared to Train/Test split.

In GridSearch we need a gamma parameter (for example in SVC with rbf kernel set the radius), GridSearchCV finds the gamma that optimizes the given evaluation metric (accuracy, precision etc).
%cross validation, no parameters only cv (number of folds)
%from sklearn.model_selection import cross_val_score; from sklearn.svm import SVC
%print('cross validation (accuracy)', cross_val_accuracy(clf, X,y, cv=5))
%print('cross validation (AUC)', cross_val_accuracy(clf, X,y, cv=5, scoring = 'roc_auc'))
%print('cross validation (recall)', cross_val_accuracy(clf, X,y, cv=5, scoring = 'recall'))
%not that we do not any parameter tunning just 5 folds and examine performance

%% Grid Search, we need gamma parameter
%from sklearn.model_selection import GridSearchCV
%grid_values = {'gamma': [0.001, 0.01, 0.1, 1 10]}
%grid_clf_acc = GridSearchCV(clf, param_grid = grid_values); grid_clf_acc.fit(X_train, y_train)
%#optimize for acc
% y_decision_fn_scores_acc = grid_clf_acc.decision_function(X_test); print('Grid best parameter (max. accuracy):', grid_clf_acc.best)
%print('Grid best score (accuracy):', grid_clf_acc.best_score_) 
%#optimize for auc
%grid_clf_auc = GridSearchCV(clf, param_grid = grid_values, scoring='roc_auc'); grid_clf_auc.fit(X_train, y_train)
%print('Grid best score (accuracy):', grid_clf_auc.best_score_) 
%the gamma parameter can be equal but also different depending on the metric used

The evaluation metric supported for model selection are
\begin{lstlisting}
%from sklearn.metrics.scorer import SCORERS
%sorted(list(SCORERS.keys()))
%['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'log_loss', 'mean_absolute_error', 'mean_squared_error', 'median_absolute_error', 'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']
\end{lstlisting}
The decision boundaries change when it is optimize for different metrics

Three data splits:
\begin{itemize}
	\item Training set: model building
	\item Evaluation set: model selection 
	\item Test set: final evaluation
\end{itemize}

\section{Results}
Results for each classifier. We will realize that the challenge of fitting the training data differs very drastically from the challenge of finding patterns that generalize to new data \cite{goodfellow2016deep}.
\subsection{Logistic regression}

Lasso regularization: answer two points: what is the baseline prediction of disease progression and  which independent variables are important factors for predicting disease progression.
%A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
%Coefficient of determination R^2 of the prediction of LogReg_Lasso on training set 0.29
%Coefficient of determination R^2 of the prediction of LogReg_Lasso on test set 0.11

\subsection{K-neighbors}
K-Neighbors (KNN) classifier is non-parametric \footnote{KNN makes no explicit assumptions about the functional form of the mapping $h:X->y$ this is contrary to what happens in naive Bayes where it is assumed that data follow a Gaussian distribution}, instance-based supervised learning algorithm. KNN doesn't assume normality of data (non-parametric) not it explicitly learn a model (instance-based) the algorithm rather memorizes the entire training set which are subsequently used as “knowledge” for the prediction phase, that is, KNN works like an on-demand process only when a query to our database is made will the algorithm use the training instances to spit out an answer. 
k-NN works as follows, given a training set $X_train$ with labels $y_train$ and an instance $x_test$ to be classified, first, finds the set of most similar instances to $x_test$ that are in $X_train$ and we call then $X_NN$. Next, it predicts the labels $y_NN$ for $X_NN$ and finally predicts the label for $x_test$ by combining the labels $y_NN$ using simply majority vote. 
It requires minimal training but relies upon expensive testing. KNN is used as a benchmark for more complex classifiers such as SVM and artifical network which contrary to KNN have lengthy training phase albeit a very fast testing phase. KNN can suffer from skewed class distributions, if a certain class is very frequent in the training set it might perform poorly because the majority voting 
it will tend to dominate the response. Finally, the accuracy of KNN can be severely degraded with high-dimension data because there is little difference between the nearest and farthest neighbor
%https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/
The algorithm requires the computation of the number of neighbors k, once this parameter is estimated performing cross validation on our dataset using a generated list of an odd number k neighbors (by default 5). For low k neighbor parameter, eg $k=1$ the decision boundaries will be very convoluted (high complexity, over fitting), for large k now the classifier needs to weigh the votes of many neighbors, not juts one, so single training data points will not have the dramatic influences in the prediction as in k=1 and the result is a smoother boundary (model with lower complexity, less variance), so for k equal all the number the prediction will be always the most frequent class (model underfit). The metric distance is the Minkowski distance ($p=1$ Manhattan, $p=2$ Euclidean). 
%https://www.coursera.org/learn/python-machine-learning/lecture/I1cfu/k-nearest-neighbors-classification-and-regression
One weakness of k-NN  is that it cannot learn that one feature is more discriminative than another.

\subsection{Naive Bayes Classifier}
The ideal model is an oracle that knows the true probability distribution that generates the data. But even if we had the oracle we still could incur in errors for example if there is noise in the distribution.
Also note that the mapping X,y could be inherently stochastic  or y could be deterministic but involves variables that are not included in the model. The error incurred by an oracle making predictions from the true distribution is called the Bayes error.

%https://www.evernote.com/shard/s263/nl/33458921/18793f59-d0f8-40c0-b1a5-6f51bab38c60?title=Naive%20Bayes%20Classifier
The naive Bayes \footnote{Bayes hardly deserves the glory, Laplace had not only the intuition but provided the mathematical formulation, this is not rare in science and human affair see Amerigo Vespucci giving the name of America, absolutely disproportionate} classifier determines to which class a sample belongs to by calculating the posterior probability using the Bayes formula $p(C|X)$ where X is the sample and C the target class, for example converter.
The Bayes rules combines two sources of information -prior and likelihood- to calculate the posterior which is the final classification criterion.
For example if the posterior (posterior = prior x likelihood) of X being a converter is larger than the posterior of X being a non converter then we classify x as converter ($p(C|X) > p(~C|X)$). Thus, given a subject X with the features values $f1_x, f2_x and f3_x$ (eg $f1_x=0$ (apoe), $f2_x=30$ (mmse) and $f3_x=12$ (school years)) we just need to calculate 
the posterior probability $P(converter|f1_x, f2_x, f3_x) = P(f1_x|converter) * P(f2_x|converter) * P(f3_x|converter) * P(converter)$.

The assumption is that the features (predictor, inputs) are independent, and this is why we can easily calculate the Likelihood (e.g. $P(fi_x|converter)$) as a product of conditional probabilities. Note that this assumption is quite strong it means that the presence of a feature in a class is unrelated to the presence of other feature, for example in the previous example having large mmse and the years of school are uncorrelated.
When the assumption of independence holds, a Naive Bayes classifier tends to perform better than other models such as logistic regression with the advantage that naive Bayes needs less training data to perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).
Bayes tend to be efficient in learning and prediction but poor in generalization compared with more sophisticated methods.

\subsection{Decision trees}
Decision trees makes very few assumptions about the dataset, this is very different from linear models which assume that data is linear. DT are nonparametric models, nonparametric is a misnomer, it does not mean that it does not have parameters, rather the opposite it has a lot, it means that the parameters are not determined prior to training, the model adapts to the data fitting to it (often overfitting). A parametric model (eg linear model) has a predetermined number of parameters,reducing thus, the risk of overfitting but increasing the risk of underfitting. In order to avoid overfitting in DT it is important to restrict this adaptive or sticky properties of decision trees (regularization), for example the max depth hyperparameter will reduce the risk of overfitting (by default is none, no limitation, so likely overfit).

Decision trees require very little preparation, for example they do not need to scale or centering the data, remember than in SVM scaling was critical otherwise the algorithm would neglect small features.
Decision trees are white box models, in contrast with neural networks and random forests. 
The Gini gives the impurity $G_i = 1- \sum_{k=1}^{n} p_{i,k}^2$, where $p_{i,k}$, is the ratio of instances of class k in node i. A node is pure if Gini is 0 i.e. all training instances belong to the same class . It is possible to use entropy rather than Gini, they have very similar results, Gini is a bit faster but Gini tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce more balanced trees. %criterion='entropy'
%DecisionTreeRegressor
We use the scikit learn implementation of decision trees algorithm (CART) which uses binary trees, however there are other algorithms that produce trees with more than two children, eg. ID3.
A decision tree can also estimate the probability that an instance belongs to a class (SVM does not have this feature)
% tree_clf.predict_proba

The CART algorithm is \textbf{greedy} (looks for the optimum split at top level but does not care if the split will lead to the lowest impurity several levels down, that is, it is trickle down economics, does the optimum split at the top and never thinks twice about the consequences down the road)). CART does not guarantee to find the optimal solution, it splits the training set using feature k and threshold $t_k$, the algo chooses the pair $(k,t_k)$ by looking for the split that produces the purest split weighted by the size, so the cost function that the algo minimizes has an impurity term factor by a size term for both right and left side.
$J(k,t_k) = \frac{m_{left}}{m}G_{left} + \frac{m_{right}}{m}G_{right}$
The algorithm stops recursing once it reaches the maximum depth (defined by the $max_depth$ hyperparameter), or if it cannot find a split that will reduce impurity.

“Unfortunately, finding the optimal tree is known to be an NP-Complete problem: it requires O(exp(m)) time, making the problem intractable even for fairly small training sets. This is why we must settle for a “reasonably good” solution.”

“Since each node only requires checking the value of one feature, the overall prediction complexity is just $O(\log_{2}(m))$, independent of the number of features. So predictions are very fast, even when dealing with large training sets.”
“However, the training algorithm compares all features (or less if $max_features$ is set) on all samples at each node. This results in a training complexity of O(n × m log(m)). For small training sets (less than a few thousand instances), Scikit-Learn can speed up training by presorting the data (set presort=True), but this slows down training considerably for larger training sets.”
 

DT have many bounties, do not need data preparation, simple to understand and interpret, versatile etc. but they have an inconvenient, instability, this is due to the orthogonality of the boundaries it creates, which makes them sensitive to dataset rotation.  One solution to this problem is to use PCA which often results in a better orientation of the training data. DT are very sensitive to small variations in the training data. Random Forests can limit this instability by averaging predictions over many trees.

k-NN breaks the input space into box looking regions, decision trees does this as well, each node of the DT is associated with a region in the input space, space is then divided into non-overlapping regions with a 1-1 correspondence between leafs and input regions.

Decision trees are a popular supervised learning method can be used for both regression and classification  a very good exploratory method to understand which are the features more relevant to predict the target.
In essence, decision trees learn a set of conditional rules (if-then) on features values that result in the prediction, so the idea is to find a set of rules useful to categorize an object (reminiscent to expert systems in the 70s and 80s) but rather than figure out manually the rules for every task the algorithm will learn it for us.
% if accuracy on training > acc test we are doing overfitting, one strategy is to keep addin
When the tree keeps adding rules it may become very complex we will memorize the data and will overfit \footnote{Overfitting happens in every machine-learning problem, the dialectics of Machine learning is in optimization versus generalization. Optimization refers to adjusting the model in order to get he best possible performance in the training set and generalization is about how well the trained model performs on data never seen before}, to avoid this we can prevent the growth of the tree using the max depth parameter which controls the maximum number of splitting points (eg 3) and minimum-maximum samples leaf which is the number of data instances that a leaf can have to avoid further splitting.  In practice, the max depth parameter is enough to deal with overfitting.
the pros are that are easy to understand via visualization, no need of feature normalization or scaling and work well with databases with heterogeneous data types. the cons is that they overfit (complex trees). In the next section we show ensemble methods such as Random Forests that might help to alleviate the overfitting problem found in Decision Trees.

\subsection{Ensemble Methods}
Ask to a thousand people and then aggregate their answers, in many cases the aggregate is better than the expert's response, by the same token, aggregating many predictors will give you better answers than a single one.
Don't look at the tree it may be lying... but to the forest!
A group of predictors is an ensemble, thus, the technique is called Ensemble Learning, an ensemble of decision trees or other type of predictors gives you individual predictions, then you can get the class that gets the most votes. This, in essence is a Random Forest.
Ensemble methods make sense once you have done 'individual methods' decision tree, svm, logit etc. 
Examples of ensemble methods bagging, boosting, stacking and random forests.

A majority vote is a hard voting classifier, this voting classifier often achieves a higher accuracy than the best classifier in the ensemble (svm, logit, dt etc), even if individual classifiers are weak learners (do slightly better than random guessing) the ensemble can still be a strong classifier, provided there are a sufficient number of weak learners and they are sufficiently diverse. % solo no puedes con amigos si   https://www.youtube.com/watch?v=Ds7tje_Y0CM

The reason why RF works is the \emph{Law of large numbers}. For example, for a coin that is slightly biased, let us say that has $51\%$ probability of heads and $49\%$ of tails, if you toss the coin 1,000 times chances are that you will get a majority  of heads, something close to 510 heads and 490 tails. The (frequentist) probability of heads majority for 1,000 tosses is $75\%$ and the more tosses you do the closer you get to 1, for 10,000 tosses the probability of majority is $97\%$. This is the \emph{Law of large numbers} the more you toss the coin the closer you will get to 51-49 ratio. In the same vein, you can build 1,000 classifiers that are individually correct let us say $51\%$ of the time, (barely better than random guessing) but if you predict the majority vote class you can hope for a 75% accuracy!
%for example for 3 coins np.power(0.49,3) + 3*(np.power(0.49,2)*0.55 + np.power(0.51,2)*0.49)  +np.power(0.51,3)
% p(aaa) or p(aab) = 0.51499 = 3*np.power(0.51,2)*0.49  +np.power(0.51,3)
"However, this is only true if all classifiers are perfectly independent, making uncorrelated errors, which is clearly not the case since they are trained on the same data. They are likely to make the same types of errors, so there will be many majority votes for the wrong class, reducing the ensemble’s accuracy.”Thus, “Ensemble methods work best when the predictors are as independent from one another as possible” One way to get diversity train them using very different algorithms so they will make different types of errors improving the accuracy.
%pg 279 “from sklearn.ensemble import RandomForestClassifier
%from sklearn.ensemble import VotingClassifier
%from sklearn.linear_model import LogisticRegression
%from sklearn.svm import SVC

%log_clf = LogisticRegression()
%rnd_clf = RandomForestClassifier()
%svm_clf = SVC()

%voting_clf = VotingClassifier(
%        estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],
%        voting='hard'
%    )
%voting_clf.fit(X_train, y_train)”
%“>>> from sklearn.metrics import accuracy_score
%>>> for clf in (log_clf, rnd_clf, svm_clf, voting_clf):
%>>>     clf.fit(X_train, y_train)
%>>>     y_pred = clf.predict(X_test)
%>>>     print(clf.__class__.__name__, accuracy_score(y_test, y_pred))
%LogisticRegression 0.864
%RandomForestClassifier 0.872
%SVC 0.888
%VotingClassifier 0.896”
In oder to do soft voting you need the classifier be able to estimate class probabilities $(predict_proba)$ \footnote{SVC does not estimate class probabilities but still you can use the SVC class for soft voting, just set the probability hyperparameter to True (this will make the SVC class use cross validation to estimate class probabilities, slowing down training, and it will add a $predict_proba()$}. Soft voting often achieves better performance than hard voting because it gives more weight to highly confident votes.
%JUST replace voting="hard" with voting="soft" and ensure that all classifiers can estimate class probabilities”


\subsection{Bagging and Pasting}
Before we saw how to introduce variability by using different classifiers, another way to introduce diversity in the voters is using the same algorithm but trained in different datasets ie. Bagging and Pasting.
%for Pasting BaggingClassifier(...bootstrap=False
“When sampling is performed with replacement, this method is called bagging (short for bootstrap aggregating). When sampling is performed without replacement, it is called pasting.”. “Both bagging and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor”
So for one training set we get n training sets by sampling it randomly (with replacement for bootstrap) and then training on each sample.“Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors”. “The aggregation function is typically the statistical mode (i.e., the most frequent prediction, just like a hard voting classifier) for classification, or the average for regression”. Note that each individual predictor has a larger bias than if it were trained on the original (same size) dataset but aggregation reduces both bias and variance.
Bagging and Pasting scale very well because they can be trained and also predict in parallel (GPUs).

%“ BaggingClassifier automatically performs soft voting instead of hard voting if the base classifier can estimate class probabilities ”

%“from sklearn.ensemble import BaggingClassifier
%from sklearn.tree import DecisionTreeClassifier

%bag_clf = BaggingClassifier(
%        DecisionTreeClassifier(), n_estimators=500,
%        max_samples=100, bootstrap=True, n_jobs=-1
%    )
%bag_clf.fit(X_train, y_train)
%y_pred = bag_clf.predict(X_test)”
“Overall, bagging often results in better models than pasting, which explains why it is generally preferred”
“With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all”

%bag_clf.oob_score_ is a proxy of accuracy in the test test, because some samples will never see them, the out of bag
%“oob_decision_function_ variable. In this case (since the base estimator has a predict_proba() method) the decision function returns the class probabilities for each training instance” tells the estimate of class for each training instance


\subsection{Random Forest}
“ Random Forest is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set”“Instead of building a BaggingClassifier and passing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees”

%“from sklearn.ensemble import RandomForestClassifier

%rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)
%rnd_clf.fit(X_train, y_train)
%“With a few exceptions, a RandomForestClassifier has all the hyperparameters of a DecisionTreeClassifier (to control how trees are grown), plus all the hyperparameters of a BaggingClassifier to control the ensemble itself”
“The Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node (see Chapter 6), it searches for the best feature among a random subset of features. This results in a greater tree diversity, which (once again) trades a higher bias for a lower variance, generally yielding an overall better model. The following BaggingClassifier is roughly equivalent to the previous RandomForestClassifier:”
%we can achieve this also with “BaggingClassifier(DecisionTreeClassifier(splitter="random”
%rnd_clf.feature_importances_

\subsection{Boosting}
Boosting is any ensemble method that combine weak learners into a strong learner. The idea is to train predictors sequentially each trying to correct its predecessor. AdaBoost (short for adaptive boosting) and Gradient boosting are the most common algorithms.

\textbf{AdaptBoost}

The idea is haver a chain of predictors where deeper in the chain predictors focus on harder instances (instances where the predictor underfitted). For example , eight of misclassified training instances is then increased and a second classifier is trained using the updated weights. The difference between AdaBoost and Gradient Descent is that in Ada we don't tweak the hyperparameters of any individual predictor it rather adds predictors to the ensemble. Once all predictors are trained, the ensemble makes predictions very much like bagging or pasting, except that predictors have different weights depending on their overall accuracy on the weighted training.

The drawback of Boosting is that is sequential and therefore it can't be parallelized so it does not scale well because we need the previous predictor the train the next one.
“If your AdaBoost ensemble is overfitting the training set, you can try reducing the number of estimators or more strongly regularizing the base estimator.”

\textbf{Gradient Boosting}
Gradient Boosting is also sequential like Ada, but rather than tweaking the weights of the instances, GB tries to fit the new predictor to the residual errors.






\subsection{Gradient boosted decision trees}
Random forest and gradient boosted are both tree based ensemble methods. Like Random forest Gradient boosted creates an ensemble of decision tree to create more powerful predictive models for classification and regression. Random forest is a bagging algorithm Gradient boosted is a boosted algorithm, bagging reduces variability (stability of the classifier) and boosting reduces both variance and bias. It reduces variance because you are using multiple models as we do in bagging and it reduces bias by training the subsequent model by telling him what errors the previous models.
Boosting was originally a theoretical invention motivated by the question "can we build a stronger model using weaker models", weak learner here mean shallow trees built in a non random way (different from RF) to create a model that makes fewer and fewer mistakes as these are added (see Schapire The Strength of Weak Learnability). An interesting property of GBDT is that it works with different loss functions, even when the derivative is not convex (eg pinball loss function). %https://www.lokad.com/pinball-loss-function-definition
 There are tow main algorithms: Adaboost and Gradient boosting. Adaboost is the original algorithm and what it does is to you tell subsequent models to punish more heavily observations mistaken by the previous models. In Gradient boosting you train each subsequent model using the residuals (the difference between the predicted and true values). The learning rate parameter controls how hard each tree tries to correct the mistakes from previous rounds (high learning rate complex trees, low learning rate simpler trees). 
What do they work? We don't really know, note that Data science is an empirical science, "because it works" is good enough \footnote{The 'As if' argument in economics (Milton Freedman) engineering view of scientific, see the the analogy of the billiard player plays “as if he knew ... complicated mathematical formulas”. For Friedman scientific theory (hypothesis or formula) cannot be tested by testing the realism of its assumptions.  All that matters is the accuracy of a theory’s predictions, not whether or not its assumptions are true. }. %http://www.rweconomics.com/BPA.htm

GBDT are often best off-the-shelf accuracy on many problems, require not much memory and are fast but like random forests the results are hard for humans to interpret, require careful tuning of the learning rate.

% Extreme gradient boosting
%http://xgboost.readthedocs.io/en/latest/model.html
%https://github.com/dmlc/xgboost
The optimization problem of decision trees (forest) is much harder than than traditional optimization problem like back propagation which just do gradient descent. Since it is not easy to train all the trees at once we can use an additive strategy: fix what we have learned, and add one new tree at a time. 

\subsection{Random forests}

Random forest is a bagging algorithm because it tries to "fix" the reliability problem existing in the decision tree model. DT are unreliable in the sense that small changes in your data produce may produce large changes very different decision trees\footnote{This is reminiscent to the the butterfly effect in chaos theory coined by Edward Lorenz}. This is where bagging comes from, we can create a robust model through bagging, that is, create different models by resampling your data. Each tree will deal with a different set of the data called the bootstrap sample, chooses at random with replacement this means that a bootstrap sample may have missing instances and also repeated instances doing this N times (one for each Tree) importantly there is not only randomness in picking the dataset for each tree but also there is randomness on the features from which to decide to split or not, if the max features is 1 we have forests with diverse complex trees and with max features closed to the number of features will lead to similar forests with simpler trees. Once the random forest is train the overall prediction is a a weighted vote.
Random forest is just the application of bagging to decision trees, and we want to do this if we care about stability.

Random forest inherit many of the benefits of decision trees. Pros widely used excellent performance on a variety of problems, easily parallelized, does not require careful normalization of features it may not be good for very highly dimensional problems (text classifiers). some of the k parameters are n estimators (default 10) is the number of trees, the max features parameters has a large effect on performance influence the diversity of trees in the forest, the max depth by default is None which means that the tree will continue to split until all nodes (or the min samples which is 2) in the leaf belong to the same class.
%https://github.com/mapattacker/datascience/blob/master/supervised.rst

%http://blog.kaggle.com/2014/08/01/learning-from-the-best/


\section{Deep networks}
\label{se:deep}

The MLP expands on the regression model seen in section \ref{se:me}. In regression we have just two layers, the input $x$ and the output $(\hat{w})$ and the goal of the procedure was to estimate the output $\hat{y}$ via OLS or related methods that estimates the weights $\hat{w}$ of the inputs in the expression $\hat{y} = \sum_i \hat{w_i} x$. In MLP we add a hidden layer which computes a nonlinear function of the weighted sums of the input features. The linear function that the hidden units apply is called the activation function. For example
\begin{equation}
h_i = tanh(w_{i0}x_0 + w_{i1}x_1 + w_{i2}x_2 ....) \\
\hat{y} = v_0 + h_0 + v_1 h_1 + ...
\end{equation}
%https://www.coursera.org/learn/python-machine-learning/lecture/v4cs3/neural-networks  
\textit{tanh} is the hyperbolic tangent function which is related to the logistic function but it is not the only choice for activation function of the hidden units, for example, \textit{relu (hockey stick), logistic} etc. The \textit{relu} activation function is the default in sklearn.

It is the addition of the hidden layer with the non linear function adds expressive power and it is what allows the network to learn more complex patterns than linear and logistic regression models. Of course this increase in expressive power comes at the price of having more weights (model coefficients) to estimate, that is, we need more training data and computation compared to a linear model \footnote{Memory, data and time are the limitations that have been overcome. The crude limitation in memory in computers (Gates infamously said  640K is good for anyone is not a problem anymore, data now there is too much of it that the problem is how to do something without it and the limitation these days it is time, but not computing time but developer-engineer time, humans cost money while computation is almost free.}.

%https://www.wired.com/1997/01/did-gates-really-say-640k-is-enough-for-anyone/
A deep-learning model is a directed, acyclic graph of layers. The topology of a network defines a hypothesis space.
The components of neural networks are: layers (the architecture of the network), inputs and target, loss function (is the feedback learning that allows learning), optimizer (tell us if we are learning). The loss function is the difference (can be calculated in multiple ways) between the predicted y and the true y, it produces a loss score which is input to the optimizer whose output will update the weights. 
Thus, what deep learning in particular and machine learning in general does is to search for useful representations of input data using the guidance of a feedback signal(loss function) within a predefined space of possibilities or hypothesis space. Note that the choice of the topology constraint the space of possibilities or hypothesis space to a specific series of tensor operations and what you ultimately expect to find is a good configuration of weight tensors. Picking the right network architecture is more an art than a science although you should try to rely as much as possible on good practices.

The fundamental data structure is the layer, a layer is a processing device which takes one or more tensors and outputs one or more tensors. The state of the layer is given by the weights (tensor) learned with the optimizer (stochastic gradient descent).
2D tensors of shape (samples, features), is often processed by densely connected layers, also called fully connected or dense layers. Sequence data, on the other hand, are stored in 3D tensors of shape (samples, timesteps, features), is typically processed by recurrent layers such as an LSTM layer. Image data, stored in 4D tensors, is usually processed by 2D convolution layers (Conv2D).
%Chollet book pg 144


System -> Loss/Objective function ( produces a number that we want to minimize during training)-> Optimizer (determines how the network will update the weights based on the loss)-> System 

The optimizer is SGD of some sort of other but the loss function changes depending on the task, for a two-class classification problem choose binary crossentropy. Without an activation function like relu (also called a non-linearity), the Dense layer would consist of two linear operations—a dot product and an addition output = dot(W, input) + b; so the layer could only learn linear transformations (affine transformations) of the input data: the hypothesis space of the layer would be the set of all possible linear transformations of the input data into a number of units layer-dimensional space. 
In order to get access to a richer hypothesis space (deep representations) we need non-linearity, or activation function, for example relu: output = relu(previous output). The optimizer \textit{rmsprop} should work in most situations.


For the loss function, we can use crossentropy (distance between two probability distributions, in this case between ground-truth distribution and our predictions), mean squared error can also be used.
Validate the approach consists in train the model for n epochs (n iterations over all samples in the x train and y train tensors) in mini batches of m samples, while at the same time monitor the loss and accuracy on the test data set.
The validation set is created by setting apart  1,000? (10 per cent of the training) samples in the training data to use as a validation set
%len(train_data) 8982
%x_val = x_train[:1000]
%partial_x_train = x_train[1000:]

%y_val = one_hot_train_labels[:1000]
%partial_y_train = one_hot_train_labels[1000:]
%train the network fro 20 epcohs
In Keras deep learning models are built by putting together compatible layers to form an useful data-transformation pipeline.  The Keras work flow is:
\begin{enumerate}
\item input and target tensors
\item define model (layers that map inputs to targets)
\item learning (compile) choose loss function, optimizer and loss score 
\item learning (compile) choose loss function, optimizer and loss score 
%model.compile(optimizer=optimizers.RMSprop(lr=0.001),loss='mse',metrics=['accuracy'])
\item iterate on your training data %model.fit(input_tensor, target_tensor, batch_size=128, epochs=10

\end{enumerate}

%for example layers.Dense(32, input_shape=(784,)) is a layer with 32 units receiving 784 inputs, so it will only accept a 2D input tensor whose dimension 0 is 784, the layer output 32 dimensions. Thhus this can only cbe connected downstream to another layer that takes 32 as input, But Keras takes  care of this, you dont need to spec the input dimension. For examples:
%model = models.Sequential()
%model.add(layers.Dense(32, input_shape=(784,)))
%model.add(layers.Dense(32)) #dont need to put 32 as input_shape it is inferred

%cross validation
In the evaluation (training phase) you also keep adjusting the network parameters (number of epochs, that is, find the minimum number of epochs beyond which the network overfits). We split the data into training set and validation set (eg $10\%$) but since we have so few data points (less than 100) we encounter the problem that the validation score has a high variance with regard to the validation split, that is, the score depends a lot on which data points you use to validate (add to this that the set is imbalance we may have by chance a validation set with 0 converters). It goes without saying that this would prevent from a reliable evaluation of the model. 
The best practice to deal with this situation is to use K-fold cross validation, split the available data into K partitions (e.g. between 4 and 5) instantiating K identical models and training each one on K-1 partitions while evaluating on the remaining partition, that validations core is the average of the K validation scores.
At the end we get a final production model with the "right" parameters (number of epochs, layers, units, optimizer, loss, activation) train the final production model on all the training data (now without separating for validation) with the best parameters and look at its performance on the test data.
% for example, let us see that we found thge epochs = 80
%model = build_model(); model.fit(train_data, train_targets,epochs=80, batch_size=16, verbose=0);
%test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)   



\section{Appendix}
Notation:  a dataset is a collection of examples which are in turn collections of features. The design matrix is the most common form of describing a dataset, $X \in R^\{n,m\}$, where n is the number of examples (rows) and m the number of features (columns), so $X^\{i,j\}$ is the value of the feature j for example i

Capacity: a model's capacity is its capacity to fit a wide variety of functions. for example, we we take the linear regression model we can increase its capacity by introducing x as another feature, note that the output is still a linear function of the parameters and it can be solved with the normal equations. MAchine learning algorithms perform best when they have a capacity appropriate for the complexity of the task and the training data available , models with high capacity can solve complex problems but when the capacity exceeds the complexity they may overfit (Occam's razor). 

Complexity: The complexity of a model can be measured with the Vapnik-Chervonenkis dimension (complexity of binary classifier). The discrepancy between training error and generalization error is bounded from above by a quantity that grows as the capacity grows but shrinks as the size of the training set increases. Interestingly, these bounds are the intellectual justification of why machine learning works.

The network or model capacity is the number of learnable parameters -number of layers and units per layer-. A model with more parameters will have more memorization capacity but that does not imply that will generalize well.On the other hand, if the network has limited memorization resources, it won’t be able to learn this mapping as easily; thus, in order to minimize its loss, it will have to resort to learning compressed representations that have predictive power regarding the targets—precisely the type of representations we’re interested in. (Keep inn mind that you should use models that have enough parameters that they don’t underfit: your model shouldn’t be starved for memorization resources). There is a compromise to be found between too much capacity and not enough capacity.

%-------------------------------------------------------------------------------
% REFERENCES
%-------------------------------------------------------------------------------
\newpage
\section*{References}
\addcontentsline{toc}{section}{References}


% BibTeX users please use
\bibliographystyle{spmpsci}
\bibliography{../bibliography-jgr/bibliojgr}

\end{document}

\section{Temp results}
Correlation 

Explanatory variables

['sexo', 'lat_manual', 'nivel_educativo', 'apoe', 'edad', 'hsnoct', 'sue_dia', 'sue_noc', 'sue_con', 'sue_man', 'sue_suf', 'sue_pro', 'sue_ron', 'sue_mov', 'sue_rui', 'sue_hor', 'sue_rec', 'imc', 'dempad', 'edempad', 'demmad', 'edemmad', 'audi', 'visu', 'a01', 'a02', 'a03', 'a04', 'a05', 'a06', 'a07', 'a08', 'a09', 'a10', 'a11', 'a12', 'a13', 'a14', 'sdhijos', 'numhij', 'sdvive', 'sdeconom', 'sdresid', 'sdestciv', 'sdtrabaja', 'sdocupac', 'sdatrb', 'hta', 'hta_ini', 'glu', 'lipid', 'tabac', 'tabac_ini', 'tabac_fin', 'tabac_cant', 'sp', 'cor', 'cor_ini', 'arri', 'arri_ini', 'card', 'card_ini', 'tir', 'ictus', 'ictus_num', 'ictus_ini', 'ictus_secu', 'tce', 'tce_num', 'tce_ini', 'tce_con', 'tce_secu', 'alfrut', 'alcar', 'alpesblan', 'alpeszul', 'alaves', 'alaceit', 'alpast', 'alpan', 'alverd', 'alleg', 'alemb', 'allact', 'alhuev', 'aldulc', 'scd_visita1', 'edadinicio_visita1', 'tpoevol_visita1', 'peorotros_visita1', 'preocupacion_visita1', 'eqm06_visita1', 'eqm07_visita1', 'eqm81_visita1', 'eqm82_visita1', 'eqm83_visita1', 'eqm84_visita1', 'eqm85_visita1', 'eqm86_visita1', 'eqm09_visita1', 'eqm10_visita1', 'act_aten_visita1', 'act_orie_visita1', 'act_mrec_visita1', 'act_memt_visita1', 'act_visu_visita1', 'act_expr_visita1', 'act_comp_visita1', 'act_ejec_visita1', 'act_prax_visita1', 'act_depre_visita1', 'act_ansi_visita1', 'act_apat_visita1', 'gds_visita1', 'stai_visita1', 'eq5dmov_visita1', 'eq5dcp_visita1', 'eq5dact_visita1', 'eq5ddol_visita1', 'eq5dans_visita1', 'eq5dsalud_visita1', 'eq5deva_visita1', 'relafami_visita1', 'relaamigo_visita1', 'relaocio_visita1', 'rsoled_visita1', 'ejfre_visita1', 'ejminut_visita1', 'valcvida_visita1', 'valsatvid_visita1', 'valfelc_visita1', 'conversion']

%-------------------------------------------------------------------------------
% SNIPPETS
%-------------------------------------------------------------------------------

%\begin{figure}[!ht]
%	\centering
%	\includegraphics[width=0.8\textwidth]{file_name}
%	\caption{}
%	\centering
%	\label{label:file_name}
%\end{figure}

%\begin{figure}[!ht]
%	\centering
%	\includegraphics[width=0.8\textwidth]{graph}
%	\caption{Blood pressure ranges and associated level of hypertension (American Heart Association, 2013).}
%	\centering
%	\label{label:graph}
%\end{figure}

%\begin{wrapfigure}{r}{0.30\textwidth}
%	\vspace{-40pt}
%	\begin{center}
%		\includegraphics[width=0.29\textwidth]{file_name}
%	\end{center}
%	\vspace{-20pt}
%	\caption{}
%	\label{label:file_name}
%\end{wrapfigure}

%\begin{wrapfigure}{r}{0.45\textwidth}
%	\begin{center}
%		\includegraphics[width=0.29\textwidth]{manometer}
%	\end{center}
%	\caption{Aneroid sphygmomanometer with stethoscope (Medicalexpo, 2012).}
%	\label{label:manometer}
%\end{wrapfigure}

%\begin{table}[!ht]\footnotesize
%	\centering
%	\begin{tabular}{cccccc}
%	\toprule
%	\multicolumn{2}{c} {Pearson's correlation test} & \multicolumn{4}{c} {Independent t-test} \\
%	\midrule
%	\multicolumn{2}{c} {Gender} & \multicolumn{2}{c} {Activity level} & \multicolumn{2}{c} {Gender} \\
%	\midrule
%	Males & Females & 1st level & 6th level & Males & Females \\
%	\midrule
%	\multicolumn{2}{c} {BMI vs. SP} & \multicolumn{2}{c} {Systolic pressure} & \multicolumn{2}{c} {Systolic Pressure} \\
%	\multicolumn{2}{c} {BMI vs. DP} & \multicolumn{2}{c} {Diastolic pressure} & \multicolumn{2}{c} {Diastolic pressure} \\
%	\multicolumn{2}{c} {BMI vs. MAP} & \multicolumn{2}{c} {MAP} & \multicolumn{2}{c} {MAP} \\
%	\multicolumn{2}{c} {W:H ratio vs. SP} & \multicolumn{2}{c} {BMI} & \multicolumn{2}{c} {BMI} \\
%	\multicolumn{2}{c} {W:H ratio vs. DP} & \multicolumn{2}{c} {W:H ratio} & \multicolumn{2}{c} {W:H ratio} \\
%	\multicolumn{2}{c} {W:H ratio vs. MAP} & \multicolumn{2}{c} {\% Body fat} & \multicolumn{2}{c} {\% Body fat} \\
%	\multicolumn{2}{c} {} & \multicolumn{2}{c} {Height} & \multicolumn{2}{c} {Height} \\
%	\multicolumn{2}{c} {} & \multicolumn{2}{c} {Weight} & \multicolumn{2}{c} {Weight} \\
%	\multicolumn{2}{c} {} & \multicolumn{2}{c} {Heart rate} & \multicolumn{2}{c} {Heart rate} \\
%	\bottomrule
%	\end{tabular}
%	\caption{Parameters that were analysed and related statistical test performed for current study. BMI - body mass index; SP - systolic pressure; DP - diastolic pressure; MAP - mean arterial pressure; W:H ratio - waist to hip ratio.}
%	\label{label:tests}
%\end{table}
