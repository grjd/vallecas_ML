%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% 1.pdflatex main.tex 2.bibtex main.aux 3. pdflatex  main.tex 4. pdflatex  main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt]{article}
\usepackage{tabularx}
\newcolumntype{b}{X}
\newcolumntype{s}{>{\hsize=.5\hsize}X}
%\usepackage[spanish]{babel}
\usepackage[english]{babel}
%\usepackage[osf]{mathpazo}
%\usepackage{bookman}
\usepackage{palatino}
\usepackage[utf8]{inputenc}
\usepackage[utf8]{luainputenc}
%\usepackage{floatrow}

%\usepackage[T1]{fontenc}
%\usepackage{pgffor}
%\usepackage{lipsum}
%\usepackage[OT1]{fontenc}
\usepackage[a4paper]{geometry}
\usepackage[myheadings]{fullpage}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{amsfonts, amsmath, amsthm, amssymb}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
 
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{graphicx, wrapfig, subcaption, setspace, booktabs}
\usepackage[T1]{fontenc}
\usepackage[font=small, labelfont=bf]{caption}
\usepackage{fourier}
\usepackage{float}
%\usepackage[protrusion=true, expansion=true]{microtype}
%\usepackage[margin=1in]{geometry}
\usepackage{microtype}
\usepackage{sectsty}
\usepackage{url, lipsum}
\usepackage{listings}
\usepackage{xcolor}
\newcounter{countCode}
\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}
\onehalfspacing
\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}
\lstnewenvironment{code} [1][caption=Ponme caption, label=default]{%
\renewcommand*{\lstlistingname}{Python} 
	\setcounter{lstlisting}{\value{countCode}} 
	\lstset{ %
	language=python,
	basicstyle=\ttfamily\footnotesize,       % the size of the fonts that are used for the code
	numbers=left,                   % where to put the line-numbers
	numberstyle=\sc,      % the size of the fonts that are used for the line-numbers
	stepnumber=1,                   % the step between two line-numbers. 
	numbersep=5pt,                 % how far the line-numbers are from the code
	numberstyle=\color{red!50!blue},
    backgroundcolor=\color{lightgray!20},
	rulecolor=\color{blue},
	keywordstyle=\color{red}\bfseries,
	showspaces=false,               % show spaces adding particular underscores
	showstringspaces=false,         % underline spaces within strings
	showtabs=false,                 % show tabs within strings adding particular underscores
	frame=single,                   % adds a frame around the code
	framexleftmargin=0mm,
	numberblanklines=false,
	xleftmargin=5pt,
	breaklines=true,
	breakatwhitespace=true,
	breakautoindent=true,
	captionpos=t,
	texcl=true,
	tabsize=2,                      % sets default tabsize to 3 spaces
	extendedchars=true,
	inputencoding=utf8, 
	escapechar=\%,
	morekeywords={print, println, size, background, strokeWeight, fill, line, rect, ellipse, triangle, arc, save, PI, HALF_PI, QUARTER_PI, TAU, TWO_PI, width, height,},
	emph=[1]{print,println,}, emphstyle=[1]{\color{blue}}, % Mis palabras clave.
	emph=[2]{width,height,}, emphstyle=[2]{\bf\color{violet}}, % Mis palabras clave.
	emph=[3]{PI, HALF_PI, QUARTER_PI, TAU, TWO_PI}, emphstyle=[3]\color{orange!50!violet}, % Mis palabras clave.
	emph=[4]{line, rect, ellipse, triangle, arc,}, emphstyle=[4]\color{green!70!black}, % Mis palabras clave.
	%emph=[5]{size, background, strokeWeight, fill,}, emphstyle=[5]{\tt \color{red!30!blue}}, % Mis palabras clave.
	%emph={[2]sqrt,baset}, emphstyle={[2]\color{blue}}, % f(sqrt(2)), sqrt a nivel 2 se pondrá azul
#1}}{\addtocounter{countCode}{1}}
%-------------------------------------------------------------------------------
% HEADER & FOOTER
%-------------------------------------------------------------------------------
\pagestyle{fancy}
\fancyhf{}
\setlength\headheight{12pt}
\fancyhead[L]{Five years of \emph{Proyecto Vallecas}}
\fancyhead[R]{Gómez-Ramírez et al. Fundaci\'on CIEN}
\fancyfoot[R]{\small{page} \thepage\ \small{of} \pageref{LastPage}}
%-------------------------------------------------------------------------------
% TITLE PAGE
%-------------------------------------------------------------------------------

\begin{document}
%IDENTIFICACIÓN DE FACTORES DE RIESGO EN DETERIORO COGNITIVO LEVE CON APRENDIZAJE AUTOMÁTICO: HACIA UNA AYUDA AL DIAGNÓSTICO MULTIFACTORIAL Y  AUTOINFORMADA

\title{ \normalsize \textsc{Identificación de Factores de Riesgo en Deterioro Cognitivo Level con técnicas de inteligencia artificial: Ayuda al Diagnóstico con datos longitudinales del Proyecto Vallecas}
		\\ [2.0cm]
		\HRule{0.5pt} \\
		\LARGE \textbf{\uppercase{Identification of risk factors in the \emph{Vallecas} longitudinal dataset. Applying Artificial Intelligence in early detection of Mild Cognitive Impairment}}
		\HRule{2pt} \\ [0.5cm]
		\normalsize \today \vspace*{5\baselineskip}}

\date{ }
\author{
		Jaime G\'omez-Ram\'irez, Marina \'Avila Villanueva, Bel\'en Frades Payo, Teodoro del Ser Quijano,\\ Meritxell Valent\'i Soler, María Ascensi\'on Zea Sevilla and Miguel \'Angel Fern\'andez-Bl\'azquez   \\  \\
		\textbf{\large{Fundaci\'on Reina Sof\'ia}} \\
		Centre for Research in Neurodegenarative Diseases
		\\ \emph{Valderrebollo, 5, 28031 Madrid}
 }

\maketitle
\begin{center}
\includegraphics[width = 60mm]{figures/logo_mciu.png}
\end{center}
\newpage
\tableofcontents
\newpage

%-------------------------------------------------------------------------------
% Section title formatting
\sectionfont{\scshape}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
% BODY

%CODE
%tensorflow/production/descriptive_stats.py
%-------------------------------------------------------------------------------

\section*{Resumen}

La multifactorialidad, comorbilidad y el carácter asíntomático en el estadio iniciales de la enfermedad de Alzheimer (EA) están dificultando enormemente la obtención de tratamientos efectivos. No debería sorprender pues que el ratio de éxito en la aprobación de fármacos en EA sea un exiguo $0.4\%$, uno de los mas bajos de la industria farmacéutica. 
La identificación de factores de riesgo, preferentemente de sencilla obtención, se hacen cada vez más necesarios habida cuenta que la prevalencia de la EA crece exponencialmente con la edad y la esperanza de vida se prolonga cada vez más en una población cuya natalidad se reduce.

Los investigadores del Proyecto Vallecas a lo largo de los últimos 6 años han ido recabando información de voluntarios de más de 70 años. Dicha información incluye entre otros, factores relativos al estilo de vida (dieta, sueño, ejercicio físico etc), datos demográficos y socioeconómicos (lugar de residencia, sexo, educación, estado civil etc.), presencia de quejas subjetivas de memoria etc.

El objetivo de este estudio es el de identificar un conjunto de factores autoinformados, es decir, factores conocidos por el propio sujeto sin necesidad de un experto o procedimiento externo, que sean de ayuda a la hora de clasificar sujetos con riesgo de padecer/desarrollar un deterioro cognitivo.
Se reclutaron 921 individuos cognitivamente sanos participantes en el estudio longitudinal enmarcado en el Proyecto Vallecas. Los participantes fueron evaluados anualmente a lo largo de los cinco años del estudio. La evolución clínica clasificó los participantes en dos grupos controles y conversores a deterioro cognitivo level (DCL) $(9,4\%)$. 
Se utilizaron técnicas de \emph{machine learning} incluyendo, reducción de dimensionalidad y aprendizaje supervisado conducentes a la obtención de modelos predictivos de conversion a DCL. Los algoritmos predictivos utilizados presentaron una precisión y exactitud muy por encima de estimadores aleatorios.
Este estudio muestra que la base de datos del Proyecto Vallecas, explotada convenientemente con algoritmos capaces de capturar patrones complejos no lineales, puede ayudar a identificar factores de riesgo de conversión a deterioro cognitivo. La detección temprana mediante este tipo de técnicas permite no solo detectar factores de riesgo en fases asintomáticas sino además establecer intervenciones personalizadas que incidan en el estilo de vida.

\section*{Abstract}
Alzheimer's Disease (AD) is a complex, multifactorial and comorbid condition. The asymptomatic behavior in early stages of the disease is a paramount obstacle to formulate a preclinical and predictive model of AD. Not surprisingly, the AD drug approval rate is one of the lowest in the industry, an exiguous $0.4\%$. The identification of risk factors, preferably obtained by the subject herself, is sorely needed given that the incidence of Alzheimer’s disease grows exponentially with age. 
For the last 6 years, researchers at \emph{Proyecto Vallecas} have collected information about the project's volunteers, aged 70 or more. The  information includes self informed factors (do not require an expert) -lifestyle, demographic and socioeconomic, subjective memory complains etc. 

We use machine learning techniques, including supervised algorithms, manifold learning, clustering and deep networks to perform predictive analytics in the \emph{Proyecto Vallecas} dataset.
We show that the longitudinal dataset of \emph{Proyecto Vallecas}, if conveniently exploited with algorithms able to deal with the complex and nonlinear patterns that embedded in the dataset, may help identifying risk factors related to MCI conversion.
The early detection with machine learning algorithms makes possible not only to detect risk factors in asymptomatic stages but also to design  personalized interventions informed by the subject's lifestyle.

\section{Introduction}

The Vallecas Project for early detection of AD is the most ambitious population-based study in Spain. The project is carried out at the Queen Sofia Foundation Alzheimer Center by a multidisciplinary team of researchers from the CIEN Foundation. The main objective of the Vallecas Project is to elucidate, through tracking of progression of the cohort, the best combination of features, clinical and others \footnote{Note in the inception of the project only clinical parameters were considered, here we advocate for a more inclusive set of parameters including neuropsychological, and also features related to the lifestyle such as nutrition and physical exercise}, that are informative about developing 
cognitive impairment in the future. Thus, it intends to identify various \emph{markers} to eventually determine the potential risk that an  individual has to develop the disease in the future. The dataset contains more than 1,000 examples described with $~500$ features that include a wide range of characteristics collected yearly. Importantly, the dimensionality of the dataset grows every year (new features are added) as it does the number of samples as the subjects keep coming for their yearly visits, however the number of examples collected per year decreases (some volunteers drop the study). Table \ref{fig:tableclustervallecas} shows a non exhaustive list of the features types that are collected in the study.

\begin{figure}[h]
        \centering
        \includegraphics[keepaspectratio, width=0.9\linewidth]{figures/tableclustervallecas}
        \caption{The Vallecas Project collects a longitudinal dataset for over 1,000 volunteers, the number of features collected amounts to more than 500. The table shows some of the most import types of features. Note that some features are collected only during the first year (basal) for example: familiar AD, APOE, level of education etc., but for the most part, features are assessed for every visit by the team of neuropsychologists and neurologists.} \label{fig:tableclustervallecas}
\end{figure}

The problem we are trying to solve here is \textbf{prediction}, in particular classification. 
%and the 
Prediction is a way to generate information you do not have, that is, a predictive machine gets an input data set and uses it to generate new information. Classification is a quintessential prediction problem, for example, given a dataset of subjects whom have been diagnosed, we aim at building a classifier that can estimate the odds that a new subject will convert. 

Before going any further it is worth giving some basic definitions on the methodology we use here. First, we need to understand what \textbf{Artificial Intelligence} (AI) is. The term AI was coined in 1956 by John McCarthy, AI studies how human intelligence can be implemented in technical systems. AI is an overarching concept including all sort of intelligent processes done by humans such as planning, problem solving, understanding language, recognizing objects, learning etc. 
Machine learning is an approach to achieve Artificial Intelligence. However, machine learning is more modest in the sense that does not claim to reverse engineer the human mind, it rather deals with making prediction as accurate as possible. While AI tries to build 'intelligent' machines, machine learning, on the other hand, focuses on one aspect of intelligence, prediction.
Finally, \textbf{Deep Learning} is the most promising and popular approach to machine learning, other approaches include decision trees, Bayesian network, kernel methods, clustering etc.

Our goal here is twofold, first and foremost demonstrate the predicting power of machine learning techniques in the \emph{Vallecas dataset}, and second introduce a new set of techniques that go under the umbrella of machine learning that provide a new framework and methodology different form canonical statistical modeling. Since this may not be obvious to researchers not acquainted with Artificial Intelligence and Machine learning, we highlight the main differences between statistical learning and machine learning in  Table \ref{tab:smml}.  

\begin{table}[htbp]
    \centering
    %\begin{tabularx}{\textwidth}{| X | X | X |}
    \begin{tabularx}{\textwidth}{bb}
        \hline
        Statistical modeling     & Machine learning        \\ \hline
        Statistical modeling is applying statistics on data to find underlying relationships by analyzing the significance of the variables.         & Machine learning is a subset of AI, builds computer programs of statistical techniques that learn from data.                \\ \hline
        Emphasis is in the significance of the variables and building confidence intervals. & Emphasis is on using computers to estimate functions rather than confidence intervals around these functions.             \\ \hline
        Statistical models start with assumptions about the variables, perform hypothesis testing. & No prior assumption about input. \\  \hline
        Fits the hyperplane with the least errors. & Optimization of parameters that minimize a cost function (e.g. gradient descent: update parameter in the opposite direction to the gradient). \\  \hline
		Relationships between the variables are formalized in mathematical equations.& Algorithm rather than a priori equations, patterns are automatically learn from data.\\  \hline
		Performs significance tests (Diagnostics of parameters p-value etc.)& Significance tests are not required.\\  \hline
		Data split into training and test sets. & Data split into training, validation and test sets.\\  \hline	
    \end{tabularx}
    \caption{Statistical modeling vs Machine learning}  \label{tab:smml}
\end{table}

%  Thus, Section \ref{} performs exploratory data analysis, including multicollinearity detection among different features and commenting on the feature engineering process used. Section \ref{} explains the different predictive models implemented and Section \ref{} deals with the evaluation of the predictive models that are described in the previous section. Importantly, we weigh on the interpretability side of the results and not only on the prediction side, section \ref{sse:interp}.
The document is self contained, it provides a painless introduction to machine learning where the emphasis is in understanding the mechanics of the techniques used rather than mathematical formalization. We follow a first principles example-based approach in which the outcome of the algorithms' implementations is explained and put in the context of the  \emph{Vallecas} dataset. Our hope is to accomplish the dual objective of being both didactic about machine learning techniques and informative about the results obtained in the \emph{Vallecas Project} using those techniques.

The structure of the document is as follows. Section \ref{se:crisp} introduces the methodology that we will use use to study the \emph{Vallecas} dataset, namely the CRISP-DM model. In Section \ref{se:eda} we define the \emph{Vallecas Index} and we perform exploratory data analysis, providing descriptive statistics and statistical inference to test hypothesis of the population. Section \ref{se:mod} explains the different machine learning techniques used for predictive analytics. Next, in Section \ref{se:res} we show the results obtained for each one of the techniques within a comparative framework described in Section \ref{se:compare}. Section \ref{se:imbalance} proposes a solution to the class imbalance problem that exists in the \emph{Vallecas} dataset (the converter class is underrepresented). We conclude with the discussion of the results and future works in Section \ref{se:dis}.

%focuses on the deployment of the most successful models and argues about the benefits and challenges of deploying our models \emph{extramural}
\section{CRISP-DM model}
\label{se:crisp}

 The voluminous and complex (heterogeneity and dimensionality) dataset collected in the \emph{Vallecas Project} make it particularly difficult to build analytics of any kind, let alone predictive analytics \footnote{In Deep Learning data is always a blessing, the more data the better, very large -tens or even hundreds of thousands of examples- are in principle preferred to more modest datasets like ours which is in the order of 1,000 thousand examples.}. In order to try to cope with this difficulty, we use a methodology well suited for machine learning analytics called CRoss Industry Standard Process for Data Mining (CRISP-DM model). We will follow the CRISP-DM methodological plan in order to effectively deal with data preparation, modeling, evaluation and deployment.
%James Wu, Foundations of predictive analytics
  
The CRISP-DM model is the most popular methodology used in machine learning. CRISP-DM consists in 6 phases that act upon themselves in an iterative loop depicted in Figure \ref{fig:crisp}.
\begin{enumerate}
%1. Problem understanding
\item \textbf{Business understanding}: Understanding the project objectives and requirements from a 'business' perspective. The 'business' here is no other than prediction. 
	\begin{itemize}
		\item Definition of the problem: We have a dataset that contains a number of features and we want to clarify the relationships between those inputs and most importantly, which if any and in which measure have predictive power about conversion to MCI and dementia.
	\end{itemize}
%2. Data Understanding Assessment of scenarios for analysis. 
\item \textbf{Data understanding}: The main resource available are spreadsheets that contain demographic (age, school years...), genetic ( APOE), cognitive performance metrics from neuropsychological tests, together with features related to life style including nutrition and physical exercise among others \footnote{ASL, MRI and fMRI data are also available (for the large majority of the volunteers) but need to be integrated with the above mentioned features. Here we will not deal with neuroimaging data. This is being undertaking in forthcoming work.}
\item \textbf{Data preparation}: Activities to construct the final dataset from the initial raw data
	\begin{itemize}
		\item Data description: format, volume, description of attributes. \footnote{Some authors do not make any distinction between features and attributes, we however distinguish between attributes meaning a a column int the dataset and feature meaning an attribute that is useful for prediction.}
		\item Exploratory data analysis (EDA): charts, plots to visualize data features, find associations and correlations. This task comprises: explore and visualize data, select attributes including remove redundant or dependent attributes, test hypothesis about correlations and associations.
		\item Data quality analysis: Deal with missing values, inconsistent values.
		%3. Data preparation
		\item Clean, wrangle and curate the dataset before the learning machinery is launched to build models. This is the most time consuming phase, $(60\%)$ of the time.
		\item Data integration : In case of having multiple datasets. This will have to be taken care of when we integrate this dataset with with the imaging dataset.
		\item Data wrangling: Handle missing values (remove rows, handle missing values), formatting into csv, json etc.
	\end{itemize}
\item \textbf{Modeling}: various modeling techniques are selected and applied (Section \ref{se:res}). Specifically:
	\begin{itemize}
		\item Logistic regression
		\item SVM
		\item k nearest neighbors
		\item Naive Bayes
		\item Decision Trees
		\item Ensemble Methods (Random Forest, Bagging and Boosting)
		\item Deep networks
		\item Clustering methods (Hierarchical clustering, PCA and manifold learning)
	\end{itemize}
\item \textbf{Evaluation}: Once the models are built, proceed to evaluate them to identify the best models according with different metrics (accuracy, precision, interpretability etc). This will be shown in Section \ref{se:compare}.
\item \textbf{Deployment}: Knowledge gained will need to be organized and presented in a way that is useful to the customer or final user.
\end{enumerate}

\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=0.5\linewidth]{figures/crisp}
        \caption{\emph{Proyecto Vallecas} adopts CRISP-DM \cite{crispdmwiki} as the methodology of choice for analytics and data mining. CRISP-DM was conceived around 1996 and it is to this day the most important methodology in data science projects. It consists in a 6 high-level iterative phases. The adoption of this kind of methodology is crucial to deal with random and false discoveries, specially in Big Data projects like ours.}\label{fig:crisp}
\end{figure}

\section{Exploratory Data Analysis}
\label{se:eda}
In this section we perform Exploratory Data Analysis (EDA). The objective of EDA is to build graphical representations that help us make sense of the data. EDA gives us a bird's eye of the dataset to help us identifying patterns and trends and most importantly, assist us in framing the type of  questions we want to ask.
But prior to EDA we need to take care of pre-processing of the data. We will start with data understanding in Section \ref{sse:biz}, plotting charts containing basic information like age distribution, the distribution of conversion conditional to gender and other demographics of the participants. In Section \ref{sse:dp} we discuss the data transformations and in Section \ref{sse:miss} we describe the approach used for handling missing data. In Sections \ref{sse:dl} and \ref{sse:fe} we comment on data leakage and feature selection, respectively. Finally, in Section \ref{sse:vs} we explain variable selection i.e. the \textbf{Vallecas index} or the subset of the input space used in the learning algorithms.

%Here we perform Exploratory Data Analysis (EDA). EDA is the first step in the data analysis process and is performed once the pre processing steps have finished. 

\subsection{Definition of the problem and data understanding}
\label{sse:biz}
The first pre-processing step is an initial assessment of the dataset in order to achieve a functional understanding of the data collected. Features in the dataset fall into two categories, those that are self-informed, that is, the subject can report herself the value of the variable e.g. age, income level, education, sleep, diet etc. and those that require an expert, for example, APOE genotype, cognitive performance measured in performance tests etc.

The input dataset consists in five years of \emph{Proyecto Vallecas} and the output is whether any given subject converts or not to MCI in any of the future years. Thus, we want to predict whether a subject that was diagnosed as non MCI in the first visit (year 1) is diagnosed as MCI in any ulterior year. The total number of visits collected in the 5 years project is 4444. Figure \ref{fig:pv5years} depicts the number of volunteers for the first five years year of the project.

\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=0.5\linewidth]{figures/pv5years}
        \caption{Number of volunteers across five years of \emph{Proyecto Vallecas}. As expected, the number of subjects decreases across time. The number of subjects in the basal visit (year 1) was 1213 of which 33 were removed because were diagnosed with MCI or AD. The number subjects participating int the study are 1180 in year 1, 965 in year 2 ($18\%$ drop rate), 865 in year 3 ($10\%$ drop rate), 770 in year 4 ($11\%$ drop rate) and 664 in year 5 ($14\%$ drop rate). The total number of visits in  five years amounts to 4444.} \label{fig:pv5years}
\end{figure}

Figure \ref{fig:sexmci} shows the distribution of sex and conversion in 5 years of \emph{Proyecto Vallecas}.
\begin{figure}[H] 
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[height=2.5in]{figures/femalemale}
        \caption{Out of 1180 subjects, 427 are men and 753 women ($64\%$ women, $36\%$ men)}
    \end{subfigure}
    ~ 
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[height=2.5in]{figures/convmci}
        \caption{Non converters and MCI converters in 5 years time. Out of a total of 915 subjects with two or more visits, 829 do not convert and 86 convert to MCI at some time after the basal visit. The conversion rate is $10.3\%$}
    \end{subfigure}%\quad
    \caption{Distribution of sex and conversion and in 5 years of \emph{Proyecto Vallecas}} \label{fig:sexmci}
\end{figure}

Figure \ref{fig:age_dx} shows the age distribution of \emph{Proyecto Vallecas}. The oldest volunteer passing a regular visit is 90 years. Only 68 subjects undergone a visit when they were 85 or older. 
In order to study people with long life expectancy the study will require to keep these few older volunteers for the next 7 years which will be particularly challenging. A more likely scenario is recruiting new subjects older than 90.

\begin{figure}[H] 
        \centering
        \includegraphics[keepaspectratio, width=0.5\linewidth]{figures/AGE_LastDX}
        \caption{Histogram of the age distribution in the last visit in \emph{Proyecto Vallecas}. The oldest \emph{Vallecas} volunteer to pay  a visit is 90 years old and the youngest one was 68.83 in her first and last visit. The average age of the volunteers is 78 years old, calculated as the average of the volunteer's ages in the last visit.} \label{fig:age_dx}
\end{figure}


\subsection{Data Preparation} 
\label{sse:dp}
  
The dataset we are dealing with here is a longitudinal dataset with 5 time points, from year 1 (basal visit) to year 5.
Data preparation is a crucial step in machine learning projects, failure to properly pre-process the dataset will cause suboptimal modeling and error-prone predictions.
%https://paginas.fe.up.pt/~ec/files_1112/week_03_Data_Preparation.pdf
Generally speaking, the data preparation stage takes more than $60\%$ of a data mining project effort \cite{witten2005data}.

% Transformation including Scaling and discretize continuous variables (binning)
An obligatory step in pre-processing is transforming the data via scaling. 
%and discretize continuous variables (binning using hot-coding).
In general, algorithms that exploit distances or similarities ('nearness') between data samples, such as k-NN and SVM (Sections \ref{sse:kneighbors} and \ref{sse:svm}), are sensitive to feature transformation. 
Scaling the dataset is thus absolutely necessary if we use techniques sensitive to scaling \cite{wu2012foundations}. Although other techniques like naive Bayes (Section \ref{sse:naivebayes}), decision trees (Section \ref{sse:dectrees}) and tree-based ensemble (Section \ref{sse:ensemble}) methods are invariant to feature scaling, it is but still a good idea to rescale/standarize the dataset. In conclusion, scaling will do no harm in techniques invariant to scaling and it is necessary in those that are sensitive to scaling. 

There are two common ways to get all attributes (except the target variable which is rarely scaled) to have the same scale: min-max scaling and standardization.

Min-max scaling, also called normalization, shifts and rescaled values so that they are between 0 and 1 (Equation \ref{eq:minmax}). 
\begin{equation} \label{eq:minmax}
z_i = \frac{x_i -min}{max -min}
\end{equation}

%Scikit-Learn provides a transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if you don’t want 0–1 for some reason.”
Standarization, on the other hand, does not bound values to a specific range, it subtracts the mean value and then divides it by the variance. 
The standarization scaling is performed with \emph{z-scaling}:
\begin{equation} \label{eq:scaling}
z_i = \frac{x_i -\mu}{\sigma_i}
\end{equation}
the set of variables $x_i, i=1..n$ are scaled to be centered around the mean ($\mu=0$) and have the same spread ($\sigma=1$) \footnote{Note that we may want to use a lognormal rather than normal as a reasonable distribution. The Benford law is pertinent here: in many naturally occurring collections of numbers, the leading significant digit is likely to be small. This law is reminiscent of Pareto or Zipf law of numbers.} %For such variables, perform a log transform, $z_i = \log x_i$}. 
The z-scale and log transform are examples of linear transformations but our dataset may need nonlinear transformations as well, for example discretize a continuous variable (discretize the age variable in bins of 5 years $70-75, 75-80, ...$). 

Although standarization is generally speaking preferred to min-max because because is less affected by outliers, the preferred scaling method in actual truth depends on the application. In principal component analysis, standarization is usually preferred over min-max scaling since we are interested in the components that maximize the variance.
On the other hand, min-max is preferred when using neural network algorithm that require data that on a 0-1 scale.

Another data preparation routine is coding continuous variables. Machine learning techniques are perfectly able to deal with continuous variables but the reason we still may be a good idea to discretize is that in doing so we will be helping the model to learn relationships between inputs and output. Binning can also provide statistical smoothing and robustness in the modeling process (The cut point of the variable can be calculated with the Gini index, entropy, chi squared or KS criteria).
A good praxis is to always encode the dataset as good as possible in order to simplify the model as much as possible.

\subsection{Missing data imputation}
\label{sse:miss}
Missing data is a real challenge for researchers of any field. This challenge is particularly acute for longitudinal datasets in which repeated measures on the same individual are collected across time. 

Multiple Imputation (MI) is a common technique used to imputed values that are missing in the dataset, new data are generated to fill this gap on the basis of existing data. In MI, instead of using a single value for each missing observation (simple or simple imputation), the MI procedure replaces each missing observation with a set of plausible values according to the uncertainty about the variable with missing values. 

Here we perform MI to predict values from a distribution that includes a random error term representing the waste distribution. This underestimates the standard errors, so the imputation is repeated several times in a loop that generates several data sets with different coefficients. Finally, the imputed data sets are combined to produce valid statistical inferences that reflect the uncertainty due to missing observations.
We used the Multivariate Imputation via Chained Equations (MICE) R Package for our multiple imputations problem with multivariate missing data \cite{buuren2010mice}. The method is based on fully conditional specification, where each incomplete variable is imputed by a separate model. The MICE algorithm is able to impute mixes of continuous, binary, unordered categorical and ordered categorical data. In addition, MICE can also impute continuous two-level data, and maintain consistency between imputations by means of passive imputation. 
%Additionally, many diagnostic plots can be implemented in order to check the quality of the imputations. Thus, the mice package contains functions to: i) inspect the missing data pattern; ii) impute the missing data m times, resulting in m completed data sets; and iii) diagnose the quality of the imputed values. Missing data can occur anywhere in the data. The mice package generates multiple imputations for incomplete multivariate data by Gibbs sampling. The algorithm imputes an incomplete column (the target column) by generating ’plausible’ synthetic values given other columns in the data. Each incomplete column must act as a target column, and has its own specific set of predictors. The default set of predictors for a given  target  consists  of  all  other  columns  in  the  data. For  predictors that are incomplete themselves, the most recently generated imputations are used to complete the predictors prior to imputation of the target column. A separate univariate imputation model can be specified for each column.  In addition, several other methods are provided and even the user can write their own imputation functions.

Our preliminary analysis allowed us to understand not only the distribution of the main variables but also to explore the nature of missing values. Nearly $10\%$ of data were missed, but no profiles of \emph{missingness}\footnote{Missingness referred to missing values in data collection. Missing data can occur because of nonresponse (no information is provided for one or more items) or attrition, i.e. dropout in longitudinal studies} were identified (i.e. the missingness spread over many individuals, variables and study visits). 
We conducted a MI procedure with fully conditional specification method implemented by the MICE algorithm in order to impute values as closer as possible to ideal predicted observations. The imputed values were generated on the basis of existing variables through five different databases, one for each study visit; a total of five imputation procedures were thereby conducted. Of note, those individuals who did not attend to any visit were excluded from the corresponding databases. The procedure was repeated up to five times to generate five data sets. The imputed data sets were analyzed using the usual procedure for complete data. Finally, the results of these analyses were combined to produce valid statistical inferences of data.

\subsection{Data leakage}
\label{sse:dl}
Data leakage describes the situation in which the data you are using to train the machine learning algorithm includes the very thing you are trying to predict. For example, if we are predicting conversion for any of the 5 years project and conversion in any given year is included as a feature.
Another example of data leakage is including the neuropsychological test data in the training data set. Since the diagnosis is based on the test results, failing to remove the test scores will lead to overfitting algorithm, that is, the algorithm will learn which combination of test scores result in one diagnosis or another. Thus, the output \emph{Does the subject converts to MCI?} would be already included in the input.

%Data leakage can occur in more subtle ways than those just described. For example in time series using features from the future not available for the current prediction using. 

%Detection of Multicollinearity
An effective way to eliminate leakage before building the model is to look for features highly correlated with the target. Multicollinearity, deals with the identification and removal of correlated variables, eliminate redundant variables is crucial for model building. We can do this in two steps, first remove variables that produce another one, for example if A = B + C + D we can remove B, C and D. 
In a second step we deal with pair wise collinearity which is not exactly the same as multicollinearity. For that we calculate the correlation between all pairs (correlation matrix) and remove those with a large value (above some threshold). To do this systematically, we can build a graph, in the node note the correlation with the target and in the edges the correlation value with the adjacent components. We will select variables with largest correlation with the target and remove its correlated variables. (page 211, \cite{wu2012foundations}).

%https://www.quora.com/Is-multicollinearity-a-problem-with-gradient-boosted-trees. Multicollinearity is not a real problem for prediction.  using stepwise strategies for selection of features. Here, the algorithm works greedily and it will simply not select at any step any feature that is redundant, so the problem does not arise.
%Multicollinearity is only a problem for inference in statistics and analysis

However, leakage can persist once the model is built, in that case we can look look for surprising feature behavior (e.g. very large information gains). The proof that there is leakage can be obtained by comparing the deployment performance versus the train and evaluation performance. Features that are removed to avoid leakage include, of course, the target variable (e.g. 'conversionMCI') and related variables that contain information about the conversion e.g. time to convert, diagnosis per year and all the neuropsychological tests scores (e.g. 'MMSE', 'Buschke memory test').
%Dummy features to remove: id, fecha nacimiento, fecha_visita

\subsection{Feature engineering}
\label{sse:fe}
Feature engineering is the process of transforming raw data into features that better represent the problem that we are trying to solve in order to improve the model accuracy on unseen data. 
Put in simple words, feature engineering asks: What is the best representation of the sample data?
In feature engineering we may create additional features, for example by combining features. The rationale of feature engineering is to bring into the modeling process the domain expertise. Importantly, feature engineering is beneficial to reducing overfitting. As it is also the case of data leakage, there is not an exact method to perform feature engineering. Future engineering is more an art than a science and requires a reasonable dose of intuition and a strong dose of experience.
%https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/

Feature selection addresses the problem of selecting a subset that are most useful to the problem. Feature selection algorithms may use a scoring method to rank and choose features, such as correlation or other feature importance methods. Regularization methods like Lasso and Ridge regression which are studied in Section \ref{sse:logreg} may also be considered algorithms with feature selection baked in, as they actively seek to remove or discount the contribution of features as part of the model building process. 
It is worth noting that success in predictive models depends on three things: the model selection, the dataset available and the engineered features. With well engineered features, it is possible to choose suboptimal models, and or suboptimal parameters and still obtain good results. Feature engineering requires domain knowledge in order to make informed decision about the selection of features that should included in the model. The result of this process -\textbf{Vallecas Index}- is described next. 


\subsection{The Vallecas Index}
\label{sse:vs}
% Variable Selection:
Feature engineering is what makes us distinguish attributes from features, attributes is just any column in our tabular dataset, a feature on the other hand is an attribute that is useful or meaningful to your problem. Feature engineering gets us closer to the final selection of features that will be used for prediction. 
Variable Selection consists of filtering the most important features and remove those that are not needed - feature selection and wrapper.


Note that it could be always possible to select all the features and let the model do the job of finding the best predictive model, but this is not a good idea in practical terms and the reason is \emph{stability}. Stable solutions require removing features that have spurious or non consistent relationship with the target. Furthermore, the principle of parsimony must apply: careful variable selection and elimination tend to produce simpler models and \emph{ceteris paribus} simple should be preferred to complex.

The group of features that we will use for learning are the following 56 \footnote{For a description of the features see the Appendix} .The features have been selected according to the domain expertise of the team of researchers at \emph{Vallecas project} and the data desiderata for machine learning projects that was explained above. 

The \textbf{Vallecas Index} is a subset of the input space used to build classifiers that predict the likelihood that a new example will convert to MCI. 
%\begin{center}
%\textit{sex, education (nivel educativo),apoe, edad visita, scd (aggregate of 9 features related to subjective cognitive complaints), cognitive complaints (aggregate of 15 features), psychiatric syndromes (aggregate of 5 features), cognitive performance (aggregate of 11 features), quality of life (aggregate of 10 features), social engagement (aggregate of 4 features), physical exercise (aggregate of 2 features), diet(aggregate of 14 features), intellectual activity (aggregate of 14 features), demographics (aggregate of 6 features: married/singles, sons, perceived socioeconomic status), professional life(aggregate of 3 features), health (aggregate of 15 features: smoker, cardiac,lipids, glucose,diabetes, ictus, heart.. ), psychiatric history(aggregate of 15 features: depression and anxiety), sleep (aggregate of 12 features), family dementia history (aggregate of 4 features), sensory disturbances (auditive and visual disturbance) } 
%\end{center}
\begin{quote}
'sexo', 'lat manual', 'edad', 'edad ultimodx', 'sue noc', 'sue rec', 'imc', 'a02', 'a03', 'a08', 'a11', 'a12', 'a13', 'a14', 'numhij', 'sdvive', 'sdeconom', 'sdresid', 'sdestciv', 'sdatrb', 'hta', 'glu', 'lipid', 'tabac cant', 'cor', 'arri', 'card', 'ictus', 'tce', 'tce con', 'dietaketo', 'dietaglucemica', 'dietasaludable', 'renta', 'nivel educativo', 'educrenta', 'physical exercise', 'familiar ad', 'scd visita1', 'preocupacion visita1', 'act aten visita1', 'act orie visita1', 'act mrec visita1', 'act visu visita1', 'act expr visita1', 'act comp visita1', 'act apat visita1', 'gds visita1', 'stai visita1', 'eq5dsalud visita1', 'eq5deva visita1', 'relafami visita1', 'relaamigo visita1', 'valcvida visita1', 'valsatvid visita1', 'valfelc visita1'
\end{quote}

\subsection{Descriptive statistics}
\label{sse:des}

Once we have identified the set of features we are interested in, we can proceed to study the associations between the features among themselves and more importantly with the target variable. This will give us an idea of the embedded structure in the dataset and will help us address questions in a most effective way. But prior to doing this, since our dataset is longitudinal, it is worth exploring the temporal evolution of features across the 5 years of the project. Of note, here we are performing exploratory data analysis, that is, we look for patterns of associations in the dataset without being concerned with prediction per se.

%Test on normality: https://medium.com/@rrfd/testing-for-normality-applications-with-python-6bf06ed646a9
%formal normality tests always reject on the huge sample sizes we work with today.hen n gets large, even the smallest deviation from perfect normality will lead to a significant result

First, we plot longitudinal variables for cognitive test (these are not included in the \emph{Vallecas index} to avoid data leakage) and self reported emotional state (included in the \emph{Vallecas index}). We also calculate the skewness of the distribution across time. For normally distributed data the skewness should be about 0, the variables shown in Figure \ref{fig:staigds} are skewed. 

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[keepaspectratio, width=0.9\linewidth]{figures/mmse}
        \caption{Mini–Mental State Examination (MMSE) scores across five years. The distribution is skewed on the right side with skewness values for years 1 to 5: $1.93, 1.96, 1.67, 1.80, 1.75$}
    \end{subfigure}
    
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[keepaspectratio, width=0.9\linewidth]{figures/fcs}
        \caption{Buschke Immediate Recall score. The distribution is left skewed and increases with time, the skewness values for each year are: $-0.52, -0.60, -0.92, -1.02, -1.12s$ }
    \end{subfigure}%\quad
    
     \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[keepaspectratio, width=0.9\linewidth]{figures/gds}
        \caption{Self-reported depression level by the volunteers for each year. The distribution is right skewed and tends to decrease with time, the skewness values for each year are: $1.93, 1.96, 1.67, 1.80, 1.75$ }
    \end{subfigure}
    
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[keepaspectratio, width=0.9\linewidth]{figures/stai}
        \caption{Self-reported anxiety level by the volunteers for each year. The distribution is right skewed and tends to decrease with time (with a strong increment in the last year) $0.67, 0.62, 0.52, 0.41, 0.74$}
    \end{subfigure}%\quad
    \caption{MMSE, Buschke, depression and anxiety levels across five years.} \label{fig:staigds}
\end{figure}

Next we study associations between features and the target (\emph{conversion to MCI}). 
Figure \ref{fig:demosalime} plots the conversion rate for different features: APOE, educational level, familiar AD (father or mother), income level calculated using residency and age and diet. Note that we explore not only features included in the \emph{Vallecas Index} also other features like APOE profile are included. 

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[keepaspectratio, width=\linewidth]{figures/demos}
        \caption{Conversion rate for different features: APOE, educational level, familiar AD (father or mother), income level calculated using residency and age.}

    \end{subfigure}
    
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/alim}
        \caption{Conversion rate for diet related features, ketogenic diet (high in protein and fat low in carbs), glucemic diet (high in carbs) and healthy diet (high in vegetables, fruit and low in fat)}
    \end{subfigure}%\quad
    \caption{Conversion rate for different features, above APOE, familiar AD, income and age, bellow diet.} \label{fig:demosalime}
\end{figure}

\textbf{Socioeconomic Risk Factors in AD}

Figure \ref{fig:demosalime} shows the conversion rate with education level and income in \emph{Proyecto Vallecas}. Top row second on the right indicates that higher rent and educational level reduce the incidence to conversion.
Figure \ref{fig:incomeresidency} shows the income distribution of \emph{Proyecto Vallecas} volunteers as estimated based on their home residency. Figure \ref{fig:multico} shows the Pearson's correlation between conversion to MCI and the features: cognitive complaints and depression (positive correlation) and income level-education and use of new technologies (negative correlation).

\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=\linewidth]{figures/incomeresidency}
        \caption{On the left, distribution of home residency by districts of \emph{Proyecto Vallecas} volunteers. All districts of the city of Madrid are represented. Note that there are 156 subjects of a total of 1180 that are not residents in the city of Madrid and are not included here. On the right, we plot the income distribution as estimated based on the home residency, 251 subjects live in low income areas, 769 in medium income areas and 160 in high income neighborhoods of Madrid.
        } \label{fig:incomeresidency}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=2.5in]{figures/jointplot_gds_visita1conversionmci}
        \caption{Self-reported depression level is positively correlated with conversion to MCI.}
    \end{subfigure}
    ~ 
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=2.5in]{figures/jointplot_scd_visita1conversionmci}
        \caption{Subjective memory cognitive complaints are positively correlated with conversion to MCI.}
    \end{subfigure}%\quad
    
     \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=2.5in]{figures/jointplot_educrentaconversionmci}
        \caption{Joint wealth and income level are negatively correlated to MCI.}
    \end{subfigure}
    ~ 
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=2.5in]{figures/jointplot_a13conversionmci}
        \caption{Reporting habitual use of new technologies and the Internet is negatively correlated to MCI conversion.}
    \end{subfigure}%\quad
   
    \caption{Pearson correlation between features correlated with conversion to MCI.} \label{fig:multico}
\end{figure}

Figure \ref{fig:heatmappearson} shows the heatmap with the correlation between conversion to MCI (last row) and features included in the \textbf{Vallecas index} (plus APOE). 
Importantly, the Pearson correlation coefficient indicates the strength of a linear relationship between two variables, and is therefore ill-suited to characterize more complex (non-linear) relationships.
Since our goal is to formulate questions based on the data patterns observed, the mathematical apparatus will necessarily get more sophisticated as we advance into the Modeling section \ref{se:mod}. 
 
 \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{figures/scd_visita1_corr}
        \caption{Heatmap calculated with the Pearson's correlation for the features (from bottom to top): conversionmci, imc, remember dreams, use of new technologies, APOE, income level, joint of education and income, depression and cognitive complaints. Cognitive complaints, depression and APOE are positively correlated to conversion and wealth-income and the use of new technologies are negatively correlated. 
        } \label{fig:heatmappearson}
\end{figure}

Figure \ref{fig:kendall} shows the Kendall's correlation (Kendall’s tau is a measure of the correspondence between two rankings) 
for all the variables included in the \textbf{Vallecas index}.

\begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{figures/kendallcorr}
        \caption{Heatmap calculated with the Kendall's correlation for a the total of all features selected in the \textbf{Vallecas index}. The heatmap is able to display significant associations (linear correlations) and the clustering of features.} \label{fig:kendall}
\end{figure}

\subsection{Statistical inference}
\label{sse:satinf}
%ANOVA TESTS
In this section we study the difference between groups using analysis of variance \footnote{ANOVA is similar to 2-sample t-test but it is more conservative (less type I error)}. In particular, we study whether the observed variance for a variable (e.g. age or diet) when partitioned into components can be attributable to conversion. Note that statistical tests such as $\tilde{\chi}^2$ and F-score estimate the degree of linear dependency between two random variables and that it is possible for two variables to be dependent and have zero covariance (linearly independent). Independence is a stronger concept than zero covariance which is only about linear dependency, for two variables to be independent we must exclude nonlinear relationships as well.%goodfellow pg 77

Table \ref{tab:anova} shows the ANOVA statistical test to determine whether differences between the means are of conversion to MCI and features are statistically significant. The null hypothesis is rejected for depression, subjective complaints, education level, income, familiar unit, use of new technologies and age. 

\begin{tabular}{llr}
\hline
\multicolumn{2}{c}{ANOVA conversionMCI} \\
\cline{1-2}
Feature    & F & Pr(>F) \\
\hline
sexo      & 0.096745   & 0.755843        \\
lat manual  & 0.294532 & 0.587463       \\
edad        & 8.081151 & 0.004573      \\
sue noc & 2.005981    & 0.157021       \\
sue rec & 4.04351     & 0.044635        \\
imc & 0.717026        & 0.397343        \\
a13 & 13.897684       & 0.000205        \\
relafami & 2.120896   & 0.145645        \\
sdvive & 5.583176     & 0.018343        \\
renta & 4.269972  & 0.039073      \\
nivel educativo & 8.281169     & 0.004099         \\
familiar ad & 1.008209       & 0.315598        \\
scd visita1 & 18.98007       & 0.000015       \\
gds visita1 & 9.116592       & 0.002603       \\
valfelc2 visita1 & 0.517526      & 0.472083       \\
dieta keto & 0.517526      & 0.472083       \\
dieta glucemica & 3.764671 & 0.052654 \\
tabac &  2.388573  & 0.122572 \\
\hline
\label{tab:anova}
\end{tabular}

Figure \ref{fig:anovabox} shows the box plot for features that reject the null hypothesis for cognitive complaints, age, education level and the use of new technologies.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=2.1in]{figures/scdbox}
        \caption{Box plot of ANOVA test for differences conversion to MCI and cognitive complaints.}
    \end{subfigure}
    ~ 
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=2.1in]{figures/edadbox}
        \caption{Box plot of ANOVA test for differences conversion to MCI and age.}
    \end{subfigure}%\quad
    
     \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=2.1in]{figures/educabox}
        \caption{Box plot of ANOVA test for differences conversion to MCI and education level.}
    \end{subfigure}
    ~ 
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=2.1in]{figures/a13box}
        \caption{Box plot of ANOVA test for differences conversion to MCI and use of new technologies (a13).}
    \end{subfigure}%\quad
    \caption{Box plot of 1-factor ANOVA tests for four features that reject the null hypothesis -cognitive complaints, age, education level and use of new technologies.} \label{fig:anovabox}
\end{figure}

\newpage
%\section{Predictive analytics with Machine Learning techniques} 
\section{Machine Learning} 
\label{se:mod}

Machine learning is changing numerous aspects in our lifes. Predictive algorithms are now capable of performing tasks that before would require an expert. The range of domains of applications in which machine learning have shown optimal results is impressive. Natural language translation (Google Neural Machine Translation), lip reading \cite{suwajanakorn2017synthesizing} and face  reconstruction of aging process using Conditional Generative Adversarial Networks \cite{antipov2017face}, to just cite some recent achievements of Artificial Intelligence.
The success of recent models to detect abnormalities in medical images (outperforming trained radiologists) have caused  great excitement in the AI community. Google and University of Toronto leader researcher Geoffrey Hinton has gone as far as to advocate to stop training radiologists in medical schools.
%The paper documents how the algorithm detected abnormalities (like fractures, or bone degeneration) better than radiologists in finger and wrist radiographs. However, radiologists were still better at spotting issues in elbows, forearms, hands, upper arms, and shoulders.

Here we are interested in building learning systems that accurately predicts whether a subject will convert to MCI. The input set is the \emph{Vallecas Index} explained above. While at this stage we focus only on the predictive power of the \emph{Vallecas Index} which mainly consists in demographic, cognitive and lifestyles features, the solutions here implemented can be directly extended with additional features, including neuroimaging (e.g. gray matter density, connectivity) and others.

We will explore a number of different algorithms and we will evaluate the goodness of fit of those algorithms with the problem at hand -predict conversion to MCI from healthy subjects. We are, methodologically speaking, promiscuous for a reason. There is no a machine learning algorithm that is universally any better than any other. This is related to the \emph{no free lunch theorem} in search and optimization \cite{wolpert1997no}. It follows that the goal of machine learning is not to seek an universal learning algorithm, instead the goal must be to understand what kinds of distributions are relevant to the real world that an AI agent is experiencing. Thus, we must content ourselves to develop algorithms that perform well in a particular task and we do this by adding preference hyperparameters \footnote{Hyperparameter is a parameter whose value is set before the learning process begins. By contrast, the values of other parameters are derived via training.} (e.g. regularization) in the algorithm.  

In the next section we deal with the didactic aspect and we provide a short introduction to machine learning, the reader familiarized with the topic can skip and go directly to the Results Section \ref{se:res}, where the models are implemented and discussed. 

\subsection{Introduction to Machine Learning} 
\label{sse:iml}
%chollet book pg216
% why machine learning
Machine learning is in essence a form of applied statistics where the emphasis is placed on estimate complicated functions rather than providing confidence intervals around those functions \footnote{The p-hacking crisis is absolutely irrelevant for scientists working on Machine learning and a symptom of the obsolescence of the statistical testing approach.}. Machine learning is not only interesting from an engineering point of view (fabricating a system with a desired behavior) but also from the standpoint of psychology because a principle-based approach of machine learning necessarily informs and is informed by the principles that underlie human intelligence.

What separates machine learning from optimization is that the optimization business is about reducing the training error while the machine business is about reducing the training error and most importantly, reducing the generalization or test error (expected error on a new input). 
Formally, we look for the model parameters $w$ that minimize the distance between the prediction and the ground truth

\begin{equation}
\argmin_{w} ||X^{(train)}w - y^{(train)}||
\label{eq:reaintest1}
\end{equation}

However, in reality what we actually care about is minimizing the test error, that is, how good our model predict the behavior of examples never seen before:
\begin{equation}
\argmin_{w} ||X^{(test)}w - y^{(test)}||
\end{equation}

But how is it possible to affect performance in the test set when we get only observations on the training set? If training and test sets were collected arbitrarily the whole enterprise of statistical learning would not stand. 
Let us see this with the simplest model of machine learning -linear regression. In a linear regression, the model is trained by minimizing the training error \emph{under the premise that both train and test test errors are identically distributed}, that is, both training and test are drawn from the same probability distribution. Thus, if we have a probability distribution $p(X,y)$ and we sample it repeatedly to obtain the training set and the test set, the expected error for either sets should be the same because both expectations are built using identical sampling process. The solution to the previous question must be clear now, the assumption that both the training and test sets come from the same data generative process is \emph{conditio sine qua non}.

There are four types of models in Machine Learning depending on the type of experience that the learning algorithm is exposed to. 
While the term experience may sound anthropoid the distinctive characteristic of a learning algorithm is precisely to improve its performance via experience. In this document we will deal with the two most important, supervised and unsupervised learning.

\begin{itemize}
\item Supervised learning: builds a map between input data to known labeled targets. Labeled targets or annotations act as a teacher.
\item Unsupervised learning: finds interesting transformations of input data (without the help of any teacher) for the purpose of compression, denoising, visualization or to better understanding the correlations in the input data. Unsupervised learning (eg. clustering, dimensionality reduction PCA) can precede supervised learning.
\item Self supervised learning: without humans in the loop, labels are generated from the input data (eg. autoencoders)
\item Reinforcement learning: model agent-environment interactions the goal is to learn actions that maximize some reward. For example, a network that looks at a video game screen and outputs the game actions that will maximize the score \cite{mnih2013playing}
\end{itemize}

Learning happens in two steps, first we build an estimator and then we fit the model to the data, the model performance can be then studied. The estimator may need to specify parameters and hyperparameters. For instance, a decision tree (Section \ref{sse:dectrees}) needs to specify the maximum depth of the tree, other models e.g. Naive Bayes (Section \ref{sse:naivebayes}) are less dependent on the choice of parameters. However, the easiness of parametrization does not come for free (\emph{no free lunch}). As a matter of fact, Naive Bayes assumes that each feature is conditionally independent of every other feature given the target category, $p(x_i|C) == p(x_j|C), \forall i,j$ which is quite unrealistic in our dataset. Furthermore, when dealing with continuous data Naive Bayes assumes also a Gaussian or normal distribution. 

Once the estimator is being learned, we need to test how good its predictions are. A predictor generates forecasts using the learned estimator fed with unknown data, that is, data that were not used to train the estimator.
% 0. Transformers : before learning, some are simple: replacing missing data with a contant, taking a log transform ..or some transforms are learning algorithms themselves like PCA
% The pipeline is: Read Data -> apply some simple or complex (PCA) transformation -> fit an appropriate model -> predict using the model for unseen data. (iteratively)
% For model tuning and selection we can use two meta estimators: GridSearchCV and RandomizedSearchCV to search the best parameters. Grid provides a grid of possible parameters and try each possible combination among them to arrive at the best one.. Randomized  optimizes this sampling the parameters to avoid combinatorial explosion of Grid. the also allow different x-validation schemes and score functions to measure performance.

In the rest of the section we will exhaustively explore machine learning models and evaluate their performance in the \emph{Vallecas Project} dataset. We start with supervised learning techniques (regression, kernel machine, nearest neighbors, naive Bayes, decision trees, ensemble methods and deep networks) to end the section with unsupervised techniques ()hierarchical clustering, PCA and manifold learning .
%The appendix section provides a more in depth description of the models implemented, in this section we will
%go through the models and give a layman's description of the models making emphasis in the applicability and pros and cons.

\subsection{Supervised Learning}
\label{sse:sup}
Supervised methods can be conceptualized as filters that try to maximize the mutual information between the features and the target. Prediction values have different interpretations depending on the task, e.g. classification (discrete target), regression (continuous target). 

A supervised learning model is a mathematical object that makes a prediction $y$ given $x$, one of the easiest incarnations of supervised learning is a linear combination of weighted input features:
\begin{equation}
y = w^Tx
\label{eq:reg1}
\end{equation}
%http://xgboost.readthedocs.io/en/latest/model.html
where $w \in R^n$ is a vector of parameters i.e. values that control the behavior of the system. The vector of parameters determine how each features affect the prediction, the input variable $x_i$ has a weight of $w_i$.
The learning algorithm are the undetermined part that the algorithm needs to learn from data. 

To find the best parameters we need to measure the performance of the model, this is the objective function. Objective functions have two terms, training loss and the regularization:
\begin{equation}
O(w) = L(w) + R(w)
\end{equation}
where O is the objective function, L is the training loss and R is the regularization. The training loss, $L()$, measures how predictive our model is on training data, for example if we use mean squared $L(w) = \sum_i (\hat{y_i} - y_i)^2$ but there are many other possible loss functions for example the $\log$ loss that will see in Section \ref{se:reslogreg}. The regularization term, $R()$, controls the complexity of the model which helps to avoid overfitting. Polynomial regression, contrary to linear regression can fit nonlinear datasets. It has more parameters than linear regression and it is therefore more prone to overfitting the training data (Learning curve is the graphical device we have to detect if the model if overfitting), so using regularization is adviced.

\subsection{Regression}
\label{sse:logreg}
The most common learning method is regression, it is then worth spending some time to explain this technique prior to get into more brave waters. 
%https://www.evernote.com/shard/s263/sh/574750b2-7b4a-4300-8c41-7e8e5150de18/f2c4ffea344f3dd11b3e2f5dcdd25633
There are two types of regression, linear regression and logistic regression. Logistic regression builds a function $f:R^n \to {0,1}$ and linear regression builds a function $f:R^n \to R$, thus a logistic regression is map from the n dimensional input to a discrete set of values e.g. $\{-1,1\}$ or $\{0,1\}$ and logistic maps the input to the set of real numbers. Logistic and Linear Regression can be used as a benchmark (baseline performance) against which more complex techniques can be compared. 
Both linear regression and logistic regression work better when attributes that are unrelated to the output variable are removed, attributes that are very similar (highly correlated) to each other should also be removed to reduce redundancy. Thus, feature engineering (Section \ref{sse:fe}) plays an important role in regards to the performance of not only regression but also the rest of methodologies explored further in this document. 

\textbf{Linear regression}

%A regression model can be built with  a logistic transform in order to get the probability of positive class and building a ranking score to rank the outputs is an example of classification \cite{patania2017topological}.
Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. In the context of prediction, linear regression aims to predict the continuous output, $\hat{y}$ as a function of the input variables $x$ each weighted by a coefficient $\hat{w}$ plus a bias term $\hat{b}$. The equation of a linear regression is \ref{eq:linreg}:
\begin{equation}
\hat{y} = \sum_i \hat{w_i}x_i + \hat{b_0}
\label{eq:linreg1}
\end{equation}
where x is the vector of explanatory variables, $x_i \in x$, $y$ is the dependent variable and $b_0$ is the bias (or intercept) and gives the slope of the line ($y$ when $x=0$). 
%Once we have built the model we need to train it which in essence consists in setting its parameters so that the model best fits the training set. Thus, we need to find a measure that tells us whether the model fits well the data, the most common measure is the root mean square error (RSME) but we use the mean square error (MSE) because the value that minimizes a function also minimizes square root.
%The MSE is a linear regression hypothesis on a training set X is

The Maximum Likelihood Estimate (MLE) is a general method to estimate parameters in statistical models including regression. In MLE we maximize the probability that a random input point will be classified correctly. In order to maximize the likelihood we use optimization processes. 
Technically speaking, solving for the minimum or maximum of some function can be interpreted as an optimization, that is, finding 
\begin{equation*}
x* = \argmin f(x)
\end{equation*}
 
Optimization in the case of linear regression is looking for the linear model that provides the "best" fit to the data -minimizing the total amount of unexplained variation in the data.
Ordinary least squares (OLS) which is a particular instance of MLE is a common method for estimating the unknown parameters in a linear regression model. OLS works by defining the "best" model as the one that minimizes the residuals which are the part of the data that aren't explained by the model. Other optimization techniques are Ridge regression, Lasso regression, Newton's method and gradient descent which will be seen in detail when we study nonlinear techniques.
%Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point.

There are two main ways to train a linear regression model (the simplest machine learning method), closed form and numerical form. The closed-form consists in using the Normal Equation to directly compute the model parameters that best fit. In the numerical form, the number of dimensions of the input set is too high to be solve the equation, "gradient-descent" an iterative optimization approach that gradually tweaks the parameters to minimize the cost function over the training set (eventually giving the same results as in the closed form). Variations of Gradient Descent (GD) are Batch GD, Mini batch GD and Stochastic GD.

The task we are trying to solve here, predicting conversion, consists in mapping input features into two labels, 0 non-converter and 1 converter, therefore it does not make sense performing linear regression. However, a linear analysis of the relationship between the target and the input features is easy to implement and provides interesting information. 

The results of the univariate feature selection algorithm are shown in Section \ref{se:reslinreg}. 
%based on univariate statistical tests

\textbf{Logistic regression}
%\label{sse:logreg}

Logistic Regression is a Machine Learning classification algorithm, the most basic, and is used to predict the probability of a categorical dependent variable. For example, a binary variable coded as 1 (e.g. converter) or 0 (e.g. non converter). Logistic regression just like linear regression computes a weighted sum of the input features (plus a bias term) but instead of outputting directly the results, it returns the logistic or the sigmoid of the result. The logistic function range is $[0,1]$ so it can be used to estimate the probability that a given instance belong to one class or another. 
%Logistic regression takes this a step further by running the output  of the linear combination of w and x through a non linear function (the sigmoid or logistic function $\sigma$). 

Some of the assumptions of logistic regression algorithms are: the dependent variable $y$ needs to be binary, only "meaningful" variables should be included in the model, the independent variables $x_i$ should be independent from each other (this happens very rarely certainly it does not in out dataset). Formally a logistic regression is solving the equation:  

\begin{equation}
\hat{y} = \sigma(\sum_i \hat{w_i}x_i +\hat{b_0})
\label{eq:logreg}
\end{equation}
%\begin{equation}
%\hat{p} = h_{\theta}(x) = \sigma(\Theta^{T} X)
%\end{equation}
Equation \ref{eq:logreg} represents the logistic model where the log-odds of the probability of an event is a linear combination of independent or predictor variables. The only difference between this and the linear regression in Equation \ref{eq:linreg1}) is the sigmoid function $\sigma$:
\begin{equation}
g(z) = \frac{1}{1+e^{-z}}
\label{eq:sigmoid}
\end{equation}
%that is, there is no Normal Equation as it is the case of the MSE and the regularization cost functions
For ease of calculation the loss function of logistic regression is the \emph{log loss} function (maximizing the likelihood is the same as maximizing the $\log$ likelihood). There is no known closed function for the \emph{log loss}, however, the good news is that the \emph{log loss} function is convex, so it is guaranteed that stochastic descent (or any other optimization algorithm) will find a global minimum given that the learning rate is not too large and we have time to wait the model reaches the minimum. \emph{Log loss} quantifies the accuracy of a classifier by penalizing false classifications, thus, minimizing the \emph{log loss} is the same as maximizing the accuracy of the classifier.
Logistic regression performs binary classification, so let $P(y=1|x)$ be the probability that the binary output y is 1 given the input feature vector x and $P(y=0|x)$ the probability that the binary output y is 0. 
\begin{equation}
P(y=1|x) = \frac{1}{1+e^{-w^{T}x]}} \textit{ , } P(y=0|x) = 1 - \frac{1}{1+e^{-w^{T}x]}}
\label{eq:sigmoid2}
\end{equation}

Then, the log loss function $J(w)$ is 

\begin{equation}
\begin{split}
J(w) &= \frac{-1}{m}\sum_{i=1}^{m} y^i\log P(y=1) + y^i \log P(y=0) \\
&  \sum_{i=1}^{m} y^i \log	\frac{1}{1+e^{-w^{T}x]}} + y^i \log 1 - \frac{1}{1+e^{-w^{T}x]}}
\end{split}
\label{eq:sigmoid3}
\end{equation}
Note that for each instance only the term for the correct class actually contributes to the sum.
% Logistic regression estimates a multiple linear regression function defined as:
% \begin{equation}
% \log \frac{\hat{p}(y=1)}{1-(\hat{p}(y=1))} = \hat{b_0} + \hat{b_1w_1} + \hat{b_2w_2} + ...
% \label{eq:logreg2}
% \end{equation}
% Once the model has estimated the probability $\hat{p}$ that the input x belongs to the positive class, it can easily make the predictions: $\hat{y} = 0$, if $\hat{p} < 0.5$, $\hat{y} = 1$ otherwise. 

Adding independent variables to a logistic regression model will always increase the amount of variance explained in the log odds \footnote{A constant model that always predicts the expected value of y, disregarding the input features, would get a $R^2$ score of 0.0.}. However, adding dimensions in the model can result in overfitting, which reduces the generalizability of the model beyond the data on which the model is fit. Of note, logistic regression models just like any other linear regression model can be regularized using $l_1$ or $l_2$ penalties. 

A disadvantage of logistic regression is that we can’t solve non-linear problems since it’s decision surface is linear. Logistic Regression separates the input into two "regions" by a linear boundary, one for each class. Therefore it is required that data is linearly separable (See Figure \ref{fig:xor}). Another disadvantage of logistic regression is that it has high reliance on a proper presentation of the data, that is, which are the really important independent variables need to be known prior to build the model. 


\begin{figure}[H]
        \centering
        \includegraphics[width=0.4\textwidth,height=\textheight,keepaspectratio]{figures/xor}
        \caption{Linear classifiers cannot implement the XOR function. For example, if the black dots represent subjects that converter to MCI  and the white dots represent non converters a linear classifier is incapable to separate these two classes.} \label{fig:xor}
\end{figure}

The logistic regression model can be generalized to support multiple classes using the \textbf{softmax classifier}. The softmax classifier is a linear classifier that uses the cross-entropy loss function. When given an instance x, the Softmax Regression model first computes a score $s_{k}(x)$ for each class k, then estimates the probability of each class by applying the softmax function (also called the normalized exponential) to the scores
\begin{equation}
\hat{y}= argmax_{k} \sigma(s(x))_k
\label{eq:softmax}
\end{equation}
The cost function in Softmax is the cross entropy (for k=2 the cross entropy is the same as the log loss). Cross entropy measures the average number of bits you actually send per option. If your assumption is perfect, cross entropy will just be equal to the entropy (i.e., its intrinsic unpredictability). But if your assumptions are wrong, cross entropy will be greater than the Kullback–Leibler divergence. We mention the \textbf{softmax classifier} for completeness but we will not implement it because we our target is binary (converter-non converter).

In Section \ref{se:deep}, we expand on these ideas defining multi layer perceptron model (MLP) and Deep networks, that is, neural networks with at least one hidden later and therefore capable of solving the problem of classifying non linear distribution of data shown in Figure\ref{fig:xor}.

The implementation and the results obtained with the logistic classifier are discussed in Section \ref{se:reslogreg}. 

\subsection{Support Vector Machine (SVM)}
\label{sse:svm}

The term support vector machine (SVM), as it is commonly used, in reality mean one of three things:
\begin{itemize}
	\item Maximum margin classifier: a linear classifier with the maximum margin 
	\item Support vector classifier: an extension of maximum margin classifier to build more flexible classifier able to separate  non purely separable classes 
	\item Support vector machines: builds non linear decision boundaries to separate classes that cannot be separated with Support vector classifier  
\end{itemize}
Support vector classifier extends maximum margin classifiers and Support vector machines uses the kernel trick to extend Support vector classifiers. Here we will implement Support vector machines both linear and non linear, the working principles of SVMs are described next.

A support vector machine is in essence an hyperplane that maximizes the boundaries between two types of data points. Specifically, a Support vector classifier is a linear classifier with the maximum margin \cite{vapnik2013nature}, that is, it represents points so that the examples of the separate categories are divided by a gap (the "street" in Figure \ref{fig:svmstreet}) as wide as possible. This is called large margin classification. 
Prediction with SVMs is entirely determined (or “supported”) by the instances located on the edge of the street called the support vectors. Thus, predicting only involves the support vectors and adding more training instances “off the street” will not affect the decision boundary at all. 

\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=0.5\linewidth]{figures/svmstreet}
        \caption{The idea behind Support Vector Machine (SVM) is to fit the widest margin "street" between the classes. The optimization algorithm generate the weights for the optimal hyperplane (maximal margin). SVMs are sensitive to the feature scales therefore it requires that data are standarized or normalized prior to build the classifier, otherwise large values would define the support vector neglecting the small ones.
        } \label{fig:svmstreet} %http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf
\end{figure}

Linear SVM works by simply calculating a decision function $w^Tx +b$ (where $w$ is the parameters vector and b is the bias). Thus if the result of $f(x) = w_{1} x_{1} + ....+ w_{n}x_{n} + b_0$ is positive the predicted class $\hat{y}$ is the positive class or else is the negative class. In order to avoid overfitting, SVM looks for a function $f$ as \emph{flat} as possible. One way to ensure this is making the vector $w$ as small as possible e.g. minimizing the norm, $\argmin_{w} ||w||^2$. This minimization problem can be written as:

\begin{equation}\label{eq:svm}
\begin{aligned}
 & \textit{minimize} \frac{1}{2} ||w||^2 \\
 & \textit{subject to}=
 \begin{cases}
    y - wx -b \leq \epsilon \\
    wx +b - y \leq \epsilon
  \end{cases}
  \end{aligned}
\end{equation}

The tacit assumption in Equation \ref{eq:svm} is that a function $f$ that approximates all pairs $(x_i, y_i)$ with precision $\epsilon$ exists.
Thus, in essence, training a linear SVM classifier means finding the value of $w$ and $b$ that make this margin as wide as possible while avoiding margin violations (hard margin) or limiting them (soft margin). Thus, we do not care about errors as long as they are less than the margin $\epsilon$ but not accepting any deviation larger than this. 
 
SVM can perform both linear and non linear classification, regression and outlier detection. When the datasets are non linearly separable we can use nonlinear SVM classifiers. One tactic to separate no linearly separable datasets is is to add features so that the new space yields a linearly separable dataset (pg 259 \cite{geron2017hands}). The \textbf{Kernel trick} makes this possible, for a discussion of this very important concept, see \cite{vapnik2013nature}. 

Overfitting in SVM is tackle via expanding or narrowing the margin 'street' with the parameter $C, C > 0$ which fixes the trade-off between the flatness of $f$ and the amount up to which deviations larger than $\epsilon$ are tolerated \cite{smola2004tutorial} (large C defines a narrow margin and therefore likely overfitting). 

%“If your SVM model is overfitting, you can try regularizing it by reducing C.” large C margins are narrow and we may overfit, C small (C=1) larger margins. “Unlike Logistic Regression classifiers, SVM classifiers do not output probabilities for each class”
%LinearSVC same as SVC(kernel='linear') but SVC is musch slower specially for large datasets  also possible use 
% GDClassifier(loss="hinge", alpha=1/(m*C)). This applies regular Stochastic Gradient Descent to train a SVM classifier but it doesnt converge as fast as the LinearSVC class

The implementation and the results obtained with this kind of classifier are discussed in Section \ref{se:ressvm}.

\subsection{Naive Bayes Classifier}
\label{sse:naivebayes}
%https://www.deeplearningbook.org/contents/ml.html pg 96
The ideal model is an oracle that knows the true probability distribution that generates the data, $p(x,y$. But even if we had the oracle we  the distribution will contain noise. For example, the mapping $x \to y$ is inherently stochastic (observation error) or the mapping is deterministic but there are latent variables not included in $X$ that affect $y$. Bayes error is the irreducible error, that is, the lowest possible prediction error incurred by an oracle making predictions from the true distribution $p(x,y)$. For example, we know that the process of flipping a coin can be generated with a binomial distribution but if we are tasked to predict the outcome we will make errors because the process in inherently stochastic \footnote{Tossing a coin is in reality a deterministic process obeying laws of physics, therefore with the "correct" motion equations we could predict the outcome.}


%https://www.evernote.com/shard/s263/nl/33458921/18793f59-d0f8-40c0-b1a5-6f51bab38c60?title=Naive%20Bayes%20Classifier
The naive Bayes \footnote{Thomas Bayes hardly deserves the glory, Laplace had not only the intuition but provided the mathematical formulation. This unfairness in author attribution is not rare in science and human affairs, e.g. Amerigo Vespucci naming America.} classifier determines to which class a sample belongs to by calculating the posterior probability using the Bayes formula,
\begin{equation}
p(y|x) = \frac{p(y)p(x|y)}{p(x)}
\label{eq:bayesrule}
\end{equation}
where x is the sample and y the target class. The Bayes rules (Equation \ref{eq:bayesrule}) combines two sources of information -the prior $(p(x))$ and the likelihood $(p(y|x))$- to calculate the posterior $(p(x|y))$ which is the final classification criterion. 
The likelihood of the features $x_i \in x$ is assumed to be Gaussian and the parameters $\sigma_{y}$ and $\mu_{y}$ are estimated using maximum likelihood estimation:
\begin{equation} 
P(x_i|y)= \frac{1}{\sqrt{2\pi\sigma_y^{2}}}e^{-\frac{(x_i-\mu_y)^2}{2\sigma_y^2}}
\label{eq:naiveb}
\end{equation}

If the posterior probability (posterior = prior $\times$ likelihood) of x being a converter is larger than the posterior of x being a non converter, then we classify x as a converter. Let us see this with an example, given a subject \emph{x} with the features values 
\begin{equation*}
x_1 = 0, x_2=30, x_3=12
\end{equation*}
where $x_1$ is APOE, $x_2$ is the mini-mental score (MMSE) and $x_3$ is the number of school years, then we need to calculate the 
posterior probability of converting, $y=1$:
\begin{equation*}
p(y=1|x_1,x_2,x_3) = p(x_1|y=1) * P(x_2|y=1) * P(x_3|y=1) * P(y=1)
\end{equation*}

The assumption is that the features (also called predictors or inputs $x_i$) are independent, and this is why we can easily calculate the likelihood (e.g. $P(x_i|y_j)$) as a product of conditional probabilities shown in the above equation. Note that this assumption is quite strong, it means that the presence of a feature in a class is unrelated to the presence of other feature, for example in the previous example it would imply that having large MMSE and the years of school are uncorrelated.

When the assumption of independence holds, a Naive Bayes classifier tends to perform better than other models such as logistic regression with the advantage that naive Bayes needs less training data to perform well in case of categorical input variables compared to numerical variable(s). %For numerical variables, normal distribution is assumed (bell curve, which is a strong assumption).
Bayes tend to be efficient in learning and prediction but poor in generalization compared with more sophisticated methods.

The implementation and the results obtained with the naive Bayes classifier in the \emph{Vallecas} dataset are discussed in Section \ref{se:resnaivebayes}.

\subsection{k-Nearest Neighbors}
\label{sse:kneighbors}

The k-nearest neighbors algorithm (k-NN) is a non-parametric \footnote{KNN makes no explicit assumptions about the functional form of the mapping $h:X->y$ this is contrary to what happens in for example, Naive Bayes where it is assumed that data follow a Gaussian distribution} instance-based algorithm used for classification and regression. In classification, a point is classified by a majority vote of its neighbors -the point will belong to the most common class amongst its k neighbors. If $k=1$ the point is simply assigned the same class of ts closest neighbor. The best choice of k depends upon the data and the empirically optimal k can be calculated with the bootstrap method \cite{steele2009exact}. In general, larger k reduces effect of noise on the classification. In binary classification (eg. converter vs non converter) it is helpful to choose k to be an odd number as this avoids tied votes.
%k-NN breaks the input space into box looking regions.

k-NN is a type of instance-based learning or lazy learning, where the function is only approximated locally and all computation is deferred until classification \cite{keller1985fuzzy}. The k-NN algorithm does not learn during the training phase, it rather works by comparing the given test observations with nearest training observations during the testing/evaluation phase. This learning 'on-the-fly' makes it computationally inefficient especially for large dimensionality datasets.
It warrants noting that scaling and feature selection must be carefully done because the accuracy of the k-NN algorithm can be severely degraded by the presence of noisy or irrelevant features. 

k-NN works as follows, given a training set $X$ with labels $Y$ and an instance $x \in X$, $y \in Y$ to be classified, the algorithm first finds the set of most similar instances to $x$, called $x_{NN}$. Next, it predicts the labels $y_{NN}$ for $x_{NN}$ and finally predicts the label for $x$ by combining the labels $y_{NN}$ simply using majority vote. See Figure \ref{fig:knnwiki} for an example . 

\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=0.3\linewidth]{figures/KnnClassification}
        \caption{How the k-NN classifies the green dot depends on the k parameter. If $k=1$, the green dot would be classified as red because the closest neighbor is red. If $k=3$ (solid circle) the green dot will be classified as red for majority vote. If $k=5$ the majority vote of the five nearest neighbors (dashed circle) will yield blue (\cite{wiki:knn}).
        } \label{fig:knnwiki} 
\end{figure}


k-NN requires minimal training but relies upon expensive testing and can be used as a benchmark for more complex classifiers such as artificial networks, which contrary to k-NN, have lengthy training phase, albeit a fast testing phase. 
%https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/
k-NN can suffer from skewed class distributions, that is, if a certain class is very frequent in the training set it might perform poorly because the majority voting it will tend to dominate the response. Finally, the accuracy of k-NN can be severely degraded with high-dimensional data because with high density there may be very little difference between the nearest and farthest neighbor.

%https://www.coursera.org/learn/python-machine-learning/lecture/I1cfu/k-nearest-neighbors-classification-and-regression
An important weakness of k-NN is that it cannot learn whether one feature is more discriminative than another. This as will see in the Results section it is an important disadvantage for the k-NN classifier in the \emph{Vallecas} dataset which is unbalanced.
The implementation and the results obtained with this kind the k-nearest neighbors algorithm classifier are discussed in Section \ref{se:reskneighbors}.

\subsection{Decision trees}
\label{sse:dectrees}

Decision trees (DT) are a popular supervised learning method can be used for both regression and classification.
In essence, decision trees learn a set of conditional rules (if-then) on features values that result in predictions, so the idea is to find a set of rules useful to categorize an object. Although this is reminiscent to expert systems developed in the 70s and 80s, expert systems required the human expert's input to specify rules, in a decision tree the algorithm automatically learns the rules for every task.

DT are nonparametric models and contrary to linear models, decision trees make very few assumptions. It is worth noting that nonparametric is a misnomer, it does not mean that a DT does not have parameters, rather the opposite it has a lot, it rather means that the parameters are not determined prior to training. Thus, the tree adapts to the data by fitting to the data samples (often overfitting). A parametric model e.g. a linear model has a predetermined number of parameters reducing thus, the risk of overfitting but increasing the risk of underfitting. In order to avoid overfitting in DT it is important to restrict the adaptive or sticky properties of decision trees using regularization, for example the max depth hyperparameter will reduce the risk of overfitting.
% (by default is none ie. no limitation, so likely overfit).

Each node in a DT is associated with a region in input space which is then divided into non-overlapping regions with a 1-1 correspondence between leaves and input regions. The decision boundaries are calculated with entropy measures such as the Gini coefficient which gives the impurity of a tree node and is calculated as:

\begin{equation} 
G_i = 1- \sum_{k=1}^{n} p_{i,k}^2
\label{eq:gini}
\end{equation}
where $p_{i,k}$, is the ratio of instances of class k in node i. A node is pure if Gini is 0 i.e. all training instances belong to the same class. The Gini coefficient is not the only possible measure, Shannon's entropy can be used and will produce very similar results. Gini is a bit faster than Shannon's entropy but Gini tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce more balanced trees. %criterion='entropy'
%However, other algorithms such as ID3 can produce Decision Trees with nodes that have more than two children”
 
Decision trees are nonparametric models (the number of parameters is not known in training time) that is to say, the model will have as many parameters as it needs to fit the data. It follows that if left unconstrained, the tree will fit the data very closely, and most likely overfitting. This is an important difference with the previous models seen above (logistic regression, SVM and naive Bayes) which are parametric (limited degree of freedom) models. 
The main concern in Decision Tree is then to reduce the degrees of freedom to avoid overfitting. The parameter $max\_depth$ regularizes the. model and reduces the risk of overfitting.

We use the \emph{sklearn} implementation of decision trees algorithm - the Classification and Regression Tree (CART) algorithm- to produce binary trees (yes/no answers) (Equation \ref{eq:cart}). However there are other algorithms that produce trees with more than two children, e.g. ID3. A decision tree can also estimate the probability that an instance belongs to a class (SVMs do not have this feature).
% tree_clf.predict_proba
The CART algorithm is \textbf{greedy}: it looks for the optimum split at the top level but does not care if the split will lead to lower impurity in the downstream levels. CART does the optimum split at the top and never thinks twice about the consequences down the road \footnote{Trickle down economics}. 
CART does not guarantee to find the optimal solution, it splits the training set using feature k and threshold $t_k$, the algorithm chooses then the pair $(k,t_k)$ by looking for the split that produces the purest split weighted by the size, so the cost function that the algorithm minimizes has an impurity term factored by the size term for both right and left side. The cost function is: 

\begin{equation}\label{eq:cart}
J(k,t_k) = \frac{m_{left}}{m}G_{left} + \frac{m_{right}}{m}G_{right}
\end{equation}
where J is the cost function, $G_{L/R}$ measures the impurity of left/right branch and $m_{L/R}$ is the number of instances in the left/right branch.  
The algorithm stops recursing once it reaches the maximum depth (defined by the $max\_depth$ hyperparameter) or if it cannot find a split that will reduce impurity. 

Finding the optimal tree is an NP-problem (cannot be solved in polynomial time). Prediction in decision trees is however fast, a prediction requires traversing the tree form the root to a leaf. Since trees are approximately balanced it will go through $\log_{2}m$ nodes and for each node it needs to check only the value of one feature, the overall complexity is $O(\log_{2}m)$. The complexity is independent of the number of features and therefore decision trees scale very good.
%“Unfortunately, finding the optimal tree is known to be an NP-Complete problem: it requires O(exp(m)) time, making the problem intractable even for fairly small training sets. This is why we must settle for a “reasonably good” solution.”
%“Since each node only requires checking the value of one feature, the overall prediction complexity is just $O(\log_{2}(m))$, independent of the number of features. So predictions are very fast, even when dealing with large training sets.”
%“However, the training algorithm compares all features (or less if $max_features$ is set) on all samples at each node. This results in a training complexity of O(n × m log(m)). For small training sets (less than a few thousand instances), Scikit-Learn can speed up training by presorting the data (set presort=True), but this slows down training considerably for larger training sets.”

When the tree keeps adding rules it may become very complex, the tree can start to memorize rather than learning  and it will overfit \footnote{Overfitting happens in every machine-learning problem, the dialectics of Machine learning is in optimization versus generalization. Optimization refers to adjusting the model in order to get the best possible performance in the training set and generalization is about how well the trained model performs on data never seen before}. To avoid this we can prevent the growth of the tree using the max depth parameter which controls the maximum number of splitting points (eg 3) and minimum-maximum samples leaf which is the number of data instances that a leaf can have to avoid further splitting. In practice, the max depth parameter is enough to deal with overfitting.

The pros of DTs are that they are simple to understand and interpret, no need of feature normalization or scaling and work well with databases with heterogeneous data types. 
DTs have at least two weaknesses: overfitting and instability. Decision trees, especially complex trees very deep and/or wide, tend to overfit. The orthogonality of the decision boundaries makes them sensitive to dataset rotation making as as consequence DTs are very sensitive to small variations in the training data.
One solution to the instability problem is to use PCA which often results in a better orientation of the training data. In the next section (\ref{sse:ensemble}) we show ensemble methods such as Random Forests that might help to alleviate the overfitting problem by averaging predictions over many trees.

The implementation and the results obtained with decision trees are discussed in Section \ref{sse:resdectrees}.

\subsection{Ensemble Methods}
\label{sse:ensemble}
How ensemble methods work can be understood by analogy as asking to a thousand people (experts) and then aggregate their answers. Likely, the response of the aggregate is better than the individual expert's responses. By the same token, the aggregation of many inaccurate predictors will give better answers than the individual predictions. Or going from analogy to allegory: \emph{Don't look at the tree it may be lying... but to the forest!}. 

Ensemble Learning refers to the learning process in which the predictors define an ensemble. It goes without saying that ensemble methods make sense only once individual methods e.g. decision tree, SVM, logit etc. are available. For example, an ensemble of decision trees or an ensemble of logistic regressors will produce as many individual predictions as elements the ensemble has. The classification problem can be solved by counting votes i.e. the winning class will be the one with the most votes. A majority vote is the decision process used in hard voting classifiers. Hard voting classifiers often achieve higher accuracy than the best individual classifiers in the ensemble. 
Soft voting consists in having some classifiers more trustworthy than others in the ensemble, the votes of the better classifiers will have more weight than the votes of the weaker ones. Thus, soft voting classifiers require to be able to estimate class probabilities.
%\footnote{SVC does not estimate class probabilities but still you can use the SVC class for soft voting, just set the probability hyperparameter to True (this will make the SVC class use cross validation to estimate class probabilities, slowing down training, and it will add a predict$\_$proba().} 
Soft voting often achieves better performance than hard voting because it gives more weight to highly confident votes. To sum up, soft voting ensembles are better than hard voting ensembles and these in turn are better than single predictors.

%JUST replace voting="hard" with voting="soft" and ensure that all classifiers can estimate class probabilities”
Ensemble classifiers are almost always a good choice because even if individual classifiers are weak learners (do slightly better than random guessing) the ensemble can still be a strong classifier, provided there are a sufficient number of weak learners and they are sufficiently diverse. % solo no puedes con amigos si   https://www.youtube.com/watch?v=Ds7tje_Y0CM
The reason why ensemble methods work is the \emph{Law of large numbers}. For example, for a coin that is slightly biased, let us say that has $51\%$ probability of heads and $49\%$ of tails, if you toss the coin 1,000 times chances are that you will get a majority of heads, something close to 510 heads and 490 tails. The (frequentist) probability of heads majority for 1,000 tosses is $75\%$ and the more tosses you do the closer you get to 1, for 10,000 tosses the probability of majority is $97\%$. Thus, the more you toss the coin the closer you will get to 51-49 ratio. In the same vein, you can build 1,000 classifiers that are individually correct let us say $51\%$ of the time, (barely better than random guessing) but if you predict the majority vote class you can hope for a $75\%$ accuracy! Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diversity is using very different algorithms so they will make different types of errors improving the accuracy \cite{geron2017hands}. Generally speaking, aggregation has a similar bias but lower variance than individual predictors.
%YS connec this with utility function Ole Peters

We can distinguish two main types of ensemble methods depending on whether variability is introduced in the models or in the data. An example of the former is a voting classifier (ensemble of different individual classifiers) (implemented in Section \ref{sse:resmajvot}). We can also use the same training algorithm for every learner and train them on different subsets of the dataset, we can distinguish between bagging (sampling with replacement) or pasting (sampling without replacement) (implemented in Sections \ref{sse:resrf} and \ref{sse:resgradboosting}). 
%Once all predictors are trained, the ensemble will make a prediction for a new instance by simply aggregating the individual predictions (majority for classifiers or mean for regression). 
 An important advantage of ensemble methods i.e. bagging and pasting is that they scale very well because predictors can be trained and make predictions in parallel (multicore GPU).
%Bagging and Pasting in sklearn is BaggingClassifier (BaggingRegressor) “BaggingClassifier automatically performs soft voting instead of hard voting if the base classifier can estimate class probabilities”
%“the ensemble’s predictions will likely generalize much better than the single Decision Tree’s predictions: the ensemble has a comparable bias but a smaller variance (it makes roughly the same number of errors on the training set, but the decision boundary is less irregular)” Excerpt From: Aurélien Géron. “Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems.” iBooks. 
%Examples of ensemble methods are bagging, boosting, stacking and random forests.

%Table \ref{tab:ensemblemethods} shows a classification of the ensemble that are implemented in Section \ref{se:resensemble}

\captionof{table}{Ensemble methods implemented: majority vote -combination of different learners (logit, SVM etc.) learning on the same dataset and bagging and boosting methods which use the same type of learners lo learn different data chunks. Boosting builds a sequence of learners so that later learners in the chain learn harder problems. An additional ensemble method not implemented here is Stacking, new model is \emph{blent} on top of other predictors defining a meta learner.} \label{tab:ensemblemethods} 
\begin{tabular}{ |p{5cm}|p{5cm}|p{5cm}| } %\label{tab:ensemblemethods} 
%\caption{Table}
 \hline
 \multicolumn{3}{|c|}{Ensemble Methods} \\
 \hline
 Majority Vote & Bagging & Boosting \\ %& Stacking
 \hline
 Different type of learners for the same data. \emph{Majority vote (\ref{sse:resmajvot})}& Same type of learner trained with different data chunks, sampled with replacement. \emph{Random Forest (\ref{sse:resrf})} & Same type of learner trained with different data chunks, sampled without replacement. Subsets contain elements misclassified by previous models. \emph{AdaBoost, XGBT (\ref{sse:resgradboosting})} \\ 
 \hline

\end{tabular}

\subsubsection{Bagging (Random Forest)}
\label{sse:rf}
%“Instead of building a BaggingClassifier and passing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees1”
Bagging short for Bootstrap Aggregating is a way to decrease the variance of the predictions by generating additional data for training using combinations with repetitions to produce multisets of the same cardinality/size as the original data.
%https://stats.stackexchange.com/questions/18891/bagging-boosting-and-stacking-in-machine-learning
Random Forest is an ensemble of decision trees generally trained via the bagging method (combinations with repetitions), typically with max samples set to the size of the training set. Random forest is a bagging algorithm because it tries to "fix" the reliability problem existing in the decision tree model. Decision trees are unreliable or unstable in the sense that small changes in the input  produce may produce very different decision trees. This is where bagging comes from, we can create a robust model through bagging, that is, create a multiset of data by resampling the original dataset. Each tree will deal with a different set of the data called the bootstrap sample chosen at random with replacement. This means that a bootstrap sample may have missing instances and also repeated instances. We do this N times (one for each tree). Importantly, there is not only randomness in picking from the multiset for each tree but also there is randomness on the features from which to decide to split or not.  
Thus, the Random Forest algorithm can introduce extra randomness when growing trees; instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features. This results in greater tree diversity,  trading a higher bias for a lower variance \cite{geron2017hands}. Once the random forest is trained, the overall prediction is done weighting votes. %, generally yielding an overall better model
%YS: last sentence kind of literal chnage

%If the max features is 1 we have forests with diverse complex trees and with max features closed to the number of features will lead to similar forests with simpler trees. 
Random forest is a solution to the stability problem in decision trees. Not surprisiogly, random forest inherit many of the strengths and weaknesses of decision trees. Pros: widely used with excellent performance on a variety of problems, easily parallelized, does not require careful normalization of features. Cons: it may not be good for very high dimensional problems (text classifiers). 
%Some of the k parameters are n estimators (default 10) is the number of trees, the max features parameters has a large effect on performance influence the diversity of trees in the forest, the max depth by default is None which means that the tree will continue to split until all nodes (or the min samples which is 2) in the leaf belong to the same class.
%https://github.com/mapattacker/datascience/blob/master/supervised.rst

%“The Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node (see Chapter 6), it searches for the best feature among a random subset of features. This results in a greater tree diversity, which (once again) trades a higher bias for a lower variance, generally yielding an overall better model. The following BaggingClassifier is roughly equivalent to the previous RandomForestClassifier:”
%we can achieve this also with “BaggingClassifier(DecisionTreeClassifier(splitter="random”
%rnd_clf.feature_importances_


\subsubsection{Adaptive and Gradient Boosting (AdaBoost,Gradient Boosting)}
\label{sse:ada}

Boosting is any ensemble method that combines weak learners into a strong learner. The idea is to train predictors sequentially each trying to correct its predecessor. It is a two-step approach, first uses subsets of the original data to produce a series of averagely performing models and then "boosts" their performance by combining them together using a particular cost function (majority vote). Unlike bagging, in boosting the subset creation is not random and depends upon the performance of the previous models: every new subsets contains the elements that were (likely to be) misclassified by previous models.
%https://stats.stackexchange.com/questions/18891/bagging-boosting-and-stacking-in-machine-learning
While bagging reduces variability (stability of the classifier) boosting reduces both variance and bias. Boosting reduces variance because it uses multiple models as we do in bagging and it reduces bias by training the subsequent model by telling him what errors the previous models.

Boosting was originally a theoretical invention motivated by the question \emph{Can we build a stronger model using weaker models?}. Note that by weak learner we mean shallow trees built in a non random way (different from RF) to create a model that makes fewer and fewer mistakes as these are added \cite{schapire1990strength}. 
Why does boosting ensemble work? We don't really know, but we must reckon that data science is an empirical science and "because it works" might be good enough. This is a far from a \emph{boutade}, epistemologists of all sorts have been kept themselves busy for centuries. See for example, the \emph{As if} argument in economics by Milton Freedman using the analogy of the billiard player who plays “as if he knew ... complicated mathematical formulas”. For Friedman a scientific theory (hypothesis or formula) cannot be tested by testing the realism of its assumptions. All that matters is the accuracy of a theory’s predictions, not whether or not its assumptions are true \cite{friedman1953essays}. %http://www.rweconomics.com/BPA.htm

AdaBoost (short for adaptive boosting) and Gradient boosting are the most common boosting, let us briefly see how they work.

\textbf{AdaBoost algorithm}
AdaBoost is the original boosting algorithm and is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. 
The individual learners can be weak, but as long as the performance of each one is slightly better than random guessing, the final model can be proven to converge to a strong learner. Gradient boosting trains each subsequent model using the residuals (the difference between the predicted and true values). The learning rate parameter controls how hard each tree tries to correct the mistakes from previous rounds (high learning rate complex trees, low learning rate simpler trees). 

The idea is having a chain of predictors where deeper in the chain predictors focus on harder instances (instances where the predictor underfit). The difference between AdaBoost and Gradient Descent is that in Ada we don't tweak the hyperparameters of any individual predictor, it rather adds predictors to the ensemble. Once all predictors are trained, the ensemble makes predictions very much like bagging or pasting, except that predictors have different weights depending on their overall accuracy on the weighted training.

The drawback of Boosting is that it is sequential and therefore it can't be parallelized so it does not scale well because we need the previous predictor the train the next one.
%“If your AdaBoost ensemble is overfitting the training set, you can try reducing the number of estimators or more strongly regularizing the base estimator.”

\textbf{Gradient Boosting algorithm}
Gradient Boosting often present the best off-the-shelf accuracy on many problems, require not much memory and are fast but like random forests the results are hard for humans to interpret. Gradient Boosting require careful tuning of the learning rate.
Gradient Boosting is also sequential, but rather than tweaking the weights of the instances, Gradient Boosting tries to fit the new predictor to the residual errors.
% Extreme gradient boosting
%http://xgboost.readthedocs.io/en/latest/model.html
%https://github.com/dmlc/xgboost
The optimization problem of decision trees (forest) is much harder than than traditional optimization problem like back propagation which just do gradient descent. Since it is not easy to train all the trees at once we can use an additive strategy: fix what we have learned, and add one new tree at a time. 
Like Random forest, Gradient Boosting creates an ensemble of decision tree. An interesting property of Gradient Boosting Decision Trees (GBDT) is that it works with different loss functions, even when the derivative is not convex (eg pinball loss function). 
%https://www.lokad.com/pinball-loss-function-definition
 %There are two main algorithms: Adaboost and Gradient boosting. Adaboost is the original algorithm and what it does is to you tell subsequent models to punish more heavily observations mistaken by the previous models. In Gradient boosting you train each subsequent model using the residuals (the difference between the predicted and true values). The learning rate parameter controls how hard each tree tries to correct the mistakes from previous rounds (high learning rate complex trees, low learning rate simpler trees). 

The implementation and the results obtained with Ensemble learning methods including Random Forest are discussed in Section \ref{sse:resensemble}.

\subsection{Deep networks}
\label{se:deep}

A multilayer perceptron (MLP) is a feedforward artificial neural network. A MLP is perhaps the classic network architecture in Deep Learning. The term "deep" refers to having networks with more than one layer, the more layers the deeper the network.
Multilayer perceptron (MLP) can be used for both classification and regression. In order to understand how MLPs work it might be useful to expand on the regression model, previously seen in Section \ref{sse:logreg}. In regression we have just two layers, the input $x$ and the output $y$ and the model parameters are estimated via covariance matrix inversion. The goal of a regression model is to estimate the output $\hat{y}$ via OLS or related estimation methods for the weights $\hat{w}$ in the equation \ref{eq:linreg}
\begin{equation}
\hat{y} = \sum_i \hat{w_i} x + b
\label{eq:linreg}
\end{equation}
 
Graphical representations of the architecture of regression models -linear and logistic- are shown in Figure \ref{fig:mich}. 

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{.8\textwidth}
        \centering
        \includegraphics[height=1.7in]{figures/michiganfigure}
        \caption{Graphical representation of linear (left) and logistic (right) regression. In linear regression, the model coefficients $\hat{w,b}$ can be estimated through methods such as OLS, Ridge, Lasso etc. Logistic regression adds an additional non-linear function eg. a logistic function.}
    \end{subfigure}
    ~ 
    \begin{subfigure}[t]{.8\textwidth}
        \centering
        \includegraphics[height=1.7in]{figures/michiganfigure}
        \caption{MLP adds a hidden layer which computes a nonlinear function (activation function) of the weighted sums of the input features. Each hidden unit $h_i$ computes a non linear function of the weighted sum of the inputs, resulting in the intermediate output values $v_i$. The activation function \textit{tanh} is the hyperbolic tangent function which is related to the logistic function, other functions are, \textit{ReLU, logistic} etc. Rectified linear unit (ReLU) is probably the best choice: fast, simple (look like a hockey stick) and propagate gradients more effectively than sigmoid. The \textit{ReLU} activation function is the default in \emph{sklearn}. The output $y$ is the weighted sum of the intermediary outputs $v_i$.}
    \end{subfigure}%\quad
    \caption{Predicting $y$ in MLP requires the computation of a non linear function in the hidden units, this gives the network more expressive power (internal representations) and allows to perform more accurate predictions than in linear or logistic regression models, especially when the relationship between the inputs $x$ and the target $y$ is complex. On the other hand, more parameters need to be estimated (MLP will likely require more training data than linear/logistic regression).} \label{fig:mich}
\end{figure}
%https://www.coursera.org/learn/python-machine-learning/lecture/v4cs3/neural-networks  

It is the addition of hidden layers with the non linear function what adds superior expressive power to MLPs making possible that the network learns more complex patterns than linear and logistic regression models. Of course, this increase in representational capacity comes at the price of having more weights (model coefficients) to estimate, that is, we need more training data and computation compared to a linear model \footnote{Memory, data and time are the limitations that have been overcome. The crude limitation in memory in computers (Gates infamously said  640K is good for anyone) is not a problem anymore. Data is not excessively problematic neither now since there is too much of it. The major limitation these days is time, but not computing time but developer-engineer time, humans cost money while computation is almost free.}.

%https://www.wired.com/1997/01/did-gates-really-say-640k-is-enough-for-anyone/
A deep-learning model aka  deep network is a directed, acyclic graph of layers. The topology of a network defines a hypothesis space.
The components of neural networks are: layers (the architecture of the network), inputs and target, loss function (is the feedback process that allows learning), optimizer (tell us if we are effectively learning). The loss function is the difference (can be calculated in multiple ways) between the predicted y and the true y, it produces a loss score which is the input of the optimizer whose output will update the weights. 
Thus, what deep learning in particular and machine learning in general does is to search for useful representations of input data using the guidance of a feedback signal (loss function) within a predefined space of possibilities or hypothesis space. Note that the choice of the topology constraints the space of possibilities or hypothesis space to a specific series of tensor operations and what you ultimately expect to find is a good configuration of weight tensors. Picking the right network architecture is more an art than a science although one should always try to rely as much as possible on good practices.

The main components of a neural network (deep or not) are as follows.
\begin{itemize}
	\item Input data and target
	\item Layers
	\item Loss/objective function: is the feedback signal during learning (produces a number that we want to minimize during training) 
	\item Optimizer: determines learning: how the network will update the weights based on the loss
\end{itemize}
The fundamental data structure in a deep network is the layer. A layer is a processing device which takes one or more tensors and outputs one or more tensors. The state of the layer is given by the weights (tensor) learned with the optimizer (stochastic gradient descent).
2D tensors of shape (samples, features), is often processed by densely connected layers, also called fully connected or dense layers. Sequence data, on the other hand, are stored in 3D tensors of shape (samples, time steps, features), is typically processed by recurrent layers such as an LSTM layer. Image data, stored in 4D tensors, is usually processed by 2D convolution layers (Conv2D) \cite{chollet2017deep}.
%Chollet book pg 144

A schematic representation of the architecture of neural networks is shown in figure \ref{fig:cholletfig}.

\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=0.5\linewidth]{figures/chollet}
        \caption{Neural networks build maps of input data to predictions. The loss function is the feedback signal that tells us how good the model is doing. The difference between the prediction and the actual target is the loss value which is used by the optimizer to update the network's weight in order to produce a better prediction in the next iteration. The optimizer is a stochastic gradient descent (SGD) of some sort of other but the loss function changes depending on the task, for a two-class classification problem use binary crossentropy as loss function. 
        } \label{fig:cholletfig}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%ç
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Dimensionality reduction and Unsupervised learning}

Computers can deal with as many dimensions as they like, humans on the other hand are limited to 3. To visualize large (larger than 3) dimensionality datasets we need to reduce the number of dimensions to 3 or less. The rationale is that despite the apparent large dimensionality the intrinsic dimensionality is low, or at least lower. For example a 10 mega pixels video camera rotating to film some scene build a 10 million dimensional space and yet the images approximately lie in a 3D space (yaw, pitch, roll), this embedding is however complex and nonlinear.
%https://towardsdatascience.com/reducing-dimensionality-from-dimensionality-reduction-techniques-f658aec24dfe
Clustering is associated with unsupervised learning but it could be useful to exploit out domain specific knowledge useful to obtain  discriminant features (distributional clustering) \cite{guyon2003introduction}. Distributional clustering is related to information bottleneck \cite{tishby2015deep}, it searches for the solution that achieves the largest possible compression while retaining the essential information about the target.

Performing dimensionality reduction before clustering or even supervised learning is considered a good practice. The unsupervised learning problem can be stated as follows. Given a set of n observations $x_1, ..., x_n$ of a random p-vector X having joint density $Pr(X)$, we want to infer the properties of this distribution without the help of a "teacher" providing correct answers or degree of error for each observation. % \cite{}.

Here we will study unsupervised learning methods in the \emph{Vallecas} dataset, which for the sake of the argument will not be anymore a labeled dataset. The goal is then to learn in the absence of a "teacher" by building internal representations that reconstruct input data patterns.
In what follows we build a number of unsupervised algorithms to study clusters in sample space. We are looking for internal representations in input space and dimensionality reduction that compress features into meaningful ways, ideally producing a division between converters and non converters. We start with Hierarchical clustering (Section \ref{sse:hierarchical}), then we implement principal component analysis (Section \ref{sse:PCA}) and single value decomposition. Manifold learning will be also studied (Section \ref{sse:manifold}). 


\subsubsection{Hierarchical clustering}
\label{sse:hierarchical}

Hierarchical clustering involves creating clusters that have a predetermined ordering from top to bottom. Hierarchical clustering falls into two main strategies: 
\begin{itemize}
	\item Agglomerative (bottom up): each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy
	\item Divisive (top down): all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.
\end{itemize}
The results of hierarchical clustering are usually presented in a dendrogram \footnote{dendrogram from greek dendrog: "tree" and gramma "drawing"}.

The results produced by the implementation of Hierarchical clustering are shown in Section \ref{sse:reshierarchical}.

\subsubsection{PCA}
\label{sse:PCA}
Principal component analysis (PCA) is a method to find the directions (principal components) in which the data has maximum variance. An eigenvalue is a number that tells how the data set is spread out on the line which is an eigenvector. Thus, an eigenvector with the highest eigenvalue is the Principal Component. In addition to eigen-decomposition of the covariance matrix, PCA can also be performed via singular value decomposition (SVD) of the data matrix.

\textbf{Singular Value Decomposition}

Eigendecomposition is not the only way to factorize a matrix, Single value decomposition (SVD) can also explore factorize a matrix into singular vectors and singular values. In linear algebra SVD is useful to partially generalize matrix inversion to non-square matrices \cite{goodfellow2016deep}. SVD can be seen also as an unsupervised method of feature construction. In this context, the goal of SVD is to form a set of features that are liner combinations of the original variables providing the best reconstruction in the least square sense \cite{duda2012pattern}. TruncatedSVD, is the algorithm that will be implemented int he Results Section \ref{sse:resunsupervised}. It is very similar to PCA but differs in that it works on sample matrices X directly instead of their covariance matrices. 

Note that when the column-wise (per-feature) means of X are subtracted from the feature values, truncated SVD on the resulting matrix is equivalent to PCA. SVD deals more efficiently with sparse matrices than PCA (this is because it does not center the data before doing SVD) \cite{halko2009finding}. When truncated SVD is applied to term-document matrices, this transformation is known as latent semantic analysis (LSA), because it transforms such matrices to a “semantic” space of low dimensionality (combat the effects of synonymy and polysemy which cause term-document matrices to be overly sparse and exhibit poor similarity under measures such as cosine similarity).
Mathematically, truncated \footnote{Truncated means just that the algorithm will return matrices with the specified number of columns (k). This is precisely how the dimensionality is reduced.} SVD applied to training samples X produces a low-rank approximation:
\begin{equation}
X = \mathbf{U} \Sigma \mathbf{V}^{T}
\label{eq:suv}
\end{equation}
where $\mathbf{U}$ is an unitary (orthogonal matrix), $\Sigma$ is a diagonal square matrix with non-negative real numbers and $\mathbf{V} ^{T}$ is the conjugate transpose of the unitary matrix V. The diagonal entries $\Sigma$ are the singular values of the matrix X that we want to factorize. The expression \ref{eq:suv} can be intuitively interpreted as a composition of three geometrical transformations: a rotation ($\mathbf {U}$), a scaling $\Sigma$, and another rotation $\mathbf{V}^{*}$.

The results produced by the implementation of PCA using both eigen-decomposition and single value decomposition are are shown in Section \ref{sse:resPCA}.

\subsubsection{Manifold learning}
\label{sse:manifold}
Manifold learning also called nonlinear dimensional reduction is an unsupervised method that deals with discovering the hidden simpler structure in a high dimensional dataset.
Manifold learning is an approach to non-linear dimensionality reduction. The rationale is that the dimensionality of many data sets is only artificially high and an embedded lower dimensionality representation exists. High dimensional datasets are very difficult if not impossible to visualize. Dimensionality reduction for visualization is then necessary. The simplest method of visualization of high dimensional data  dimensionality reduction is taking a random projection of the data which will work as a section (a view) of the data. This method is  suboptimal because if by chance we take dimensions with little variance most information will be lost. 
As we saw in Section \ref{sse:PCA}, principal component analysis can identify meaningful components, however, PCA and related linear methods that focus on finding linear projection of the data will likely fail to identify important non-linear patterns in the data. 
In this vein, Manifold Learning can be thought of as an attempt to generalize linear frameworks like PCA to be sensitive to non-linear structure in data. 
%Though supervised variants exist, the typical manifold learning problem is unsupervised: it learns the high-dimensional structure of the data from the data itself, without the use of predetermined classifications.
A number of algorithms for manifold learning exist including among others Isomap, Hessian Eigenmapping, Spectral Embedding, Multi-dimensional Scaling and t-distributed Stochastic Neighbor Embedding (t-SNE). Here we will focus on tSNE, a recent and very successful algorithm to visualize high-dimensional data.
%https://distill.pub/2016/misread-tsne/

\textbf{tSNE: stochastic neighbor embedding}
%\label{sse:tsne}

Let us briefly explain the t-distributed stochastic neighbor embedding (t-SNE), a popular algorithm for exploring high-dimensional data is  \cite{maaten2008visualizing}. First, some definitions:

\theoremstyle{definition}
\begin{definition}{Data point}
A data point is a point $x_i$ in the original space $\mathbf{R}^D$, where D is the number of dimension (features).
\end{definition}

\begin{definition}{Map point}
A map point is a point $y_i$ in the map space $y \in \mathbf{R}^2$ (if we want to map the original space in 2D). The map space will contain the final representation of the original data space ($b:X \to Y$), where b is an isomorphic from the original space (X) to the map space (Y), that is, for every map point y there is one point x. 
\end{definition}
 
Now, How the map space (Y) is built? We want to preserve the structure of the original data (X), that is, if two points are close together in X their images in Y must be closed together as well. We need to define a distance that quantifies closeness, for example, the Euclidean distance. Thus, 
\begin{equation} \label{eq:tsne}
p_{j|i} = \frac{\exp\left(-\left| x_i - x_j\right|^2 \big/ 2\sigma_i^2\right)}{\displaystyle\sum_{k \neq i} \exp\left(-\left| x_i - x_k\right|^2 \big/ 2\sigma_i^2\right)}
\end{equation}
which measures how close $x_i$ is from $x_j$ if we assume a Gaussian distribution around $x_i$. Note that the variance $\sigma$ is different for every point and it is chosen such as in dense areas is smaller than in sparse areas. 
Finally, the similarity $p_{ij}$ needs to be a symmetric measure of the conditional probability, then:
\begin{equation} \label{eq:tsne2}
p_{ij} = \frac{p_{j|i} + p_{i|j}}{2N}
\end{equation}
the similarity measure gives us the similarity matrix for the original dataset (the similarity matrix can be calculated for simplicity with constant $\sigma$ or using a different $\sigma_i$ per point i).
Bellow we show the distance (Euclidean) matrix and the similarity matrix both for constant and variable $\sigma$.

%images here: https://github.com/oreillymedia/t-SNE-tutorial/blob/master/images/similarity.png
Next, we need to compute the similarity matrix for the map points,
\begin{equation}
q_{ij} = \frac{f(\left| x_i - x_j\right|)}{\displaystyle\sum_{k \neq i} f(\left| x_i - x_k\right|)} \quad \textrm{with} \quad f(z) = \frac{1}{1+z^2}
\end{equation}
note that q is built using a Cauchy distribution. Cauchy is "pathological" distribution since both its expected value and its variance are undefined. This choice of the distribution for the map points obeys to the fact that the volume of the N dimensional ball of radius r scales at $r^N$ which means that when N is large if we pick random points uniformly in the ball, most points will be closed to the surface, that is, a N dimensional orange will have most of the points in the skin rather than in the pulp.
%https://github.com/oreillymedia/t-SNE-tutorial see similation of distance from origin 
%(t-student with one degree of freedom) rather than a Gaussian distribution as in p, another difference is that the similarity matrix p is fixed and the similarity matrix q depends on the map points. 

When reducing the dimensionality of a dataset, if we used the same Gaussian distribution for the data points and the map points, we would get an imbalance in the distribution of the distances of a point's neighbors. This is because the distribution of the distances is so different between a high-dimensional space and a low-dimensional space. Yet, the algorithm tries to reproduce the same distances in the two spaces. This imbalance is actually what happens in the original SNE algorithm \cite{hinton2003stochastic}.
The t-SNE algorithm works around this problem by using a Cauchy $(0,1)$ distribution for the map points, since this distribution has a much heavier tail than the Gaussian distribution it compensates the original imbalance. For a given similarity between two data points, the two corresponding map points will need to be much further apart in order for their similarity to match the data similarity. So Cauchy distribution leads to more effective data visualizations, where clusters of points are more distinctly separated.

%What we want is that the matrices P and Q be as similar as possible, that is, similar data points yield similar map points.
%The Kullback-Leiber divergence measures the distance between the two matrices P and Q. 
% \begin{equation}
% KL(P||Q) = \sum_{i, j} p_{ij} , \log \frac{p_{ij}}{q_{ij}}
% \end{equation}
% to minimize the score we perform gradient descent:
% \(\frac{\partial , KL(P || Q)}{\partial y_i} = 4 \sum_j (p_{ij} - q_{ij}) g\left( \left| x_i - x_j\right| \right) u_{ij} \quad \textrm{where} , g(z) = \frac{z}{1+z^2}.\)
% we try to approximate the true distribution P using Q, KL(P|Q) = H(P,Q) - H(P), the cross entropy minus the entropy.
% So, KL-divergence is better not to be interpreted as a "distance measure" between distributions, but rather as a measure of entropy increase due to the use of an approximation to the true distribution rather than the true distribution itself. (if we knew the true distribution P of the random variable, we could construct a code with average description length H(P). If, instead, we used the code for a distribution Q, we would need H(P)+K(P||Q) bits on the average to describe the random variable) so you need more bits to describe the situation if you are going to use Q while the true distribution is P, so what KL measures is the inefficiency caused by the approximation of Q to P.
%https://stats.stackexchange.com/questions/111445/analysis-of-kullback-leibler-divergence

The results produced by the implementation of manifold learning, in particular t-distributed stochastic neighbor embedding (tSNE) are shown in Section \ref{sse:resmanifold}.


\newpage

\section{Results}
\label{se:res}

In this section we provide the results for the machine learning techniques explained in the previous section.

\subsection{Univariate feature selection}
\label{se:reslinreg}
%CODE::run_feature_ranking(Xy_df_scaled, formula)

The algorithm used for univariate feature selection is described in the snippet \ref{SelectKBest}. Univariate feature selection works by selecting the best features based on univariate statistical tests (e.g ANOVA F-value between label/feature).

\begin{code}[caption=SelectKBest, label=SelectKBest]
from sklearn.feature_selection import SelectKBest
#Build the design matrix for feature ranking (selection)
selector = SelectKBest(f_classif, nboffeats=12)
selector.fit(X, y)
\end{code}

From the Vallecas Index regression model (snippet \ref{formula}) the algorithm estimates the 12 most important features according to the ANOVA F-value between target and features (Table \ref{tab:ranking}).

\begin{code}[caption=Regression formula, label=formula]
'conversionmci ~ sexo + lat_manual + nivel_educativo  + edad_ultimodx + renta+ sue_noc + sue_rec+ familiar_ad + imc+ a02 + a03 + a08 + a11 + a12 + a13 + a14+ numhij+ sdvive + sdeconom + sdresid + sdestciv + sdatrb+ sdatrb+ hta + glu + lipid + tabac_cant + cor + arri + card+ ictus + tce + tce_con+ dietaketo + dietasaludable + dietaglucemica+ physical_exercise + scd_visita1 + preocupacion_visita1 + act_aten_visita1 + act_orie_visita1 + act_mrec_visita1 + act_visu_visita1 + act_expr_visita1 + act_comp_visita1 + gds_visita1 + stai_visita1 + act_apat_visita1 + relaamigo + relafami + valcvida2_visita1 + valsatvid2_visita1 + valfelc2_visita1 + eq5dsalud_visita1 + eq5deva_visita1'
\end{code}

\begin{table}[H]
\caption{Ranking with ANOVA F-value between target/feature} \label{tab:ranking} 
\begin{center} 
\begin{tabular}{ll}
\hline
\multicolumn{1}{c}{} \\
%\cline{1}
Ranking & Feature     \\
\hline
1 & scd visita1 (cognitive complaints)         \\
2 & preocupacion visita1 \\
3 & a13 (internet) \\
4 & gds visita1 (depression)\\
5 & nivel educaivo \\
6 & a08 (cultural activities)\\
7 & eq5dsalud visita1 \\
8 & a02 (creative activities) \\
9 & sdvive (home members)\\
10 & edad ultimodx \\
11 & renta \\
12 & sue rec (remember dreams) \\
\hline
\end{tabular}
\caption{The table shows the ranking between target/feature using ANOVA F value. Other statistical tests e.g. Chi square, mutual information, are also possible and will produce slightly different results. For example, using the mutual information, the ranking is \emph{scd visita1, a08, tce con, renta, gds visita1, preocupacion visita1, arri, nivel educativo, physical exercise, dietaketo, dietasaludable, a11}. Interestingly, when mutual information is used features related to diet and physical exercise show up. Note that mutual information captures also nonlinear dependence between variables.}
\end{center}
\end{table}

%Selector {} scores: 12 [18.98007015 15.46747422 13.89768416  9.11659202  8.28116868  8.22660999 6.35350838  6.22921748  5.58317551  4.40881512  4.2699721   4.04350996]
%Top features:
%Index([u'scd_visita1', u'preocupacion_visita1', u'a13', u'gds_visita1',
%       u'nivel_educativo', u'a08', u'eq5dsalud_visita1', u'a02', u'sdvive', u'edad_ultimodx', u'renta', u'sue_rec'],


\subsection{Logistic regression}
\label{se:reslogreg}
%CODE 

The classification loss function used in Logistic regression is the \emph{log loss} previously described in Section \ref{sse:logreg}. Logistic regression uses Maximum Likelihood to estimating the parameters given observations, that is, the parameter values that maximize the probability p of event 1 and (1-p) of non-event 0(Equation \ref{eq:sigmoid3}).


of making the observation. this means finding parameters that maximize the probability p of event 1 and (1-p) of non-event 0, as you kno

It should be remarked that \emph{log loss} heavily penalizes classifiers that are confident about an incorrect classification, e.g. if the classifier assigns a very small probability to the correct class then the corresponding contribution to the \emph{log loss} will be very large indeed. 
For example, if the classifier assigns a very small probability to an observation to be the correct class then the corresponding contribution to the \emph{log loss} of the observation will be very large indeed. Mathematically this can be directly deduced from the log loss function: it is very steep as the predicted probability approaches 0, but it smoothly declines as the predicted probability improve, that is, it penalizes heavily being very wrong i.e. assign a probability close to 0 of a true event (Figure \ref{fig:Log-loss-curve}). 
\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=0.5\linewidth]{figures/Log-loss-curve}
        \caption{\emph{Log loss} curve, it penalizes heavily classifiers that are confident about an incorrect classification. For example, if the classifier assigns a very small probability to the correct class (close to 0 in the abscissa axis) it yields a large loss value. As the predicted probability improves (moving towards 1 in the  abscissa axis) the slope is smooth. The rationale behind this is that it is better to be somewhat wrong than emphatically wrong. 
        }
\label{fig:Log-loss-curve}
\end{figure}
%https://datawookie.netlify.com/blog/2015/12/making-sense-of-logarithmic-loss/

The performance of the classifier will be affected by both the properties of the classifier e.g. the loss function mentioned above and the characteristics of the dataset, specifically its unbalanceness, the converters class is under represented vis a vis the class of non converter 10 to 1. In order to deal with this problem the classifier will use the \emph{balanced} weight option to give more weight to examples of the underrepresented class. Unless we say other less, we will use this heuristic in the implementation of classifiers. To know more about the \emph{balanced} weight heuristics see \cite{king2001logistic}. 

Logistic regression is vulnerability to overfitting, we deal with this problem using regularization ($l_2$ penalty) with the  \emph{liblinear} solver (liblinear solver supports logistic regression and SVM). 
Formally, the $l_2$-regularized Logistic Regression solves the unconstrained optimization problem indicated below \cite{fan2008liblinear}
\begin{equation}
\textit{min}_{w} \frac{1}{2}w^Tw + C \sum_{i=1}^{n}\log(1+ e^{-y_iw^Tx_i})
\end{equation}
%https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf
where $w$ is the weight vector argument of the \emph{log loss function}, $(x_i,y_i)$ the instances and $C > 0$ is a penalty parameter (when $C=0$ there is no penalty).

The dataset is split into training data ($80\%$) and test data ($20\%$). Table \ref{tab:logreg} shows the classification metrics for logistic regression algorithm with the hyperparameters above mentioned. The table shows the performance of the classifier in generalizing new data in the test test. 
The recall of the positive class or \emph{sensitivity} is lower than the recall of the negative class or \emph{specificity}, this is due to the unbalanced distribution of the labels i.e. the converters are less frequent (19) than the non converters (164). The table is for $C = 1.0$, C is the inverse of regularization strength. We find that the optimal C is $C = 1000.0$ (very small regularization strength) achieves very small, almost irrelevant gains (precision of 1 class for $C = 1000.0$ is 0.16, for $C = 1.0$ precision of 1 class is 0.15). 
%Cross-validation is one way to find a good set of hyperparameters

\begin{table}[H]
\caption{Classification metrics for logistic regression} \label{tab:logreg} 
\begin{center} 
\begin{tabular}{lllll}
\hline
\multicolumn{1}{c}{} \\
%\cline{1}
Class & precision & recall & f1-score & support     \\
\hline
0 & 0.92  &    0.70   &   0.79   &    164 \\
1 & 0.15  &    0.47   &   0.23   &     19 \\
\hline
\end{tabular}
\caption{The table shows main classification metrics for the logistic regression classifier in the test set. The precision and the recall of the positive class is low due to the unbalanced data.} \label{tab:logregress}
\end{center}
\end{table}

Figure \ref{fig:logresres} shows the results of the logistic regression model. Remember that it is assumed that the logit transformation of the outcome variable (\emph{conversionMCI}) has a linear relationship with the predictor variables (\emph{Vallecas index}), therefore only the linear relationships between individual features and the target variable are captured.

\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=0.8\linewidth]{figures/LR_results-balanced}
        \caption{Logistic regression results $l_2$ regularization, balanced weight option and liblinear solver. The feature about remembering dreams (sue rec) shows the strongest negative association with converting. Other features with large weight in absolute value are arrhythmia (arri), head contusions (tce), anxiety (stai) and age (edad)} \label{fig:logresres}
\end{figure}


%Scitkit-Learn actually adds an $l_2$ penalty by default”
%Lasso regularization: answer two points: what is the baseline prediction of disease progression and  which independent variables are important factors for predicting disease progression.
%Coefficient of determination R^2 of the prediction of LogReg_Lasso on training set 0.29
%Coefficient of determination R^2 of the prediction of LogReg_Lasso on test set 0.11
Figure \ref{fig:lr-cm} shows the confusion matrix and the Receiver Operating Characteristic (ROC) metric. 
The ROC is a graphical plot that illustrates the performance of a binary classifier as its discriminant threshold is varied. The curve is created by plotting the true positive rate (TPR, y axis) against false positive rate (FPR, x axis) at various threshold values. The top left corner of the plot is the ideal point - a false positive rate of zero, and a true positive rate of one. 
Thus, the larger area under the curve (AUC) the better is usually the classifier.
The steepness of ROC curves is also important, a steep curve -maximize true positive rate while minimizing the false positive rate- is preferred.
%dangeti -statistics for ml. book page 89
The confusion matrix tabulates the number of misclassifications. The diagonal elements of the confusion matrix show the number of correct classifications for each class and the off-diagonal elements the examples that are misclassified. 


\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=1\linewidth]{figures/LR-CM}
        \caption{Confusion matrix and ROC for logistic regression classifier. In the confusion matrix, the y-axis has the actual values and the x-axis the predicted values, therefore the counts on the diagonal are the number of correct predictions and elements of the diagonal are incorrect predictions.
        On the left, the confusion matrix of the training set ($80\%$ of example the entire dataset), center the performance on the test set (the remaining $20\%$) and on the right the ROC curves and the area under the curve (AUC). The discontinuous black diagonal line refers to pure 'luck' prediction. In blue the training set and in red the test set.}
\label{fig:lr-cm}
\end{figure}

Figure \ref{fig:lr-cv7} shows the learning curve for the logistic regression classifier using cross-validation (7 splits).
\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=0.5\linewidth]{figures/LR-CV7}
        \caption{Learning curve for logistic regression for cross-validation score ($cv=7$) and scoring = 'accuracy'. In red the training score and in gree . The curve seems to suggest that the validation score could increased adding more examples, if eventually training and cross-validation have the same score it means that the classifier is not over-fitting. 
		The 7-fold cross validation average accuracy is $0.74$, a quite satisfactory result. Maximizing for sensitivity (recall of the positive class) yields less satisfactory result, average recall $0.58$, due to the underepresentation of the positive class. Note also that the optimal regualarization parameter $C$ (inverse of the regularization strength) has different values depending on he scoring used, maximizing accuracy requires very little regularization, $C*=1,000$ but if we try to maximize recall regularization is stronger $C= 0,1$. 
        } \label{fig:lr-cv7}
\end{figure}

Logistic regression shows an acceptable performance, especially given the model's simplicity. 
The model's behavior does not easily fit into overfitting neither underfitting patterns, the problem, however, resides in data unbalanced. Both training and validation error are larger for the underrepresented class 1 (non converter) than for the more frequent class 0 (non converter). Precision and recall of class 1 metrics are below the random guessing, even if we maximize for recall rather than accuracy we obtain a slightly better than random guess ($0.58$). 

%Underfitting – Validation and training error high
%Overfitting – Validation error is high, training error low
%Good fit – Validation error low, slightly higher than the training error
%Unknown fit - Validation error low, training error 'high'
%If the training accuracy increases(positive slope) while the validation accuracy steadily decreases(negative slope) then a situation of over fitting may have occurred. Time to stop Training

\subsection{SVM}
\label{se:ressvm}
	% #SVM for both linear and non linear kernel
	% #print('Building a Sigmoid (Linear) svm_estimator.....\n')
	% #learners['svm_sigmoid_estimator'] = run_svm_classifier(X_train, y_train, X_test, y_test, kernel='sigmoid')
	% #dict_learners["svm_sigmoid"]=learners['svm_sigmoid_estimator']
Here we show the implementation of Support vector machines (SVMs) for classification using both linear and non linear boundaries. SVMs are versatile (different kernels can be defined for the decision function), memory efficient and effective classifiers for small-medium size datasets. SVMs scales well with the number of features and they are also effective when n > m (more dimensions than examples). If the number of features is much greater than the number of samples, choosing optimally the Kernel function and the regularization term to avoid over-fitting is crucial. Note that this is not the case in the \emph{Vallecas index}, where $n~50 < m~1000$. Although SVMs do not directly provide probability estimates, these can be calculated using cross-validation (averaging across multiple realizations) \footnote{A "surrogate" for probability estimate can be obtain using distances, We use the \emph{sklearn} SVC algorithm which includes a confidence defined as distance between the test instance and the decision boundary}.

A technical description of SVMs was given in Section ref\ref{sse:svm} here we will explain the choices made in its implementation and the quality of its predictions. 
The dataset is again split into training data ($80\%$) and test data ($20\%$). 

Table \ref{tab:svm} shows the classification metrics in the test set for linear SVM with a linear kernel. The only hyperparameter that we care of in linear SVM is the soft margin constant constant, $C$. Using Grid-Search we find $C* =100.0$. Remember from Section \ref{sse:svm} that C is a regularization parameter that controls the trade-off between maximizing the margin and minimizing the training error \footnote{Small C (by default $C=1$), tends to emphasize the margin, while large C may tend to overfit the training data}.


%http://fourier.eng.hmc.edu/e161/lectures/svm/node5.html
%When the two classes are not linearly separable (e.g., due to noise), the condition for the optimal hyper-plane can be relaxed by including an extra term: 
% \begin{table}[H]  #Sigmoid kernel C=1
% \caption{Classification metrics for SVM}
% \begin{center} 
% \begin{tabular}{lllll}
% \hline
% \multicolumn{1}{c}{} \\
% %\cline{1}
% Class & precision & recall & f1-score & support     \\
% \hline
% 0 & 0.93  &    0.57   &   0.71   &    164 \\
% 1 & 0.15  &    0.63   &   0.24   &    19 \\
% \hline
% \end{tabular}
% \caption{The table shows the main classification metrics for the SVM classifier (sigmoid kernel) in the test set. The SVM classifier underperforms the previous logit classifier except in \emph{sensitivity} (recall of the positive class.}  \label{tab:svm} 
% \end{center}
% \end{table}

\begin{table}[H]
\caption{Classification metrics for linear SVM}
\begin{center} 
\begin{tabular}{lllll}
\hline
\multicolumn{1}{c}{} \\
%\cline{1}
Class & precision & recall & f1-score & support     \\
\hline
0 & 0.93  &    0.70   &   0.80   &    164 \\
1 & 0.17  &    0.53   &   0.26   &    19 \\
\hline
\end{tabular}
\caption{The table shows the main classification metrics for the SVM classifier with linear kernel and soft margin $C=100$. The performance of the linear SVM classifier is very similar to the logistic classifier (Table \ref{tab:logregress}). 
Both linear SVM and logistic classifier have linear decision boundaries, that is to say, the classifier needs the inputs to be linearly separable. Linear SVM only requires the regularization parameter $C$ which trades off misclassification of training examples against simplicity of the decision surface (low C makes the decision surface smooth, while a high $C$ aims at classifying all training examples correctly by giving the model freedom to select more samples as support vectors).}  \label{tab:svm} 
\end{center}
\end{table}

Figure \ref{fig:svm-cm} shows the confusion matrix and the Receiver Operating Characteristic (ROC) metric of the SVM implemented which uses a sigmoid kernel and balances the weight of the classes to deal with the unbalanced problem (give more weight to the underrepresented class).
\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=1\linewidth]{figures/SVM-CM-linear}
        \caption{Confusion matrix and ROC for SVM classifier with linear kernel and $C=100$. The classifier performs better than random luck and is very similar to the logistic regressor performance.} \label{fig:svm-cm}
\end{figure}

Figure \ref{fig:svm-cv7} shows the learning curve for the logistic regression classifier using cross-validation (7 splits).
\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=0.5\linewidth]{figures/SVM-CV7-linear}
        \caption{Learning curve for SVM classifier with with linear kernel and $C=100$, in red the training score and in green the cross-validation ($cv=7$) score. SMV roughly converges to a score of ~0.7.
        } \label{fig:svm-cv7}
\end{figure}

\textbf{SVM parameters for non linear SVM}

%GridSearchCV to perform an exhaustive search over the parameter values for an estimator. The parameters are optimized by cross-validated grid-search over a parameter grid. C and Gamma are the parameters for a nonlinear SVM with a Gaussian radial basis function kernel.
The above SVM implements linear classification using the dot product (Equation \ref{eq:svm}). If we replace the dot product with a nonlinear kernel function (such as a Gaussian radial basis kernel function (Equation \ref{eq:kernelgamma}) or a polynomial kernel), SVM can classify non linear relationships as well.
\begin{equation} \label{eq:kernelgamma}
K(x_i,y_i) = e^{-\gamma ||x_i-x_j||^2}
\end{equation}

%The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.
% the C parameter is the cost of misclassification.  
In order to make sense of the results that will be shown we must first study the hyperparamerters. In addition to the regularization term $C$, non linear kernel have free parameter $\gamma$ which gives us the influence of single training examples. A small $\gamma$ means a Gaussian with a large variance so the class of the support vector $x_j$ will have influence on deciding the class of the vector $x_i$ even if the distance between them is large. 
When gamma is very small, the model is too constrained and cannot capture the complexity of the data and the resulting model will behave similarly to a linear model.
%The region of influence of any selected support vector would include the whole training set. The resulting model will behave similarly to a linear model with a set of hyperplanes that separate the centers of high density of any pair of two classes.
%literal from http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html
On the other hand, if $\gamma$ is large, it means that the  variance will be small and the support vector does not have wide-spread influence. Technically speaking, large gamma leads to high bias and low variance models, and vice-versa. 


%https://www.quora.com/What-are-C-and-gamma-with-regards-to-a-support-vector-machine
The behavior of the SVM model is very sensitive to the $\gamma$ parameter. If $\gamma$ is too large, the radius of the area of influence of the support vectors only includes the support vector itself and no amount of regularization with C will be able to prevent overfitting.
%A large C gives you low bias and high variance. Low bias because you penalize the cost of missclasification a lot.

Table \ref{tab:svmnonlinear} shows the classification metrics for the nonlinear SVM (Radial Basis Function Kernel) with $C*= 10.0$ and $\gamma *= 0.03125$

\begin{table}[H]
\caption{Classification metrics for RBF SVM} 
\begin{center} 
\begin{tabular}{lllll}
\hline
\multicolumn{1}{c}{} \\
%\cline{1}
Class & precision & recall & f1-score & support     \\
\hline
0 & 0.93  &    0.79   &   0.85   &    164 \\
1 & 0.20  &    0.47   &   0.29   &    19 \\
\hline
\end{tabular}
\caption{The table shows the main classification metrics for the nonlinear SVM classifier (RBF kernel) in the test set with the best hyperparameters C and $\gamma$. The nonlinear SVM classifier performs better than the linear SVM, in particular in \emph{specificity} (recall of negative class).} \label{tab:svmnonlinear} 
\end{center}
\end{table}

Figure \ref{fig:svm-rbf-cm} shows the confusion matrix and the Receiver Operating Characteristic (ROC) metric of the nonlinear SVM with Radial Basis Function kernel kernel, balanced the weight of the classes to deal with the unbalanced problem and the hyperparameters $C*= 10.0$ and $\gamma *= 0.03125$. The nonlinear SVM classifier produces netter results than the linear SVM classifier.

\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=1\linewidth]{figures/SVM-RBF-CM}
        \caption{Confusion matrix and ROC for nonlinear SVM classifier (RBF kernel) $C*= 10.0$ and $\gamma *= 0.03125$. The classifier performs similar to the linear SVM (Figure \ref{fig:svm-cm}). Larger C will causes overfitting.} \label{fig:svm-rbf-cm}
\end{figure}

Figure \ref{fig:svm-cv7} shows the learning curve for the logistic regression classifier using cross-validation (7 splits).
\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=0.5\linewidth]{figures/SVM-RBF-CV7}
        \caption{Learning curve for nonlinear SVM classifier with RBF kernel $C*= 10.0$ and $\gamma *= 0.03125$, in red the training score and in green the cross-validation ($cv=7$) score. The nonlinear SMV performs similar to the linear SVM (Figure \ref{fig:svm-cv7}). The gap between the training score and the cross validation score suggests high variance, indicating that more data will likely will help improving the results. 
        } \label{fig:svm-cv7}
\end{figure}

SVM with both linear and non linear (RBF) kernel shows an acceptable performance. 
The model's behavior shows overfitting, especially for $C>10$, the problem, however, resides in the fact that the dataset is  unbalanced. While training error is very low (better than logistic regression), the validation error is still larger for the underrepresented class 1 (non converter) than for the more frequent class 0 (non converter). Precision and recall of class 1 metrics are below the random guessing. 

\subsection{Naive Bayes}
\label{se:resnaivebayes}

Here we fit a Gaussian Naive Bayes classifier (NB). NB is a simple and easy to implement algorithm \footnote{The hyperparameter is the prior probabilities of the classes.} extensively used with in document classification, at the same time it is known to be a bad probability estimator, so the probability outputs are not to be taken too seriously \cite{scikit-learn}. %from predict_proba
%http://scikit-learn.org/stable/modules/naive_bayes.html.  literal
A technical description of Naive Bayes classifier delineating the independence assumption in the distribution of $p(x| y)$ was provided in Section \ref{sse:naivebayes}.
 
The dataset is again split into training data ($80\%$) and test data ($20\%$). Table \ref{tab:naive}

Table \ref{tab:naive} shows the classification metrics for the implemented Gaussian Naive Bayes classifier.
\begin{table}[H]
\caption{Classification metrics for Naive Bayes} 
\begin{center} 
\begin{tabular}{lllll}
\hline
\multicolumn{1}{c}{} \\
%\cline{1}
Class & precision & recall & f1-score & support     \\
\hline
0 & 0.91  &    0.63   &   0.74   &    164 \\
1 & 0.13  &    0.47   &   0.20   &    19 \\
\hline
\end{tabular}
\caption{The table shows the main classification metrics for the Gaussian Naive Bayes classifier in the test set. The recall of NB underperforms logistic regression and SVM classifier seen before.} \label{tab:naive} 
\end{center}
\end{table}

Figure \ref{fig:nb-cm} shows the confusion matrix and the Receiver Operating Characteristic (ROC) metric of the Gaussian Naive Bayes classifier implemented.
\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=1\linewidth]{figures/NB-CM}
        \caption{Confusion matrix and ROC for GaussianNB classifier. The classifier performs better than random luck. The AUC of GaussianNB and the previous classifiers are quite similar.} \label{fig:nb-cm}
\end{figure}

Figure \ref{fig:nb-cv3} shows the learning curve for the logistic regression classifier using cross-validation (7 splits).
\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=0.5\linewidth]{figures/NB-CV3}
        \caption{Learning curve for Gaussian Naive Bayes, in red the training score and in green the cross-validation ($cv=5$) score. The curves seem to indicate that the model is bias, feeding the GaussianNB model with more data may not help capturing the underlying relationship.
        } \label{fig:nb-cv3}
\end{figure}
%https://www.ritchieng.com/machinelearning-learning-curve/

\subsection{k-Nearest Neighbors}
\label{se:reskneighbors}
%%%CODE
%run_kneighbors

Here we show the implementation of the k-nearest neighbors vote algorithm (k-nn). The algorithm was introduced in Section \ref{sse:kneighbors} its main characteristics are briefly reminded next.
The k-nn algorithm is characterized by minimal training time (the function is approximated locally) but expensive testing (all computation is deferred until classification). k-nn is non-parametric i.e. doesn't assume normality of data not it explicitly learns a model (instance-based), the algorithm rather memorizes the entire training set which are subsequently used as “knowledge” for the prediction phase.
%, that is, k-nn works like an on-demand process only when a query to our database is made will the algorithm use the training instances to spit out an answer. 

The algorithm will look for the optimal k among possible odd (to avoid voting results in a draw) values and will evaluate the results using cross validation. For example, $(cv=5)$ and $(k=1,3,5,7....49)$ then the algorithm is trained $\frac{50}{2} \times 5$ times. 
%https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/
For low k, eg $k=1$ the decision boundaries will be very convoluted (high complexity, over fitting), for large k the classifier needs to weigh the votes of many neighbors, not just one, so single training data points will not have the dramatic influences in the prediction as they have in $k=1$, result of a larger k is a smoother boundary (model with lower complexity, low bias error).
%, so for k equal all the number the prediction will be always the most frequent class (model underfit). 

The metric distance is the Minkowski distance ($p=1$ Manhattan, $p=2$ Euclidean).
The algorithm requires the computation of the number of neighbors k, once this parameter is estimated performing cross validation on our dataset using a generated list of an odd number k neighbors. The optimal cross validation score for k can be calculated by maximizing accuracy or recall, if the scoring is set to accuracy the, $k*=1$ but the precision, recall and the f1-score are 0. Maximizing instead by recall $k*=1$ with the results shown in Table \ref{tab:knn} 

\begin{table}[H]
\caption{Classification metrics for k-nn ($k=1$, scoring = 'recall')} 
\begin{center} 
\begin{tabular}{lllll}
\hline
\multicolumn{1}{c}{} \\
%\cline{1}
Class & precision & recall & f1-score & support     \\
\hline
0 & 0.90  &    0.93   &   0.93   &    164 \\
1 & 0.20  &    0.16   &   0.18   &     19 \\
\hline
\end{tabular}
\caption{The table shows main classification metrics for the k-nn algorithm with $k*=1$, scoring='recall' and $cv=5$. The k-nn algorithm fails to predict true positives. The recall of the positive class is also known as \emph{sensitivity} is $0.16$ the recall of the negative class (\emph{specificity}) is $0.93$. The algorithm suffers from the data unbalanced and it cannot deal with it because the algorithm is based on a majority vote of neighbors which will be on almost always $0$, that is, the majority vote will be $0$ in a population with $90\%$ of $0$s and $10\%$ of $1$s.} \label{tab:knn} 
\end{center}
\end{table}

Figure \ref{fig:knn-cm} shows the confusion matrix and the Receiver Operating Characteristic (ROC) metric of the k-nn algorithm. 

\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=1\linewidth]{figures/KNN5-cv5_CM-recall}
        \caption{Confusion matrix and ROC for the k-nn classifier. The k-nn algorithm is very good at predicting the 0 class (non converter) but it fails at predicting the positive class (converters). The accuracy of the training set is $100\%$, in the test set the \emph{specificity} is $(152/164)$ and the \emph{sensitivity} is very poor $(3/19)$. The AUC (0.54) of the test set is as good as random guessing.
        }
\label{fig:knn-cm}
\end{figure}

Figure \ref{fig:knn-cv3} shows the learning curve for the logistic regression classifier using cross-validation (7 splits).
\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=0.5\linewidth]{figures/KNN3-cv5_curve-recall}
        \caption{Learning curve for $k*=1$ and scoring='recall', in red the training score and in green the cross-validation ($cv=5$) score. The algorithm clearly overfits.
        } \label{fig:knn-cv3}
\end{figure}


\subsection{Decision Trees}
\label{sse:resdectrees}
Decision trees require very little preparation, for example they do not need to scale or centering the data, remember than in SVM scaling was critical otherwise the algorithm would neglect small features.
Importantly, decision trees are known as \emph{white box} models they can identify the features that are more relevant to predict the target.
Like SVMs decision trees are versatile machine learning algorithms that can fit complex datasets. Decision trees are the most important components of Random Forests -a powerful ensemble methods that will bee studied in the next Section.

The dataset is again split into training data ($80\%$) and test data ($20\%$). The hyperparameter max$\_$depth is set to 5 for better performance and it uses balanced mode to give more weight to the underrepresented class, the rest of parameters are set by default in \emph{sklearn} \cite{scikit-learn}. Table \ref{tab:dectree} shows the classification metrics for the implemented decision tree classifier.

\begin{table}[H]
\caption{Classification metrics for the decision tree classifier}
\begin{center} 
\begin{tabular}{lllll}
\hline
\multicolumn{1}{c}{} \\
%\cline{1}
Class & precision & recall & f1-score & support     \\
\hline
0 & 0.91 &  0.71  &  0.80  & 164 \\
1 & 0.13 &  0.37  &  0.19  & 19  \\
\hline
\end{tabular}
\caption{The table shows the main classification metrics for the decision tree classifier. The metrics for the negative class 0 (non-converter) are good but those for the converters are very poor due to the data unbalanced problem.  
}  \label{tab:dectree} 
\end{center}
\end{table}


Figure \ref{fig:dt-md5} shows the confusion matrix and the AUC for the Decision Tree implemented. 
\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=1\linewidth]{figures/DT_depth5-CM}
        \caption{Confusion matrix and ROC for Decision Tree classifier with $max\_depth$ =5. The classifier generalizes poorly, the AUC is slightly better than luck, $AUC=0.56$. The sensitivity is $117/164$ and the sensitivity is $(7/19)$.
        } \label{fig:dt-md5}
\end{figure}

Figure \ref{fig:dt-md5-learning} shows the learning curve for the Decision Tree using cross-validation $cv=5$ splits. 
\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=0.5\linewidth]{figures/DT_depth5}
        \caption{Learning curve for Decision Tree classifier with $max\_depth=5$, in red the training score and in green the cross-validation ($cv=5$) score. The accuracy is $0.66+ -0.14$. The cross validation for scoring = 'recall' and optimal $max\_depth$ =8 gives a recall of $0.33+ -0.17$. For hyperparameter $max\_depth$ $>5$ the decision tree overfits. 
        } \label{fig:dt-md5-learning}
\end{figure}

Figure \ref{fig:dt-md5-features} identifies the most important features, that is to say, the features selected by the CART algorithm to split the tree so that the cost function (Equation \ref{eq:cart}) is minimized. The algorithm will stop when it cannot find a split that reduces the impurity of the node or it reaches the $max\_depth$ as it is the case.
\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=1\linewidth]{figures/DT_depth5-featuresimportance}
        \caption{On the left the ranking of the most important features of the Decision Tree. The three most important features ( purer splits) are cognitive complaints, dyslipidemia and anxiety. On the right it is shown that 14 features are enough to provide $95\%$ of the total information of the tree.
        } \label{fig:dt-md5-features}
\end{figure}

Finally, Figure \ref{fig:dt-md5-dot} plots the Decision Tree, max$\_$depth $=5$, min$\_$samples$\_$split $=2$ and split$\_$criterion Gini.
\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=1\linewidth]{figures/dt-nospaces}%decision_tree_depth5
        \caption{Decision tree for classification of converter to MCI with the \emph{Vallecas Index}. The root node is cognitive complaints (scd) gives the best possible split according to the Gini measure (Equation \ref{eq:gini}). The maximum depth of the tree was set to 5.
        } \label{fig:dt-md5-dot}
\end{figure}

\subsection{Ensemble Methods}
\label{sse:resensemble}
In the previous sections we have shown the performance of supervised classifiers (decision trees, SVMs, k-nn, logistic regression and Naive Bayes), in this section we will study ensemble of classifiers. Depending on how variety is introduced we can distinguish two types of ensembles -variety is in the classifiers or variety is in the data. The implementation of the former is described in Section \ref{sse:resmajvot} and the last comprises bagging and boosting methods and is implemented in Sections \ref{sse:resrf} and \ref{sse:resgradboosting} respectively. Both approaches rely on the common principle that many weak predictors, in the aggregate, can bring about a strong predictor.

\subsubsection{Majority vote}
\label{sse:resmajvot}

We have trained a bunch of classifiers in the previous sections (decision trees, SVMs, k-nn, logistic regression and Naive Bayes) The majority-vote classifier is a hard classifier. 

A simple way to create a better classifier is to aggregate the predictions of different classifiers, for example combining the predictions of a logistic classifier, SVM, decision tree and naive Bayes. The predictions of the aggregate classifier is the class with the most votes. For example if logistic classifier, SVM and decision tree predict class 0 and naive Bayes predicts class 1, the prediction of the aggregate is class 0 (3 votes against 1).

Table \ref{tab:ensemble} shows the classification metrics for the implemented Ensemble classifier.
\begin{table}[H]
\caption{Classification metrics for Ensemble classifier}
\begin{center} 
\begin{tabular}{lllll}
\hline
\multicolumn{1}{c}{} \\
%\cline{1}
Class & precision & recall & f1-score & support     \\
\hline
0 & 0.92  &    0.81  &   0.86   &    164 \\
1 & 0.18  &    0.37   &   0.25   &    19 \\
\hline
\end{tabular}
\caption{The table shows the main classification metrics for the Ensemble classifier. The metrics for the negative class 0 (non-converter) are excellent but those for the converters are poor due to the data unbalanced problem in our dataset.
The ensemble classifier components are five predictors: logistic regression, non linear SVM, naive Bayes, k-nn and decision tree.
}  \label{tab:ensemble} 
\end{center}
\end{table}


\subsubsection{Bootstrap aggregation (Bagging)}
\label{sse:resrf}

Decision trees are sensitive to the specific data on which they are trained (instability problem mentioned in Section \ref{sse:dectrees}). In a bagged decision tree, first we create many random sub-samples of our dataset with replacement, then we train the tree on each sample to finally calculate the average prediction from each model. 
The only parameters that need to be considered in bagging is the number of decision trees included in the ensemble.

\textbf{Random Forest}

%https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/
%with replacement (meaning we can select the same value multiple times).

%OJO Literal https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/
%If the training data is changed (e.g. a tree is trained on a subset of the training data) the resulting decision tree can be quite different and in turn the predictions can be quite different.
A random forest is an ensemble of decision trees, trained via the the Bootstrap Aggregation method aka bagging. Random forests are an improvement over bagged decision trees. Decision trees (CART algorithm) are greedy, that is to say, the selected variable to split is the one that minimizes the error without concern of what will happen down the road. As such, even using bagging, the decision trees can have a lot of structural similarities and in turn have high correlation in their predictions. Combining predictions from multiple models in ensembles works better if the predictions from the sub-models are uncorrelated or at best weakly correlated.
%OJO Literal 
Random forest changes the algorithm for the way that the sub-trees are learned so that the resulting predictions from all of the subtrees have less correlation. The CART algorithm used in decision trees, when selecting a split point, it is allowed to look through all variables and all variable values in order to select the most optimal split-point. The random forest algorithm changes this procedure so that the learning algorithm is limited to a random sample of features of which to search. The number of features that can be searched at each split point (m) must be specified as a parameter to the algorithm. You can try different values and tune it using cross validation (by default $\sqrt(m)$).

We show the results with random forest built with the optimal hyperparameters estimated with the GridSearchCV procedure
: 
\begin{quote}
max$\_$features=(10,100,1000), max$\_$leaf$\_$nodes=(2,4,8,16), n$\_$estimators=(8,18,28) and min$\_$samples$\_$leaf=(8,16,32).
\end{quote}
%parameters = {'n_estimators':10 ** np.arange(1,4), 'max_features':2 ** np.arange(0, 5), 'min_samples_leaf':[8,18,28], 'max_leaf_nodes':[8,16,32]}

Table \ref{tab:rf} shows the classification metrics for the random forest classifier with the optimal number of max$\_$features, max$\_$leaf$\_$nodes, number of estimators and min$\_$samples$\_$leaf.

\begin{table}[H]
\caption{Classification metrics for Random Forest}
\begin{center} 
\begin{tabular}{lllll}
\hline
\multicolumn{1}{c}{} \\
%\cline{1}
Class & precision & recall & f1-score & support     \\
\hline
0 & 0.91  &    0.90   &   0.90   &    164 \\
1 & 0.23  &    0.26   &   0.24   &    19\\
\hline
\end{tabular}
\caption{The table shows the main classification metrics for the random forest classifier using both the original test set and the synthetic balanced set generated with the SMOTE algorithm. The algorithm does not generalizes at all. Different hyperparametrization could improve this situation but the imbalanced problem can not be solved with a different configuration in parameters space. }  \label{tab:rf} 
\end{center}
\end{table}

Figure \ref{fig:rf_cm} shows the confusion matrix and the ROC for random forest. 
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{.9\textwidth}
        \centering
        \includegraphics[height=1.7in]{figures/RF_CM}
        \caption{Confusion matrix and AUC for random forest classifier with the hyperparameters set to 1000 decision trees, max$\_$features= 10, min$\_$samples$\_$leaf=28 and max$\_$leaf$\_$nodes=16.}
    \end{subfigure}
    ~ 
    \begin{subfigure}[t]{.9\textwidth}
        \centering
        \includegraphics[height=1.7in]{figures/RFGrid_CM}
        \caption{Confusion matrix and AUC for random forest classifier with the optimal hyperparameters calculated with the GridSearchCv algorithm for maximizing accuracy. The random forest has excellent accuracy but it entirely fails to detect positives in the test set.}
    \end{subfigure}%\quad
    
    \caption{Confusion matrix and AUC for random forest classifier (top) and random forest with optimal hyperparameters that maximize scoring (down)} \label{fig:rf_cm}
\end{figure}

Figure \ref{fig:rf_learning} shows the learning rate for random forest classifier. 
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=2.1in]{figures/RF_learning}
        \caption{Learning curve of the random forest classifier with the hyperparameters set to 1000 decision trees, max$\_$features= 10, min$\_$samples$\_$leaf =28 and max$\_$leaf$\_$nodes=16.}
    \end{subfigure}
    ~ 
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=2.1in]{figures/RFGrid_learning}
        \caption{Learning curve of the random forest classifier with optimal hyperparameters calculated with the GridSearchCV search algorithm.}
    \end{subfigure}%\quad
    
    \caption{Learning curve for random forest classifier, in red the training score and in green the cross-validation ($cv=3$) score. The learner overfits and more data will likely improve the predictions.} \label{fig:rf_learning}
\end{figure}

Figure \ref{fig:rf-features} identifies the most important features of the random forest which are cognitive complaints, age, depression and income level.
\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=1\linewidth]{figures/DT_depth5-featuresimportance}
        \caption{On the left the ranking of the most important features of the random forest. The four most important features ( purer splits) are cognitive complaints, age, depression and income level. On the right it is shown that 32 features are enough to provide $95\%$ of the total information of the forest (1000 trees).   
        } \label{fig:rf-features}
\end{figure}

\subsubsection{Boosting}
\label{sse:resgradboosting}

Before, we saw the bagging (bootstrap aggregating) method implemented using a random forest, now we will explore boosting methods.
Boosting is a sequential process combining several (weak) learners into a strong learner. Bootstrap Aggregation is a general procedure used to reduce the variance in algorithms with high variance, for example decision trees. The general idea of boosting is to have predictors that are sequentially trained, each trying to correct its predecessor, that is to say, if model B follows A, B will attempt to correct the the errors made by the previous model A.
The most popular boosting methods are \emph{AdaBoost} (short for Adaptive Boosting) and Gradient Boosting, both are studied next.

\textbf{Adaptive Boosting (\emph{AdaBoost})}

In Adaptive boosting or \emph{AdaBoost}, multiple sequential models are created, each correcting the errors from the last model. \emph{AdaBoost} assigns weights to the observations which are incorrectly predicted and the subsequent model works to predict these values correctly. The \emph{AdaBoost} algorithm works by initially giving equal weight to all observations, then as predictors are evaluated higher weights are given to the data points with wrong predictions. This process is repeated until the error function does not change or the maximum limit of the number of estimators is reached.

Table \ref{tab:ada} shows the classification metrics for the \emph{AdaBoost} classifier.
\begin{table}[H]
\caption{Classification metrics for \emph{AdaBoost} classifier} 
\begin{center} 
\begin{tabular}{lllll}
\hline
\multicolumn{1}{c}{} \\
%\cline{1}
Class & precision & recall & f1-score & support     \\
\hline
0 & 0.90  &    0.99   &   0.94   &    164 \\
1 & 0.33  &    0.05   &   0.09   &    19 \\
\hline
\end{tabular}
\caption{The table shows the main classification metrics for the \emph{AdaBoost} classifier. The metrics for the negative class 0 (non-converter) are excellent but those for the converters are extremely poor due to the data unbalanced problem in our dataset.
} \label{tab:ada} 
\end{center}
\end{table}

Figure \ref{fig:rf_ada} shows the confusion matrix and the ROC for AdaBoost algorithm. 
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{.8\textwidth}
        \centering
        \includegraphics[height=1.7in]{figures/ADA_CM-20-1-5}
        \caption{Confusion matrix and AUC for AdaBoost algorithm with the hyperparameters set to 20 estimators; learning rate 1 and  $max\_depth=5$}
    \end{subfigure}

    
    \caption{Confusion matrix and AUC for the AdaBoost algorithm which as we can see clearly learns the perfectly the training set and the test set for non converters but fails to correctly classify the few converters in the test set. } \label{fig:rf_ada}
\end{figure}

Figure \ref{fig:rf_learning} shows the learning rate for random forest classifier. 
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{.8\textwidth}
        \centering
        \includegraphics[height=2.5in]{figures/ADA_CM-20-1-5_cv7learning}
        \caption{Learning curve of the AdaBoost classifier with  hyperparameters set to 20 estimators; learning rate 1 and  $max\_depth=5$. The algorithm clearly overfits, it will then benefit from more, the other canonical approach for dealing with overfitting -simplifying the model (reduce the number of estimators)- does not help.}
    \end{subfigure}

    
    \caption{Learning curve of the AdaBoost classifier, in red the training score and in green the cross-validation ($cv=3$) score. The learner clearly overfits and more data will likely improve the predictions.} \label{fig:rf_learning}
\end{figure}

Table \ref{tab:adaimp} shows the ranking of the 10 most important features.
\begin{tabular}{ll}
\hline
\multicolumn{2}{c}{AdaBosst most important features for conversionMCI} \\
\cline{1-2}
Feature    & Gini Importance  \\
\hline
dietaketo      & 0.148133         \\
edad$\_$ultimodx  & 0.093430        \\
imc        & 0.087666      \\
eq5deva$\_$visita1 & 0.059781         \\
gds$\_$visita1 & 0.046132           \\
stai$\_$visita1 & 0.039684               \\
a08 & 0.039355             \\
dietaglucemica & 0.036256          \\
renta &  0.034612           \\
numhij & 0.032114     \\

\hline
\label{tab:adaimp}
\end{tabular}


% OJO literal Geron
%One way for a new predictor to improve its predecessor is to try to correct those instances that the predecessor uderfitted. Thus, we can imagine a chain of predictors, one attached to the next, if the first classifier gets many instances wrong, their weight get boosted (initially each predictor has a weight $1/m$, m number of predictors) and the second classifier therefore does a better job on these instances and so on. Once all predictors are trained, the ensemble makes predictions very much like bagging or pasting, except that predictors have different weights depending on their overall accuracy on the weighted training set. This sequential structure of predictors in the the AdaBoost algorithm makes impossible (as it was the case in bagging) to parallelize predictors.
%
%from sklearn.ensemble import AdaBoostClassifier
%ada_clf = AdaBoostClassifier(
%        DecisionTreeClassifier(max_depth=1), n_estimators=200,
%        algorithm="SAMME.R", learning_rate=0.5
%    )
%ada_clf.fit(X_train, y_train)”


\textbf{Gradient Boosting (GDB)}
The idea of gradient boosting originated with Leo Breiman's interpretation of boosting as an optimization algorithm on a suitable cost function. Gradient Descent Boosting optimizes a cost function over function space by iteratively choosing a function (weak hypothesis) that points in the negative gradient direction \cite{friedman2001greedy}. 

Table \ref{tab:gbc} shows the classification metrics for the Gradient Boosting classifier.
\begin{table}[H]
\caption{Classification metrics for GradientBoosting classifier} 
\begin{center} 
\begin{tabular}{lllll}
\hline
\multicolumn{1}{c}{} \\
%\cline{1}
Class & precision & recall & f1-score & support     \\
\hline
0 & 0.90  &    0.99   &   0.94   &  164 \\
1 & 0.0  &    0.0   &   0.0   &   19 \\
\hline
\end{tabular}
\caption{The metrics for the negative class 0 (non-converter) are excellent but the algorithm fails to predict class 1 example due to data unbalanced.
} \label{tab:gbc} 
\end{center}
\end{table}

Figure \ref{fig:rf_gbc} shows the confusion matrix and the ROC for GradientBoosting algorithm. 
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{.8\textwidth}
        \centering
        \includegraphics[height=1.7in]{figures/GBC_CM}
        \caption{Confusion matrix and AUC for GradientBoosting algorithm with the hyperparameters set to $100$ estimators; learning rate $0.1$ and max$\_$depth=5}
    \end{subfigure}
    \caption{Confusion matrix and AUC for the GradientBoosting algorithm which as we can see clearly learns the perfectly the training set and the test set for non converters but fails to correctly classify the few converters in the test set. } \label{fig:rf_gbc}
\end{figure}

Figure \ref{fig:gbc_learning} shows the learning rate for random forest classifier. 
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{.8\textwidth}
        \centering
        \includegraphics[height=2.5in]{figures/GBC_learning}
        \caption{Learning curve of the GradientBoosting classifier with  hyperparameters set to 100 estimators; learning rate $0.1$ and max$\_$depth=3. The algorithm overfits very strongly, it will then benefit from more data. The other canonical approach for dealing with overfitting -simplifying the model (reduce the number of estimators)- does not help here.}
    \end{subfigure}
    \caption{Learning curve of the GradientBoosting classifier, in red the training score and in green the cross-validation ($cv=3$) score. The learner clearly overfits and more data will likely improve the predictions.} \label{fig:gbc_learning}
\end{figure}


\subsection{Deep Networks}
\label{sse:resdeep}

We will use both the sklearn and the Python library Keras to implement deep networks. Keras is a deep-learning framework for Python that provides a convenient way to define and train almost any kind of deep-learning model. 
Keras provides a high-level API for developing deep-learning models supporting different backend including TensorFlow \footnote{TensorFlow is itself wrapping a low-level library for tensor operations called Eigen. On GPU, Tensor-Flow wraps a library of well-optimized deep-learning operations called the NVIDIA CUDA Deep Neural Network library (cuDNN).}, Theano and the Microsoft Cognitive Toolkit (CNTK). Keras is able to run in both CPUs and GPUs. 
%It doesn’t handle low-level operations such as tensor manipulation and differentiation. Instead, it relies on a specialized, well-optimized tensor library to do so, serving as the backend engine of Keras. Rather than choosing a single tensor library and tying the implementation of Keras to that library, Keras handles the problem in a modular way; thus several different backend engines e.g. , Theano backend, and the Microsoft Cognitive Toolkit (CNTK) can be plugged seamlessly into Keras. 

When building a deep learning model there are two key architecture decisions to be made are, How many layers to use and How many units per layer. Additionally, we need to select the activation function for each layer e.g. ReLU, sigmoid. A ReLU function return 0 for negative values and 1 for positive ones and a sigmoid “squashes” arbitrary values into the $[0, 1]$ interval something which can be directly interpreted as a probability.

\textbf{sklearn MLP}
%http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html

Figure \ref{fig:mlp_cm} shows the confusion matrix and the ROC for different MLPs created using a regularization term $alpha = 0.01$ and the solver for weight optimization a quasi-Newton method (lbfs). Adam and stochastic gradient descent optimizers methods have worse performance. Since we use lbfs the classifier does not use minibatches as would be the case for stochastic optimizers
%‘lbfgs’ is an optimizer in the family of quasi-Newton methods.
%‘sgd’ refers to stochastic gradient descent.
%‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, 
%alpha : float, optional, default 0.0001 L2 penalty (regularization term) parameter.
%Size of minibatches for stochastic optimizers. If the solver is ‘lbfgs’, the classifier will not use minibatch. When set to “auto”, batch_size=min(200, n_samples)


\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{1\textwidth}
        \centering
        \includegraphics[height=1.7in]{figures/MLP-1-1}
        \caption{Confusion matrix and the ROC for MLP with 1 hidden layer with only one unit. The network predicts always the majority class (non converter)}
    \end{subfigure}

    \begin{subfigure}[t]{1\textwidth}
        \centering
        \includegraphics[height=1.7in]{figures/MLP-1-10}
        \caption{Confusion matrix and the ROC for MLP with 1 hidden layer with 10 units. The network overfits, provides excellent results for the training set bit less good for the test set especially for the positive class (converters)}
    \end{subfigure}%\quad

    \begin{subfigure}[t]{1\textwidth}
        \centering
        \includegraphics[height=1.7in]{figures/MLP-1-100}
        \caption{Confusion matrix and the ROC for MLP with 1 hidden layer with 100 units. The network overfits, provides perfect results for the training set.}
    \end{subfigure}%\quad

    \begin{subfigure}[t]{1\textwidth}
        \centering
        \includegraphics[height=1.7in]{figures/MLP-16-16-32}
        \caption{Confusion matrix and the ROC for MLP with 3 hidden layer with 16,16 and 32 units each. The network overfits, provides perfect results for the training set.}
    \end{subfigure}%\quad
    
    \caption{Confusion matrix and the ROC for different MLPs. All except the first MLP (1 layer 1 unit) overfits, the MLPm 1 layer/100 units and the deep network with 3 layers with 16,16,12 units per layer learn perfectly the training set. MLPs overfit more data will help improve the \emph{sensitivity} (recall of the positive class).} \label{fig:mlp_cm}
\end{figure}


Figure \ref{fig:mlp_learning} shows the learning rate for MLPs.
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=2.1in]{figures/MLP-1-1_learning}
        \caption{Learning curve for MLP with 1 hidden layer with only 1 unit.}
    \end{subfigure}
 	~   
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=2.1in]{figures/MLP-1-10_learning}
        \caption{Learning curve for MLP with 1 hidden layer with 10 units.}
    \end{subfigure}%\quad
    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=2.1in]{figures/MLP-1-100_learning}
        \caption{Learning curve for MLP with 1 hidden layer with 100 units.}
    \end{subfigure}%\quad
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=2.1in]{figures/MLP-16-16-32_learning}
        \caption{Learning curve for MLP with 3 hidden layers with 16,16,32 units each.}
    \end{subfigure}%\quad
    \caption{Learning curve for  MLPs, in red the training score and in green the cross-validation ($cv=7$) score. MLPs overfit and more data will likely improve the predictions. Interestingly, the deep network (3 hidden layers $16 \times 16 \times 32$ parameters) does not substantially improve the results of the network with one layer and 100 units (100 parameters).} \label{fig:mlp_learning}
\end{figure}




\textbf{Keras}

In Keras, deep learning models are built by putting together compatible layers to form an useful data-transformation pipeline. The snippet \ref{deepkeras} creates a three-layer network, the first and second layers have both 16 units with \emph{ReeLU} activation and the last layer is one neuron with a \emph{sigmoid} activation function to approximate a probability.

%from keras import losses
%from keras import metrics
% model.compile(optimizer=optimizers.RMSprop(lr=0.001),
%               loss=losses.binary_crossentropy,
%               metrics=[metrics.binary_accuracy])
\begin{code}[caption=Deep network 3 layers, label=deepkeras]
from keras import models, layers
#model architecture with 3 layers
model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(X.shape[0],)))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
#optimizer, metric and loss function 
model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])
\end{code}
Without an activation function like \emph{ReLU} (also called a non-linearity), the Dense layer would consist of two linear operations—a dot product and an addition $output = dot(W, input) + b$; so the layer could only learn linear transformations (affine transformations) of the input data: the hypothesis space of the layer would be the set of all possible linear transformations of the input data into a number of units layer-dimensional space. 
In order to get access to a richer hypothesis space (deep representations) we need non-linearity, or activation function, for example \emph{ReeLU}: output = relu(previous output). The optimizer \textit{rmsprop} should work in most situations.
%YS: literal

For the loss function, we can use crossentropy (distance between two probability distributions, in this case between ground-truth distribution and our predictions), mean squared error can also be used. 

In order to monitor during training the accuracy of the model on data it has never seen before, we create a validation set by setting apart the $10\%$ of examples from the original training data. Then we train the model for 20 epochs of the training set \footnote{An epoch is an iteration over all samples in the $x\_train$ and $y\_train tensors$}. At the same time, we monitor loss and accuracy validation samples set apart.
When the dataset are too big that we cannot pass the entire dataset at once we talk about epochs and batches (gradient descent  is an iterative optimization algorithm to calculate minima i.e. compute partial derivatives for each weight and each sample). To reduce this computational burden we can split the dataset into chunks and update the weights of the network at the end of each step. In one epoch or iteration we pass the entire dataset once both forward and backward. Since one epoch is too big (entire dataset) to feed to the computer at once we divide it in several smaller batches.

We need more than one epoch because gradient descent is an iterative algorithm, as the number of epochs increases, more number of times the weight are changed in the neural network and the curve goes from underfitting to optimal to overfitting curve. The right number of epochs depend on the task. Now, you can pass the entire dataset or split it into chunks (batches)
%https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9?gi=f60c6ca67f67
The number of batches is equal to number of iterations for one epoch, let us say we have 1,000 examples with batches of size 250, then we have 4 iterations to complete 1 epoch.
% iterations is the number of batches needed to complete one epoch.

\begin{code}[caption=fitting deep network, label=deepkerasfit]
history = model.fit(partial_x_train, #partial is training - validation
                    partial_y_train, 
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val)) 
                    #compute loss and accuracy on the validation samples
\end{code}

As it was mentioned in Section \ref{se:deep} the creation of an validation set helps dealing with the critical problem in machine learning -overfitting. We split the data into training set and validation set (eg $10\%$) but since we have so few data points (less than 100) we encounter the problem that the validation score has a high variance with regard to the validation split, that is, the score depends a lot on which data points you use to validate (add to this that the set is imbalance we may have by chance a validation set with 0 converters).

%figures here
Figure \ref{fig:kerasmetricsres} shows the validation metrics for k-fold, $k=4$, learning in a deep network with two hidden layers of 16 units each with ReLU activation with drop out regularization, Adam optimizer and binary cross entropy as loss function. The network in order to deal with the unbalanced problem (underrepresentation of positive class) gives 7 times more importance to positive class than to negative class. this will have consequences in precision and recall shown in the figure. The Cophenetic Correlation Coefficient of the Ward linkage is 0.563 and for Centroid linkage is 0.724 (The closer the value is to 1, the better the clustering preserves the original distances).

\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=1\linewidth]{figures/Keras_KFOLD4}
        \caption{Clockwise: accuracy, loss, precision and recall metrics for a deep network with two layers and 6 units each. The network starts overfitting after 20 epochs. The accuracy is closed to 0.8. Although precision increases the values are very small, the recall is maximum 1. The precision is the proportion of positive identifications correctly identified and the recall tells us what proportion of actual positives was identified correctly. This means that the network identifies all the positive cases but it identifies positive cases in excess which is due to the fact that the network gives 7 times more importance to positive class than to negative class. 
        } \label{fig:kerasmetricsres}
\end{figure}

The results produced by the implementation of deep network using both sklearn and keras are shown in Section \ref{sse:resdeep}.

\subsection{Dimensionality reduction and Unsupervised learning}
\label{sse:resunsupervised}
Here we show the results of unsupervised learning methods in the Vallecas dataset. First we explore hierarchical clustering (Section \ref{sse:reshierarchical}) and later we delve into principal component analysis via an eigen-decomposition of the covariance matrix (Section \ref{sse:reshierarchical}) and via singular value decomposition (SVD) of the data matrix \ref{sse:resPCA}.

\subsubsection{Hierarchical clustering}
\label{sse:reshierarchical}

The dendrogram the and distance matrix are shown in figure \ref{fig:dendrogram}. We use two different agglomerative hierarchical clustering methods, centroid and Ward linkage. In the centroid method the distance between two clusters is the distance between the two mean vectors of the clusters, the Ward method on the other hand does not directly define a measure of distance between two points or clusters, it rather performs a one-way univariate ANOVAs for each variable. At each step, the two clusters that provide the smallest increase in the combined error sum of squares will merge. 
\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=1\linewidth]{figures/dendrogram}
        \caption{Dendrogram and distance matrix calculated with the Euclidean distance metric and using two different linkage algorithms, centroid and Ward. Both are agglomerative hierarchical clustering methods, that is, each data point is initially a cluster to iteratively combine the existing clusters at each step, resulting always in one final all inclusive cluster. The heatmap depicts the Euclidean distance of the data matrix.
        } \label{fig:dendrogram}
\end{figure}


\subsubsection{Principal component analysis}
\label{sse:resPCA}
Next we show linear dimensionality reduction using both eigen-decomposition (snippet \ref{eigendecomposition}) and truncated singular value decomposition (SVD) (snippet \ref{svdsnippet}). 

\begin{code}[caption=PCA eigenvectors, label=eigendecomposition]
from sklearn.decomposition import PCA
pca = PCA().fit(X)
PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)
# n components=None, all components are kept
# svd solver='auto', depending on the input data size and the number of components randomized
# or full SVD is computed.
# whiten=False, the signal is not transformed (unit component-wise variances)
\end{code}

Figure \ref{fig:PCA_variance} shows the explained variance with PCA. To explain $70\%$ of the total variance we need 14 dimensions and to explain the $95\%$ we need 37 features for a total dimensional input of 57. PCA could help to improve the results of the machine learning classifiers, replacing the data input by the principal components. 

\begin{figure}[H] 
    \centering
    \begin{subfigure}[t]{.4\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/PCA70_variance}
        \caption{With 14 dimensions we can explain $70\%$ of the total variance of the 57 dimensional input.}
    \end{subfigure}
    \hfill %%
    \begin{subfigure}[t]{.4\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/PCA95_variance}
        \caption{With 37 dimensions we can explain $95\%$ of the total variance of the 57 dimensional input.}
    \end{subfigure}%\quad
    \caption{Dimensionality reduction with PCA shows potential to summarize data. We need only 14 ($24\%$ of the input dimensions) to explain  $70\%$ of the variance of the input space. The PCA explained variance ratio of the three first components is 0.112, 0.093, 0.072 (eigenvectors with the highest variance).} \label{fig:PCA_variance}
\end{figure}

Before we calculated PCA using the eigen-decomposition, dimensionality reduction can also be calculated using truncated SVD (aka latent semantic analysis or LSA. We set the number of components to 2 (snippet \ref{svdsnippet}).  SVD is the generalization of the eigen-decomposition of a positive semidefinite normal matrix (seen above) to any matrix via an extension of the polar decomposition. 
The SVD represents an expansion of the original data in a coordinate system where the covariance matrix is diagonal. A SVD consists in calculating the matrix of singular values S ( singular values) from the data matrix A, $A = USV$
Truncated SVD does not need to center the data before computing the decomposition and it can efficiently work with sparse matrices (this is why it is called truncated). 

\begin{code}[caption=PCA truncated SVDs, label=svdsnippet]
svd = TruncatedSVD(n_components=2)
X_reduced = svd.fit_transform(X_all)
\end{code}

Figure \ref{fig:PCASVD} shows the scatter plot of the SVD for 2 dimensions, in yellow the converters and in indigo the non converters. Two dimensions are not enough to identify cluster of converters and non converters. It ought to be emphasized that dimensionality reduction (e.g. PCA or SVD) is distinct from clustering but it still can help clustering high-dimensional data by reducing to a useful low dimensional representation.

\begin{figure}[H] 
        \centering
        \includegraphics[keepaspectratio, width=0.5\linewidth]{figures/SVD_2D}
        \caption{Scatter plot of single value decomposition with two components. The explained variance ratio of the components is 0.1132 and 0.0312, making a total explained variance ratio of  0.144. The singular values of the covariance matrix S are 98.21 20.26. Although the scatter plot is not useful as a clustering method i.e. separate converter from non converter in a 2D space, this dimensionality reduction procedure is however helpful to reduce the original dataset to a useful low dimensional representation to feed a classifier. 
        } \label{fig:PCASVD}
\end{figure}

\subsubsection{Manifold learning}
\label{sse:resmanifold}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Here we show the results of implementing the t-SNE algorithm described in Section \ref{sse:manifold} to visualize high dimensional data space.
%There’s a reason that t-SNE has become so popular: it’s incredibly flexible, and can often find structure where other dimensionality-reduction algorithms cannot. Unfortunately, that very flexibility makes it tricky to interpret. Out of sight from the user, the algorithm makes all sorts of adjustments that tidy up its visualizations. Don’t let the hidden “magic” scare you away from the whole technique, though. The good news is that by studying how t-SNE behaves in simple cases, it’s possible to develop an intuition for what’s going on.


Figure \ref{fig:TSNE} shows the scatter plot of the tSNE algorithm that creates a two-dimensional maps from original dataset of 57 dimensions. In red the converters and in blue the non converters. The hyperparameters were set to perplexity =30, the learning rate is 200 and the  gradient calculation algorithm is the Barnes-Hut approximation.

\begin{figure}[H] 
        \centering
        \includegraphics[keepaspectratio, width=0.5\linewidth]{figures/TSNE}
        \caption{Scatter plot of tSNE algorithm that creates a two-dimensional maps from original dataset of 57 dimensions. 
        } \label{fig:TSNE}
\end{figure}

%class sklearn.manifold.TSNE(n_components=2, perplexity=30.0, early_exaggeration=12.0, learning_rate=200.0, n_iter=1000, n_iter_without_progress=300, min_grad_norm=1e-07, metric=’euclidean’, init=’random’, verbose=0, random_state=None, method=’barnes_hut’, angle=0.5)[source]¶

\subsection{Comparative evaluation of models}
\label{se:compare}
In this section we will try to construct a coherent view of the performance of the different predictive algorithms shown above. 
%YS change this
Prior to try to identify the best model for the task at hand we will comment on the performance metrics and validation procedures used. In particular we will delve into cross validation reduce variance in the validation set and will describe how to interpret the metrics provided in the Results section. The impatient reader can go directly to Section \ref{sse:permets} which contains the comparative analysis for the different models implemented.

Evaluating a model always boils down to splitting the available data into three sets: training, validation, and test. The learner is  trained on the training data and the model is evaluated on the validation set. The last step is ot test the model with the test set.  
Why not just two sets: training and testing? Because adjusting the optimal configuration of the model (parameter and hyperparameters) is a feedback loop, the performance of the model on the validation set is the feedback signal.
%Results for each classifier. We will realize that the challenge of fitting the training data differs very drastically from the challenge of finding patterns that generalize to new data \cite{goodfellow2016deep}.


As it was noted int the previous section we trained the \emph{Vallecas} dataset using a split of $80\%$ for the training set and $20\%$ for test set. The validation set was $10\%$ of the test set which gives us only 73 samples, a small number so the validation scores might have a high variance with regard to the validation split possibly preventing us from reliably evaluating the model. The best practice to deal with this situation is to use K-fold cross validation, split the available data into K partitions (e.g. between 3 and 7) instantiating K identical models and training each one on K-1 partitions while evaluating on the remaining partition. That validations core is the average of the K validation scores. Figure \ref{fig:xvalchollet} represents the cross validation process to effectively deal with high variance in the validation set \cite{chollet2017deep}. 
\begin{figure}[H]
        \centering
        \includegraphics[keepaspectratio, width=.8\linewidth]{figures/xvalidationchollet}
        \caption{Train the model in the training set and evaluate your model on the validation data. Finally, the model is tested on the test set. We need to K-fold cross validation to avoid information leaks. Evaluating on the validation set and modifying it as a result of the evaluation will leak an increasingly significant amount of information about the validation set into the model, ending up with a model that performs artificially well on the validation data (the model was optimized to perform well on the validation set). Cross validation reduces this high variance problem.} \label{fig:xvalchollet}
        %You train on the training data and evaluate your model on the validation data. Once your model is ready for prime time, you test it one final time on the test data. Cross validation  information leaks running one experiment, evaluating on the validation set, and modifying your model as a result—then you’ll leak an increasingly significant amount of information about the validation set into the model At the end of the day, you’ll end up with a model that performs artificially well on the validation data, because that’s what you optimized it for
\end{figure}

Once we have clarified the need of the k-fold cross validation shown in the Results Section we will comment on the evaluation criteria and the challenges of adopting criteria for the \emph{Vallecas} dataset.

Roughly speaking, there are two factors that tell us how well our machine learning is doing:
\begin{itemize}
\item Underfitting: the error on the training set is too large.
\item Overfitting: the gap between the error in the training and test sets is too large. 
\end{itemize}

Ideally, we want to build a classifier that is sufficiently complex to do not underfit but not as complex as to memorize the training set. This trade-off between optimization and generalization is the main issue in machine learning. Optimization refers to the process of adjusting a model to get the best performance possible on the training data (the learning in machine learning), whereas generalization refers to how well the trained model performs on data it has never seen before. Validation is used in order to deal with information leakage that could drive overfitting in the model.

In the Results section we make extensive use of the \textbf{confusion matrix}, a very helpful mathematical object to study model performance. For a binary prediction task the confusion is a $2 \times 2$ matrix where rows represent the true values and the columns the predicted values. Thus, the values in the diagonal are true predictions and those off-diagonal are predictions that the model got wrong. From the confusion matrix \emph{accuracy, recall, precision} and $F_\beta$ metrics are easily computed.
%from sklearn.metrics import confusion_matrix
Let the confusion matrix M,  
\begin{equation*}
M=
  \begin{bmatrix}
    TN & FP  \\
    FN & TP 
  \end{bmatrix}
\end{equation*}
then, 
\begin{equation*}
\begin{split}
&\text{Accuracy} = \frac{TN + TP}{TN+TP+FN+FP} \\
&\text{Recall} = \frac{TP}{TP+FN} \\
&\text{Precision} = \frac{TP}{TP+FP} \\
&\text{Specificity} = \frac{FP}{FP+TN} \\
&F_1 = 2\frac{\text{precision*recall}}{\text{precision}+\text{recall}} = \frac{2 * TP}{2TP + FN + FP}
\end{split}
\end{equation*}
\emph{Recall}, also called \emph{true positive rate}, \emph{sensitivity} and \emph{probability of detection} is the fraction of all positive instances that the classifier identifies correctly as positive. In biomedical applications in which is crucial to do not miss the positives (converters) recall is a very important metric.
\emph{Precision} is the fraction of positive predictions that are correct (finding sick when they truly are)
\emph{Specificity} or \emph{false positive rate} is the fraction of all negative instances that the classifier incorrectly identifies as positive (here the smaller the better).
\emph{Precision} and \emph{Recall} are complementary in the sense that you can increase the precision but at the expense of the recall and viceversa. Biomedical (e.g. tumor detection) applications are \emph{Recall} focused. \emph{Precision} oriented tasks are for example document classification. 

The $F_{\beta}=1$ score combines both precision and recall into a single number computed with the harmonic mean of precision and recall, More generally we can modulate how much emphasis we do to precision versus recall using the parameter $\beta$. 
If the task is precision oriented, $\beta=0.5$, false positives hurts performance more than false negatives. If recall oriented, $\beta=2$, false negatives hurts performance more than false positives. Formally, 
\begin{equation*}
\begin{split}
F_{\beta} = (1+\beta^2)\frac{\text{precision*recall}}{(\beta^2 * precision)+recall} 
\end{split}
\end{equation*}
%=\\ \frac{2 * TP}{2TP + FN + FP}

%from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
%print('Accuracy:{:.2f}'.format(accuracy_score(test, predicted)));precision_score, recall_score;f1_score
%print(classification_report(test,pred,target_names=['not converter 0', 'converter 1']))
%varying the decision threshold for predict_proba https://www.coursera.org/learn/python-machine-learning/lecture/0YPe1/classifier-decision-functions

Receiving Operating Characteristic ROC curves are also extensively used in the Results Section. The area under the curve (AUC) is a way to summarize the classifier performance in one single number. Note that ROC AUC is a good metric only for balanced-classification problems. The false positive rate or precision is represented in the (x-axis) and the true positive rate or recall is represented in the (y-axis). An ideal classifier would achieve ideal precision (1.0) and ideal recall (1.0) (the top right corner, $AUC=1$).

Other metrics that will be shown in Section \ref{sse:permets} are:
\begin{itemize}
	\item Hamming loss: Calculates the Hamming (distance) between two sets
	\item Jaccard similarity: average of Jaccard similarity coefficients (Jaccard index)
	\item Hinge loss: computes the average distance between the predictions and the data using hinge loss (only prediction errors) Used in SVM
	\item Matthews correlation coefficient or phi coefficient: it is regarded as a balance measure useful even if the classes are of very different sizes as it is our case here. The range is $[-1,1]$, where +1 is perfect prediction, 0 average random prediction and -1 inverse prediction \cite{boughorbel2017optimal}. %It is also called phi coefficient.
	%from sklearn import matthews_corrcoef, matthews_corrcoef(y_pred, y_true) # tp*tn - fp*fn/ sqrt()
	\item Zero one loss: computes the sum or average of the 0-1 classification loss $L_{0-1}$ over n samples. By default normalizes over the sample
\end{itemize}

%http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
Whether we are doing classification both continuous and discrete or clustering we need to study the behavior of our model prediction, that is, model selection will rely on model evaluation criteria. Evaluation criteria consists in computing scoring objects that gives us information about model performance. It ought to be remarked that scoring is not dataset independent, for example, accuracy may be an optimal indicator of model performance in some situations but suboptimal in others. For example, in a binary classification problem -converter vs not converter- in which the converters are a small minority (e.g. $10\%$, accuracy is not the best measure to use because accuracy is just the number of correct prediction divided by the total number of instances. A dummy classifier that doesn't look at the features at all and always predict the most frequent class (i.e. non converter) will have an accuracy of $90\%$, that is, the dummy classifier will predict the right label for 90 out of 100 examples. 
%https://www.coursera.org/learn/python-machine-learning/lecture/BE2l9/model-evaluation-selection
Dummy classifiers provide a null metric and work as a sanity check on the model's performance. Whenever the model's performance is close to the null accuracy baseline of a dummy classifier we likely are in any of these cases:
\begin{itemize}
	\item Features are ineffective or missing.
	\item Poor choice of hyperparameters or kernel in the model (eg. in SVM) 
	\item Large class imbalance 
\end{itemize}
%Scoring objects take care of that, as a rule,  higher return value better than lower return values.

The \emph{Vallecas} dataset falls into the third situation: class imbalance, that is, the converter class is underepresented. We will deal with the class imbalance problem specifically in Section \ref{se:imbalance}.

\subsubsection{Model selection in \emph{Vallecas} dataset}
\label{sse:permets}

Figure \ref{fig:dummies} shows how the different models perform, comparing accuracy with three dummy predictors: random predictor, majority predictor and minority predictor on the \emph{Vallecas} dataset. 
\begin{figure}[H] 
        \centering
        \includegraphics[keepaspectratio, width=1\linewidth]{figures/compareagainstdummies2}
        \caption{Comparative analysis of the accuracy for the built predictors (in blue) compared against dummy predictors (in red) for accuracy score. From left to right: naive Bayes, logistic regression, extreme gradient boosting, random forest, AdaBoost, SVM, k-NN and decision tree. All the classifiers show better accuracy score than random classifier (uniform predictions). However, a dummy predictor that always predict the majority class i.e. non converter has better accuracy than some predictors e.g. logistic regression, naive Bayes and others. Ensemble methods -random forest, extreme gradient boosting and AdaBoost have the best accuracy score.
        } \label{fig:dummies}
\end{figure}

As it was already mentioned the accuracy score is suboptimal for imbalanced datasets. In the case that the number of positive and negative classes are very different e.g. not in the same order, more meaningful measure is the Matthew’s Correlation Coefficient. Other measures such as Cohen's kappa, recall, precision, F1, ROCAUC and log loss are explored. 
Figure \ref{fig:mat4dummy} shows the score for all these measures for different classifiers, the scores are for both the training set (top row) and the test set (bottom row). 
%For clarity, Figure \ref{fig:mat4dummy} shows the scores a first group of predictors (naive Bayes, k-NN, logistic regression and SVM) the scores of the rest of classifiers (decision tree, ensemble methods and deep networks) are depicted in figure \ref{fig:restmat4dummy}.

The best test scores of the four predictors shown in Figure \ref{fig:mat4dummy} is the SVM with non linear kernel. The k-NN has the best accuracy but it clearly overfits(see training (first row) metrics). The nonlinear SVM classifier outperforms the other classifiers in the Matthews coefficient which is considered the best metric for imbalanced datasets like the \emph{Vallecas}. Nonlinear SVM outperforms also the other three classifiers -naive Bayes, logit, k-NN- in Cohen's kappa, precision, F1 and ROC AUC metrics. It is worth noting that these results may be susceptible to change if we change the hyperparameters of the models. For example, in k-NN we used k=3 which is the optimal k for the cross validation implemented of 5 folds. For $k=1$ the Matthews coefficient improves but it is till worse than for SVM and logit.
\begin{figure}[H] 
        \centering
        \includegraphics[keepaspectratio, width=1\linewidth]{figures/allmetrics4}
        \caption{Comparative analysis of four predictors -naive Bayes, logit, SVM and k-NN- with different classification metrics. In the top row the score for the training set and on the bottom row for the test set. The k-NN has the best accuracy but at the same time has the worst Matthews coefficient. The top row (training metrics) clearly shows that k-NN overfits. Based on the results the best predictor of this group is the SVM with RBF kernel, it has the best scores for Matthews coefficient, Cohen's kappa, precision, F1 and ROC AUC. 
        } \label{fig:mat4dummy}
\end{figure}

Figure \ref{fig:allmetricsensemble} shows the test scores for the predictors decision tree and ensemble methods -random forest, XGB, AdaBoost and voting classifier. The predictors shown in this figure are all ensemble methods except the decision tree which is however the basic unit  of random forests. XGB and AdaBosst are both boosting algorithms which in essence combine weak (slightly better than random) learners to create a highly accurate prediction. Random forest is an ensemble of many decision trees and the Voting classifier is an ensemble of the learners shown in figure \ref{fig:mat4dummy} -naive Bayes, logit, SVM and k-NN- implemented using hard voting (soft voting votes have a weight in hard voting votes are binary).
%An AdaBoost [1] classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.

\begin{figure}[H] 
        \centering
        \includegraphics[keepaspectratio, width=1\linewidth]{figures/allmetricsensemble}
        \caption{Comparative analysis of ensemble predictors -extreme gradient boosting, random forest, AdaBoost, Majority Voting- and decision tree. Of the two boosting classifiers, XGB and Ada, Ada shows better performance. The best Matthews coefficient is for the Majority Voting classifier which is an ensemble of all the classifiers shown in Figure \ref{fig:mat4dummy} (SVM dominates). The hyperparameters of the random forest classifier (red) are n$\_$estimators $= 1000$, min$\_$samples$\_$leaf $= 8$ and the max$\_$features is set the default i.e. $\sqrt{features} \sim 8$. The extreme gradient boosting algorithm (yellow) has the best accuracy $\sim 0.88$. Note that the boosting algorithms are implemented with the hyperparameters set to n$\_$estimators $= 100$, learning rate $=0.001$ and max$\_$depth $=5$; but it could be possible to find another configuration with improved results.} \label{fig:allmetricsensemble}
\end{figure}

Finally we evaluate the performance of deep networks (multi layer perceptrons). TO DO
%YS


\section{The class imbalance problem}
\label{se:imbalance}
The class imbalance problem -the number of samples of one class being under represented compared to another class- is predominant in many scenarios, in particular when dealing with anomaly detection, for example, fraudulent transaction, electrical grid fault, electrical theft, conversion to disease  etc. In this situation, a predictive model that does not take into account this could be very unreliable \cite{he2009learning}.
%The imbalance problem needs to be addressed in the design phase with an emphasis on improving the identification of the minority class as opposed to achieving higher overall accuracy.
How imbalanced is our data set can be easily quantified with the balancing ratio, r. Let X be an imbalanced set with $X_{min}$ the subset of the undersampled class and $X_{maj}$ the subset of the majority class. The balancing ratio \emph{r}:
\begin{equation}
r_X = \frac{X_{min}}{X_{maj}} \sim 0.1
\end{equation}
%Our dataset the imbalance is around 0.1 which is far from what is more often considered as imbalance with ratios as skewed as 100:1, 1,000:1 or 10,000:1.

The goal is to have high accuracy in the minority class without jeopardizing the accuracy of the majority class. Using machine learning algorithms off the box when one class dominates the other, as it is the case in the \emph{Vallecas} dataset (the balancing ratio \emph{r}), comes with a warning because any algorithm could just classify everything as non converter and still be right the $90\%$ of the time. Standard classifier algorithms e.g. Logistic Regression, Decision trees etc. have a bias towards the majority class, that is, they tend to only predict the majority class data and the features of the minority class are treated as noise and are often ignored. Note that classifiers can also learn from imbalanced data sets, some studies have found comparable result for imbalanced datasets and the same balanced using sampling techniques \cite{japkowicz2002class}.

%https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/
The balancing process is equivalent to resample the dataset in such a way that the balancing ratio increases \emph{r}, we can do so under sampling the majority class and or over sampling the minority class. Luckily, there are at least four ways to deal with this problem (there is of course a fifth option which is doing nothing about it). 
\begin{enumerate}
	\item Synthesis of new minority class instances, eg. SMOTE algorithm creates artificial data based on the feature similarities between existing minority samples. %Sampling (synthetic data, algorithms are SMOTE, SMOTEBoost and EasyEnsemble)
	\item Over sampling the minority class to achieve data balancing by generating new points in $X_{min}$.
	\item Under sampling the majority class to reduce the number of samples in $X_{maj}$, for example cleaning the majority space by eliminating redundant data points.
	\item Tweak the cost function to make misclassification more costly for the minority class.
\end{enumerate}
We used the fourth option (tweak the cost function) with those machine learning algorithms (decision tree, random forest, SVM, SGD, logit) that have the \textit{class\_weight} parameter for unbalanced datasets. The idea is that the cost of misclassifying a minority class instance is worse than misclassifying the majority class. 
As the Results Section showed this approach is not enough to achieve satisfactory metrics for the converter class (very poor \emph{sensitivity} or recall of the positive class). Although penalizing misclassification of the under sampled class might help, the approach is still lacking, for example using a N-fold cross validation the validation set may contain none or very few instances.of converters.

In this section we will address the skewness of the dataset in terms of labels using a different approach, namely creating new minority class instances, implemented using the Synthetic Minority Over-sampling Technique (SMOTE) algorithm. The SMOTE algorithm synthesizes new minority instances of existing (real) minority instances, this new synthetic dataset is used to train the classifier. SMOTE mitigates the problem of overfitting because it does not mimic minority instances it rather generates new ones, another advantage is that the there is no information loss. The disadvantages of SMOTE are that the algorithm does not take into account neighboring examples across classes which could introduce noise, SMOTE is not very effective for high dimensional data and it may suffer from  over generalization and variance \cite{wang2004imbalanced}.

The SMOTE algorithm takes two parameters: \textit{K}, how many of the closest neighbors are considered for synthesis and \textit{dup\_size}, how many times existing data points get reused for synthesis, being zero is a special case leading to balanced classes. The algorithm loops through the existing, real minority instance and at each loop iteration, one of the \textit{K} closest minority class neighbors is chosen and a new minority instance is synthesized somewhere between the minority instance and that neighbor. For example, if \textit{dup\_size} = 20 and we have only 10 minority instances, the SMOTE algorithm will create 200 new data points of the minority class. Specifically, to create a synthetic sample SMOTE randomly selects one of the K-nearest neighbors (eg under Euclidean distance), then multiply the feature vector with a random number between 0 and 1 and finally add this vector to x
\begin{equation}
x_{new} =  x_i + \delta * (\hat{x_i} - x_i)
\end{equation}
where $x_{new}$ is the synthetic new point, $x_i$ is the minority instance and $\hat{x_i}$ is one of the k nearest neighbors. Thus, the resulting new point is along the line between $x_i$ and the randomly selected neighbor $\hat{x_i}$. 
%Thus, the \textit{dup\_size} parameter is the number of times the algorithm should loop through the existing real minority instances. 
 
%YS: Tomek links
SMOTE can be complemented with data cleaning techniques, notably Tomek links \cite{tomek1976two} to reduce the overlapping (new instances become 'tied' together leading to overfitting \cite{mease2007boosted}) that is introduced in the sampling methods.
A Tomek link is a pair if minimally distanced nearest neighbors that belong to different classes. By eliminating the Tomek links (pairs of points that are two close or noise) we cleanup the undesired overlapping that the synthetic sampling may introduce. After the Tomek links are removed all the minimally distanced nearest neighbors belong to the same class. Then, if we remove the overlapping points it could be possible to establish well-defined clusters. 


\subsection{Results with the synthetic balanced dataset generated with SMOTE}
\label{se:resimbalance}

Figure \ref{fig:dummiesSMOTE} shows how the different models perform, comparing accuracy with three dummy predictors: random predictor, majority predictor and minority predictor on the synthetic (balanced) \emph{Vallecas} dataset using the SMOTE algorithm. This figure needs to be compared with the performance in the original (unbalanced) dataset shown in Figure \ref{fig:dummies}.
\begin{figure}[H] 
        \centering
        \includegraphics[keepaspectratio, width=1\linewidth]{figures/compareagainstdummies2-SMOTE}
        \caption{Comparative analysis of the accuracy for the built predictors (in blue) compared against dummy predictors (in red) for accuracy score. The SMOTE algorithm created a balanced dataset adding minority class points.   
        From left to right: naive Bayes, logistic regression, extreme gradient boosting, random forest, AdaBoost, SVM, k-NN and decision tree. All the classifiers show better accuracy score than all the dummy classifiers (contant '0', constant '1' and uniform predictions). The boosting classifiers (extreme gradient boosting and AdaBoost) are the best ones. Note that this also happened in the original (unbalanced) dataset.
        } \label{fig:dummiesSMOTE}
\end{figure}


Figure \ref{fig:allmetricsensembleSMOTE} shows the test scores for the predictors in the synthetic balanced dataset created with the SMOTE algorithm. The Extreme Gradient boosting is the best classifier having the best performance in all metrics with the only exception of log loss, closely followed by the other two ensemble classifiers: AdaBoost and Random Forest. 

\begin{figure}[H] 
        \centering
        \includegraphics[keepaspectratio, width=1\linewidth]{figures/allmetrics_last_SMOTE}
        \caption{Comparative analysis of ensemble predictors in the synthetic balanced dataset created with the SMOTE algorithm. The top row shows the performance in the training set and the bottom row in the test test. The learners are shown in different colors, form left to right: random forest, Gaussian naive Bayes, AdaBoost, SVM, k-NN, extreme gradient boosting, decision tree and logistic regression. The ensemble learners dominate, in particular  extreme gradient boosting reaching an impressive 0.85 in all metrics (except log loss)} \label{fig:allmetricsensembleSMOTE}
\end{figure}

The metrics for the different algorithms applied to the synthetic balanced dataset generated via the SMOTE algorithm as shown bellow.
Logistic regression in Table \ref{tab:logreg_smote}, k-NN in Table \ref{tab:k-NN_smote}, non linear SVM in Table \ref{tab:svm_smote}, Gaussian naive Bayes in Table \ref{tab:nb_smote}, decision tree in Table \ref{tab:dt_smote} and ensemble algorithms: random forest \ref{tab:rf_smote}, AdaBoost in Table \ref{tab:ada_smote} and extreme gradient boosting \ref{tab:xgb_smote}.
The best results are obtained in the Boosting algorithms, bagging (via SMOTE) and adaptive boosting with the AdaBoost algorithm (Table \ref{tab:ada_smote}) or extreme gradient boosting (Table \ref{tab:xgb_smote}). 
%YS: falta extreme gradient

%logreg
\begin{table}[H]
\caption{Classification metrics for logistic regression} \label{tab:logreg_smote} 
\begin{center} 
\begin{tabular}{cccccccc}
\hline
\multicolumn{1}{c}{} \\
%\cline{1}
Class & precision & precision \textit{SMOTE}& recall & recall \textit{SMOTE}& f1-score & f1-score \textit{SMOTE} & support     \\
\hline
0 & 0.92  &  0.57 &  0.70  & 0.73 &   0.79   &  0.64 &  164 \\
1 & 0.15  &  0.62 &  0.47 &  0.45  &  0.23   &  0.52  &  19/164 \\
\hline
\end{tabular}
\caption{The table shows classification metrics for the logistic regression classifier for both the original test set and the synthetic balanced dataset generate with the SMOTE algorithm. Precision of the positive class goes down for the negative class and goes up for the positive class. Recall is mostly unaffected. The 10-fold cross validation average accuracy is 0.796}
\end{center}
\end{table}

%kNN
\begin{table}[H]
\caption{Classification metrics for k-NN classifier} \label{tab:k-NN_smote} 
\begin{center} 
\begin{tabular}{cccccccc}
\hline
\multicolumn{1}{c}{} \\
%\cline{1}
Class & precision & precision \textit{SMOTE}& recall & recall \textit{SMOTE}& f1-score & f1-score \textit{SMOTE} & support     \\
\hline
0 & 0.90  &  0.58 &  0.98  & 0.59 &  0.94   &  0.58 &  164 \\
1 & 0.20  &  0.58 &  0.05  & 0.57 &  0.08   &  0.57  &  19/164 \\
\hline
\end{tabular}
\caption{The table shows classification metrics for the k-NN classifier for both the original test set and the synthetic balanced dataset generate with the SMOTE algorithm. The k-NN algorithm overfits very clearly in the original dataset, in the new synthetic dataset the algorithms does slightly better than a random classifier, the accuracy of the K-NN (k=5) is 0.579.}
\end{center}
\end{table}

%SVM RBF 
\begin{table}[H]
\caption{Classification metrics for SVM classifier with RBF kernel} \label{tab:svm_smote} 
\begin{center} 
\begin{tabular}{cccccccc}
\hline
\multicolumn{1}{c}{} \\
%\cline{1}
Class & precision & precision \textit{SMOTE}& recall & recall \textit{SMOTE}& f1-score & f1-score \textit{SMOTE} & support     \\
\hline
0 & 0.93  & 0.63  &  0.79  & 0.88 &  0.85    &  0.73 &  164 \\
1 & 0.20  & 0.80  &  0.47  & 0.48 &  0.29    &  0.60  &  19/164 \\
\hline
\end{tabular}
\caption{The table shows classification metrics for the non linear SVM classifier with RBF kernel. The precision in the synthetic dataset is notably improved, the accuracy in the balanced dataset is 0.68.}
\end{center}
\end{table}

%kNN
\begin{table}[H]
\caption{Classification metrics for naive Bayes classifier} \label{tab:nb_smote} 
\begin{center} 
\begin{tabular}{cccccccc}
\hline
\multicolumn{1}{c}{} \\
%\cline{1}
Class & precision & precision \textit{SMOTE}& recall & recall \textit{SMOTE}& f1-score & f1-score \textit{SMOTE} & support     \\
\hline
0 & 0.91  & 0.74  &  0.63  & 0.54 &  0.74   &  0.63 &  164 \\
1 & 0.13  & 0.64 &   0.47  & 0.81 &  0.20   &  0.72 &  19/164 \\
\hline
\end{tabular}
\caption{The table shows classification metrics for the naive Bayes classifier in the test set and the synthetic balanced dataset generated via the SMOTE algorithm. The metrics of the positive class are greatly enhanced, on the contrary and as expected the negative class deteriorate in the balanced dataset. The accuracy of the GaussianNB classifier in the balanced test set is 0.68.}
\end{center}
\end{table}

%DT
\begin{table}[H]
\caption{Classification metrics for decision tree classifier} \label{tab:dt_smote} 
\begin{center} 
\begin{tabular}{cccccccc}
\hline
\multicolumn{1}{c}{} \\
%\cline{1}
Class & precision & precision \textit{SMOTE}& recall & recall \textit{SMOTE}& f1-score & f1-score \textit{SMOTE} & support     \\
\hline
0 & 0.91  & 0.74  & 0.71   & 0.73  &  0.80   &  0.73 &  164/164 \\
1 & 0.13  & 0.73  & 0.37  & 0.74  &   0.19   & 0.74 &   19/164 \\
\hline
\end{tabular}
\caption{The table shows classification metrics for the decision tree in both the original test set and the synthetic balanced dataset generated with the SMOTE algorithm. The metrics of the positive class are greatly enhanced, on the contrary and as expected the negative class deteriorate in the balanced dataset but not as much as it is gained in the positive  class. The accuracy of the decision tree classifier on the balanced test set is 0.73.}
\end{center}
\end{table}

%Random forest

\begin{table}[H]
\caption{Classification metrics for random forest classifier} \label{tab:rf_smote} 
\begin{center} 
\begin{tabular}{cccccccc}
\hline
\multicolumn{1}{c}{} \\
%\cline{1}
Class & precision & precision \textit{SMOTE}& recall & recall \textit{SMOTE}& f1-score & f1-score \textit{SMOTE} & support     \\
\hline
0 & 0.89 & 0.84  &  0.93 & 0.97 &  0.91   & 0.90  &  164 \\
1 & 0.00  & 0.96  & 0.00   & 0.82 &  0.00   & 0.88   & 19/164 \\
\hline
\end{tabular}
\caption{The table shows classification metrics for the logistic regression classifier in the test set and the synthetic dataset generate with the SMOTE algorithm. Precision, recall and. F1 score for the n¡balanced dataset suffer are tremendously improved eg. from 0 sensitivity to 0.82. Accuracy of Grid-RandomForestClassifier classifier on test set 0.89 with the hyperparameters calculated using GridSearch: estimators:1000 and minimum number of sample in a leaf equals to 8.}
\end{center}
\end{table}

%Ensemble Voting
% \begin{table}[H]
% \caption{Classification metrics for Ensemble (hard voting) classifier} \label{tab:voting_smote} 
% \begin{center} 
% \begin{tabular}{cccccccc}
% \hline
% \multicolumn{1}{c}{} \\
% %\cline{1}
% Class & precision & precision \textit{SMOTE}& recall & recall \textit{SMOTE}& f1-score & f1-score \textit{SMOTE} & support     \\
% \hline
% 0 & 0.91  & 0.71  &  0.90  & 0.80 &  0.91   &  0.75&  164 \\
% 1 & 0.20  & 0.77 &  0.21  & 0.66 &  0.21   &  0.71  & 19/164 \\
% \hline
% \end{tabular}
% \caption{The table shows classification metrics for the  the Ensemble classifier using hard voting for prediction. The metrics for the positive class 1 (converter) strongly improve, while for the negative class 0 (non-converter) decreases moderately. The dominant predictor for the balanced dataset is now the decision tree.}
% \end{center}
% \end{table}

%Ada
\begin{table}[H]
\caption{Classification metrics for AdaBoost classifier} \label{tab:ada_smote} 
\begin{center} 
\begin{tabular}{cccccccc}
\hline
\multicolumn{1}{c}{} \\
%\cline{1}
Class & precision & precision \textit{SMOTE}& recall & recall \textit{SMOTE}& f1-score & f1-score \textit{SMOTE} & support     \\
\hline
0 & 0.90  & 0.84  & 0.99   & 0.79 &  0.94   & 0.82  &  164 \\
1 & 0.33  & 0.80 &  0.05  &  0.85 &  0.09   & 0.82   & 19/164 \\
\hline
\end{tabular}
\caption{The table shows classification metrics for the or the AdaBoost classifier for both the test set and the synthetic balanced dataset generate with the SMOTE algorithm. Prediction, recall and F1 score for the positive class 1 (converter) improved very strongly, e.g. from a \emph{sensitivity} of 0.05 in the original dataset to 0.85 in the balanced dataset. The accuracy of the of AdaBoost algorithm in the synthetic balanced dataset is 0.82.}
\end{center}
\end{table}

%XBG
\begin{table}[H]
\caption{Classification metrics for Extreme Boosting classifier classifier} \label{tab:xgb_smote} 
\begin{center} 
\begin{tabular}{cccccccc}
\hline
\multicolumn{1}{c}{} \\
%\cline{1}
Class & precision & precision \textit{SMOTE}& recall & recall \textit{SMOTE}& f1-score & f1-score \textit{SMOTE} & support     \\
\hline
0 & 0.90  & 0.88  & 0.99   & 0.98 &  0.84   & 0.93  &  164 \\
1 & 0.00  & 0.98 &  0.00  &  0.87 &  0.00   & 0.92   & 19/164 \\
\hline
\end{tabular}
\caption{The table shows classification metrics for the or the Extreme Gradient boosting classifier for both the test set and the synthetic balanced dataset generate with the SMOTE algorithm. Random forest completelly failed to generalize for posivie class 1 (converters), however for the balanced dataset the algorithm generalizes very well, e.g. \emph{sensitivity} from 0.00 in the orginal dataset to 0.87 in the synthetic dataset. The accuracy of XGBClassifier classifier on the balanced synthetic test set is 0.92.}
\end{center}
\end{table}


We see that the best approach is achieved with the ensemble algorithms, in particular the extreme gradient boosting algorithm (Table \ref{tab:xgb_smote}), AdaBoost (Table \ref{tab:ada_smote}) and random forest (Table \ref{tab:rf_smote}). Bagging plus boosting is then the favored approach. First, bagging, balance the unbalanced dataset using Synthetic Minority oversampling technique (SMOTE) by creating synthetic instances. And train the newly balanced data set using a Gradient Boosting Algorithm. Note that ensemble based methods (eg. gradient tree boosting) are not an alternative to sampling techniques per se – so it is better to use SMOTE first and then  the ensemble algorithm SMOTE+Gradient boosting. However, XG boosting can be applied directly on the imbalanced data, XG Boost is a more advanced form of Boosting and takes care of imbalanced data set by balancing it in itself- so use of sampling techniques may not be necessary. This technique is good at avoiding overfitting which naturally occurs when replicas of the minority set are added. 
We can also integrate sampling techniques with ensemble learning techniques,  (SMOTE + Adaboost), SMOTE can introduce new synthetic sampling at each boosting iteration and the successive classifier ensemble focus more on the minority class.

%YS
Deep networks TO DO

%kNN
% \begin{table}[H]
% \caption{Classification metrics for XXX classifier} \label{tab:logreg_smote} 
% \begin{center} 
% \begin{tabular}{cccccccc}
% \hline
% \multicolumn{1}{c}{} \\
% %\cline{1}
% Class & precision & precision \textit{SMOTE}& recall & recall \textit{SMOTE}& f1-score & f1-score \textit{SMOTE} & support     \\
% \hline
% 0 &   &   &    &  &     &   &  164 \\
% 1 &   &   &    &  &     &    &  19 \\
% \hline
% \end{tabular}
% \caption{The table shows classification metrics for the logistic regression classifier in the test set and the synthetic dataset generate with the SMOTE algorithm. Precision of the positive class goes down for the negative class and goes up for the positive class. Recall is mostly unaffected. The predictor k-NN (k=5) accuracy is 0.561}
% \end{center}
% \end{table}



\section{Discussion and Conclusions}
\label{se:dis}

There is a growing concern about the impact of AD and other forms of dementia sufferers in low income areas in the world.
The Alzheimer’s Society estimates that $71\%$ of dementia sufferers in 2050 will be in poor to middle income countries.
%https://newrepublic.com/article/115936/alzheimers-drastic-rise-low-income-countries

In \emph{Proyecto Vallecas} we make the most of the dataset addressing prescient issues like the relationship between socioecomic status and health. There are concluding reports from governmental agencies that show an association between wealth and life expectancy. 
Educational level also affects positively life expectancy. Even within European countries, life expectancy varies greatly by education level. Life expectancy of males at age 25 with high education level is 10 years longer than for those with low educational level in Poland and the Czech republic \cite{imf2018}. 

Here we show that it is possible to build prediction machines with very satisfactory performance in a subset of the \emph{Vallecas} dataset specifically comprising self-informed features, i.e. variables that can be reported by the subject herself e.g. age, income level, education, sleep, diet etc. 
We aimed at building systems, in the most cost effective way, that predict whether a subject will convert to mild cognitive impairment. Features that require lab tests (e.g APOE gene variant.) and or require an expert to be collected e.g. neurologist, neuropsychologist were excluded. We identified a set of self-informed features with predictive power called the \textbf{Vallecas Index}. The \textbf{Vallecas Index} should not be considered in any way a minimal or complete list of features. Interestingly we find using two different methoda -PCA and random forest- that the dimension of the index can be reduced from 56 to less than 20. 
How prediction is affected by dropping features in the \textbf{Vallecas Index} via elimination or grouping of features needs to be systematically studied.

The size of the subset of the \emph{Vallecas} dataset used here is 1,000 labeled examples which falls short to be qualified as Big Data. Admittedly, taking into consideration the heterogeneity and complexity of the data collected, the \emph{Vallecas} dataset does not lend itself either to traditional database modeling. The necessity of using artificial intelligence and machine learning techniques to extract knowledge in the \emph{Vallecas} dataset is, nevertheless, self-evident.
In machine learning, the rule of thumb to achieve acceptable performance is 5,000 labeled examples per category \cite{goodfellow2016deep}. Although our dataset is only a portion of that our results show that the size of the dataset is not the major impediment. The main difficulty is that the dataset is imbalanced: the converter class is underrepresented with a ratio of 10 to 1 in favor of the non conversion class. This situation can be dealt with getting more data, this is certainly the best option but it is costly and require time. A workaround is to add synthetic data points to create a balanced dataset. We opted for this solution using the Synthetic Minority Oversampling Technique (SMOTE) algorithm to add class 1 (converters) data points to build a balanced dataset
Ensemble methods, in particular Boosting, outperformed the other methods achieving precision and recall over $85\%$ of correct prediction of conversion to MCI in the synthetic (balanced) dataset. 

%The goal is to build algorithms capable of predicting the next stage (non conversion, conversion) in the cohort of \emph{Proyecto Vallecas}. 
We approached the problem of conversion prediction from an engineering point of view, training prediction systems that are capable to generalize when they experience new data. Our results constitutes a proof of work that demographic, socioeconomic and life style factors (i.e. the \textbf{Vallecas Index}) collected systematically in large scale and exploited with artificial intelligence techniques holds promise to identifying modifiable risk factors to conversion to MCI. 
Our long term ambition is building systems tested and validated in the \textit{Vallecas} dataset to be deployed in larger scenarios e.g. health insurers, municipal or regional health systems, that provide comprehensible and accurate predictions. 
The results obtained here makes us very optimistic about the future. 

%-------------------------------------------------------------------------------
% REFERENCES
%-------------------------------------------------------------------------------
\newpage

\addcontentsline{toc}{section}{References}


% BibTeX users please use
\bibliographystyle{spmpsci}
\bibliography{../bibliography-jgr/bibliojgr}

\end{document}


\section{Appendix}
\section{Appendix}
\label{se:ap}
SMOTE

% What is ML and why we need it? Separation between statistics and ML (CS), ML uses experience for doing a better job each time (feedback, teleology)
% %Literal Tangeti
% Machine learning is a branch of study in which a model can learn automatically from the experiences based on data without exclusively being modeled like in statistical models.

% The trade-off between optimization and generalization is the main issue in machine learning. Optimization refers to the process of adjusting a model to get the best performance possible on the training data (the learning in machine learning), whereas generalization refers to how well the trained model performs on data it has never seen before.
% Validation is used in order to deal with information leakage that could drive overfitting in the model.


% Connectionist learning (these days called machine learning), procedures can be divided into three broad classes: Supervised procedures studied in Section \ref{sse:sup}, reinforcement procedures which are not studied here and unsupervised procedures.
% The essential between these three types of approaches is the existence and role of the teacher in the learning.
% While supervised algorithms require a teacher to specify the desired output vector, reinforcement requires only require a single scalar evaluation of the output and finally unsupervised algorithms do not require any teacher nor additional information to  construct internal models that capture regularities in the input vectors  \cite{hinton1990connectionist}. There are often ways of converting one kind of learning procedure into another e.g. self supervised learning can be converted into an unsupervised learning by using the input itself to do the supervision i.e. the desired output vector is identical with the input vector.
% %A learning algorithm for boltzmann machines

% Random under sampling: eliminate randomly majority class examples, for example taking out $10\%$ samples without replacement from the majority class to combining them with the minority class. It reduces the training set discarding potentially useful information, this approach is only advisable for really large datasets.

% Random over-sampling increases the number of elements in the minority class by randomly replicating. Here, contrary to udersampling, there is no information loss but it may increase the likelihood of overfitting since it replicates the minority class events.

% Cluster-Based Over Sampling k-NN algorithm is independently applied to minority and majority class instances to identify clusters in the dataset. Subsequently, each cluster is oversampled such that all clusters of the same class have an equal number of instances and all classes have the same size. The disadvantage of this algorithm, like most oversampling techniques is the possibility of over-fitting the training data.  

% Modified synthetic minority oversampling technique (MSMOTE): Two approaches: Bagging (Bootstrap Aggregating) and Boosting. In bagging we generate ‘n’ different bootstrap training samples with replacement. And training the algorithm on each bootstrapped algorithm separately and then aggregating the predictions at the end. 
% Boosting is an ensemble technique to combine weak learners (small changes in data induce big changes in the classification model.) to create a strong learner. 

AdaBoost (Adaptive Boosting) is the first original boosting technique which creates a highly accurate prediction rule by combining many weak and inaccurate rules. 
Gradient Tree boosting: Decision Trees are used as weak learners. Gradient Boosted trees are harder to fit than random forests
XGBoost (Extreme Gradient Boosting) is an advanced and more efficient implementation of Gradient Boosting Algorithm. It is 10 times faster than the normal Gradient Boosting as it implements parallel processing. Unlike gradient boosting which stops splitting a node as soon as it encounters a negative loss, XG Boost splits up to the maximum depth specified and prunes the tree backward and removes splits beyond which there is an only negative loss.
these four approaches fall into two main classes: Sampling (random, synthetic and cluster based) and Cost Sensitive (Adaptive boosting, cost sensitive decision trees).  
Cost-sensitive methods for imbalanced consists in including the cost associated with misclassifying examples, that is, instead of creating a balanced data distribution with sampling, cost sensitive penalizes learning that misclassifies with the cost matrix. AdaBoost introduces a cost via assigning a weight for the data instances as an updating strategy. Thresholding is another valid forms of cost-sensitive learning (adjust the threshold of predicted probabilities return by the classifier to reduce the misclassified examples).



%https://stackoverflow.com/questions/20082674/unbalanced-classification-using-randomforestclassifier-in-sklearn
%sample_weight parameter is to balance the target classes in training dataset
%len(X) == len(y) == len(sample_wight), each element of sample 1-d array represent weight for a corresponding (observation, label) pair. For 5:1 imbalance, if 1 class is represented 5 times as 0 class is, do: sample_weight = np.array([5 if i == 0 else 1 for i in y])
% scikit-learn 0.17, there is class_weight='balanced' option which you can pass at least to some classifiers:
%The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)).


Thresholding can be used if the algorithm return probabilities, for example \textit{predict proba} adopting an appropriate trade-off level, this level can be optimized by generated a curve of the evaluation metric (e.g. F-measure). The limitation here is that you are making absolute trade-offs. Any modification in the cutoff will in turn decrease the accuracy of predicting the other class. If you have exceedingly high probabilities for the majority of your common classes (e.g. most above 0.85) you are more likely to have success with this method. It is also algorithm independent (provided the algorithm returns probabilities).
Note that the F-measures which combine precision (exactness) and recall (completeness) don’t take into account the true negative rate. Therefore, it is often recommended that in imbalanced settings to use metrics such as Cohen’s kappa metric.
Notation:  a dataset is a collection of examples which are in turn collections of features. The design matrix is the most common form of describing a dataset, $X \in R^\{n,m\}$, where n is the number of examples (rows) and m the number of features (columns), so $X^\{i,j\}$ is the value of the feature j for example i

Capacity: a model's capacity is its capacity to fit a wide variety of functions. for example, we we take the linear regression model we can increase its capacity by introducing x as another feature, note that the output is still a linear function of the parameters and it can be solved with the normal equations. Machine learning algorithms perform best when they have a capacity appropriate for the complexity of the task and the training data available , models with high capacity can solve complex problems but when the capacity exceeds the complexity they may overfit (Occam's razor). 

Complexity: The complexity of a model can be measured with the Vapnik-Chervonenkis dimension (complexity of binary classifier). The discrepancy between training error and generalization error is bounded from above by a quantity that grows as the capacity grows but shrinks as the size of the training set increases. Interestingly, these bounds are the intellectual justification of why machine learning works.

The network or model capacity is the number of learnable parameters -number of layers and units per layer-. A model with more parameters will have more memorization capacity but that does not imply that will generalize well.On the other hand, if the network has limited memorization resources, it won’t be able to learn this mapping as easily; thus, in order to minimize its loss, it will have to resort to learning compressed representations that have predictive power regarding the targets—precisely the type of representations we’re interested in. (Keep inn mind that you should use models that have enough parameters that they don’t underfit: your model shouldn’t be starved for memorization resources). There is a compromise to be found between too much capacity and not enough capacity.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Topological Data Analysis}
\label{sse:TDA}
%https://www.quora.com/Is-there-a-relationship-between-manifold-learning-and-topological-data-analysis
TDA focuses on identifying global features within the data.
Manifold learning focuses on reducing dimensionality by finding a good map from the data to a low-dimensional manifold. Topological data analysis could be helpful to analysis (manifold) data before and after dimensionality reduction (i.e. manifold learning). The topology of the manifold should be the same and independent of the dimensionality of the ambient space.

Manifold learning assumes that the data lies on a (usually low) dimensional submanifold and has as a goal to create a representation that faithfully records this local neighborhood structure given in the manifold. 
TDA, on the other hand, makes very few assumptions about the data and the goal is not to faithfully reconstruct the data - or to fit the data to a model - but to provide (unbiased) summaries of the geometric/topological structure in the data.
Persistent homology and mapper both take as inputs some kind of "space" (e.g simplicial complex) and both use a function to create a summary. For Mapper the summary is (loosely speaking) a recording of the connectivity of the fibers of the  function while for persistent homology it's the relationship between the homology of sublevel sets of the function. The output is a "barcode" for persistent homology or a simplicial complex in the case of mapper.The main point here is that to create such a summary very little is needed beyond some notion of similarity or proximity in your data.

As we saw in the results (no separation of data points!), the manifold assumption (locally uniform and smooth) is quite strong and outside of scientific contexts have not found data sets that look like manifolds.

%https://jsseely.github.io/notes/TDA/
TDA involves ‘fitting’ a topological space to data, then perhaps computing topological invariants of that space. TDA is related to two familiar problems: clustering and manifold learning. In some sense, TDA is a generalization of both problems.
We can view many data analysis problems as ‘fitting a space to data’. E.g. both PCA and linear regression involve fitting a linear subspace to data; the space perspective complements the statistical (least-squares) and algebraic (SVD, pseudoinverse) perspectives of these techniques.
How do we generalize these linear techniques? A stock answer: manifold learning. But, manifolds are restrictive objects. A consequence of being locally Euclidean is that they cannot contain singularities and must have the same dimension everywhere. A mathematician’s motivation for working with manifolds is that one can do calculus on them. But if this is not the goal, then there may be little reason to assume a restrictive type of space in a data analysis context. Indeed, the singularities of a space are often the interesting points of study (e.g. bifurcation points), and one wants tools to capture these.
%Simplicial complexes
Topological spaces are a natural thing to turn to whenever one wants to ‘fit a space to data’. Yet topological spaces, without restriction, are too general. Simplicial complexes, which can be viewed as a convenient middle ground of specificity and generality in modeling spaces. Simplicial complexes lie somewhere between graphs and hypergraphs. All graphs are special kinds of simplicial complexes, and all simplicial complexes are special kinds of hypergraphs. “Most” topological spaces of interest can be discretized (triangulated) and represented as a simplicial complex.
%Persistent homology
The workhorse technique of TDA is persistent homology
%Mapper
An especially underused TDA technique is “mapper”. I found this particularly useful for visualization, and I wonder why it isn’t as widely applied as, say, t-SNE. Mapper fits a simplicial complex to data (often, just a graph), but in a very flexible way. The goal of mapper is either data visualization or clustering. The key insight offered by this technique is that many interesting “clusters” in real data are not clusters in the classical sense (as disconnected components), but are the branches of some single connected component. 
We didnt find clusters with SVD nor tSNE, can we find some with Mapper??


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Instead of building a BaggingClassifier and passing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees.


%for example for 3 coins np.power(0.49,3) + 3*(np.power(0.49,2)*0.55 + np.power(0.51,2)*0.49)  +np.power(0.51,3)
% p(aaa) or p(aab) = 0.51499 = 3*np.power(0.51,2)*0.49  +np.power(0.51,3)
%However, this is only true if all classifiers are perfectly independent, making uncorrelated errors, which is clearly not the case since they are trained on the same data. The voters will likely make the same types of errors, since they were trained on the same data, so there will be many majority votes for the wrong class, reducing the ensemble’s accuracy.

%pg 279 “from sklearn.ensemble import RandomForestClassifier
%from sklearn.ensemble import VotingClassifier
%from sklearn.linear_model import LogisticRegression
%from sklearn.svm import SVC

%log_clf = LogisticRegression()
%rnd_clf = RandomForestClassifier()
%svm_clf = SVC()

%voting_clf = VotingClassifier(
%        estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],
%        voting='hard'
%    )
%voting_clf.fit(X_train, y_train)”
%“>>> from sklearn.metrics import accuracy_score
%>>> for clf in (log_clf, rnd_clf, svm_clf, voting_clf):
%>>>     clf.fit(X_train, y_train)
%>>>     y_pred = clf.predict(X_test)
%>>>     print(clf.__class__.__name__, accuracy_score(y_test, y_pred))
%LogisticRegression 0.864
%RandomForestClassifier 0.872
%SVC 0.888
%VotingClassifier 0.896”



\textbf{Bagging and Pasting}
Before we saw how to introduce variability by using different classifiers, another way to introduce diversity in the voters is using the same algorithm but trained in different datasets ie. Bagging and Pasting.
%for Pasting BaggingClassifier(...bootstrap=False
“When sampling is performed with replacement, this method is called bagging (short for bootstrap aggregating). When sampling is performed without replacement, it is called pasting.”. “Both bagging and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor” \cite{geron2017hands}

So for one training set we get n training sets by sampling it randomly (with replacement for bootstrap) and then training on each sample.“Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors”. “The aggregation function is typically the statistical mode (i.e., the most frequent prediction, just like a hard voting classifier) for classification, or the average for regression”. Note that each individual predictor has a larger bias than if it were trained on the original (same size) dataset but aggregation reduces both bias and variance.
Bagging and Pasting scale very well because they can be trained and also predict in parallel (GPUs).

%“ BaggingClassifier automatically performs soft voting instead of hard voting if the base classifier can estimate class probabilities ”

%“from sklearn.ensemble import BaggingClassifier
%from sklearn.tree import DecisionTreeClassifier

%bag_clf = BaggingClassifier(
%        DecisionTreeClassifier(), n_estimators=500,
%        max_samples=100, bootstrap=True, n_jobs=-1
%    )
%bag_clf.fit(X_train, y_train)
%y_pred = bag_clf.predict(X_test)”
“Overall, bagging often results in better models than pasting, which explains why it is generally preferred”
“With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all”

%bag_clf.oob_score_ is a proxy of accuracy in the test test, because some samples will never see them, the out of bag
%“oob_decision_function_ variable. In this case (since the base estimator has a predict_proba() method) the decision function returns the class probabilities for each training instance” tells the estimate of class for each training instance






%http://blog.kaggle.com/2014/08/01/learning-from-the-best/
%“from sklearn.ensemble import RandomForestClassifier

%rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)
%rnd_clf.fit(X_train, y_train)
%“With a few exceptions, a RandomForestClassifier has all the hyperparameters of a DecisionTreeClassifier (to control how trees are grown), plus all the hyperparameters of a BaggingClassifier to control the ensemble itself”

\textbf{The kernel trick}
 %“ It makes it possible to get the same result as if you added many polynomial features, even with very high-degree polynomials, without actually having to add them. ”
Another 'trick' to deal with nonlinearity in the dataset is adding similarity features, which is achieved using a similarity function that measures how much each instance matches a particular landmark. The Gaussian Radial function (GRF) is a bell-shape  similarity function. % that returns 0 for very far from the landmark to 1 very close to it. 
We transform the dataset using the function $\phi_{\gamma}(x,d) = e(-\gamma ||x-d||^2)$, where $\gamma$ is a hyperparameter (approximated using grid ) and d is the distance from x to the landmark (with this transform the dataset may become linearly separable). Increasing $\gamma$ makes the Bell curve narrower, which means that the decision boundary is more wiggling around individual instances. Conversely a large $\gamma$ makes the Bell curve wider and the decision boundary is smoother (instances have more influence), so $\gamma$ acts like a regularization parameter, if the model is overfitting, reduce $\gamma$ (wider) and if you underfit increase (too much regularization) $\gamma$ (narrow) or C or both. The C parameter behaves in a similar way (pg 263 for 1 dimensional to 2 dimensional \cite{geron2017hands}).

How to create a landmark? the easiest is one landmark at each point, the problem s that we make the dateset bigger, from m points and n features to $m \times m$. Thus, compute the additional features (m-n) can become computationally expensive, however the kernel trick does the magic 
%“SVC(kernel="rbf", gamma=5, C=0.001)”

GRF is not he only kernel there are vene specialized kernels, for example  string kernels for classifying texts or DNA sequences (Levenshtein distance). Which kernel to use? Asa rule of thumb start with linear kernel \emph{LinearSVC}(\emph{LinearSVC} is faster than \emph{SVC(kernel=linear)}, especially is the training set is very large or has many features, if this is not the case use Gaussian Radial basis function (RBF).

In terms of complexity, \emph{LinearSVC} (\emph{liblinear}) does not support the kernel trick, but it scales almost linearly with the number of training instances and features, so its training time complexity is $O(m \times n$).
The \emph{SVC} (\emph{libsvm}) class supports the kernel trick and the training time is cuadratic or cubic with the number of instances $O(m^{2} \times n$) . 


\textbf{Notes on Training the model }


%\textbf{Over and underfitting}
If a model performs well on the training data but generalizes poorly according to the cross-validation metrics, then your model is overfitting. If it performs badly in both is underfitting or what is the same the model is too complex or too simple. Another way to look at model's behavior is using learning curves: plots performance in training and validation set as a function of the training data size. To generate the plots, simply train the model several times on different sized subsets of the training set.
Undefitting: both training and validation set reach a plateau, i.e. throwing more data doesn't translate into better performance.
Thus, underfitting is easily detected when adding more training example doesn't improve the performance, what is needed is a more complex model or come up with better features.

The hallmark of overfitting is the gap between curves -the model performs better in the training set- however if we use a largerr dataset the curves can eventually meet.

The hallmark of undefiting is plateau.


\textbf{Regularization}

The bias/variance tradeoff: Increasing a model’s complexity will typically increase its variance and reduce its bias. Conversely, reducing a model’s complexity increases its bias and reduces its variance. To reduce the overfitting we can use regularization, for example, in a polynomial model just reduce the number of polynomial degrees.
For a linear model, regularization is typically achieved by constraining the weights of the model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net, which implement three different ways to constrain the weights.”
Always scale the inputs if using regularization.


Ridge or Tikhonov regularization is a regularized version of linear regression, a term $\alpha \sum_{i=1}^{n}(\theta_i)^2$ (the hyperparameter $\alpha$ tells how much regularization we want, if 0 then is identical to linear regression. Note also that the bias term $\theta_0$ is not regularized) is added to the cost function, the goal is not only to fit the data but keeping the model weights as small as possible. Note that the regularization term is only used during training, once the model is trained the model's performance is done using unregularized cost measure.
As with linear regression we can compute Ridge using the closed form or gradient descent.
\begin{equation}
\hat{\Theta} = (X^{T}X + \alpha A)^{-1}X^{T}y
\end{equation}

Lasso uses $l_1$ instead of the square of the $l_2$ norm as in ridge as a regularization term in the cost function. Lasso tends to eliminate entirely the coefficients of the least important features, that is, Lasso Regression automatically performs feature selection and outputs a sparse model (i.e., with few nonzero feature weights).
%Scikit-Learn example using the Lasso class. Note that you could instead use an SGDRegressor(penalty="l1")”

elastic net is a middle ground between Ridge $(r=0)$ and Lasso $(r=1)$. “It is almost always preferable to have at least a little bit of regularization, so generally you should avoid plain Linear Regression. Ridge is a good default, but if you suspect that only a few features are actually useful, you should prefer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to zero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated.”
%“from sklearn.linear_model import ElasticNet”

\textbf{Early stopping}
An alternative approach to deal with overfitting is early stopping: stops training as soon as the validation error reaches a minimum. In the learning curve is the validation set reaches a minimum and then rises , this means that the model has started to overfit the data. With early stopping you just stop training as soon as the validation error reaches the minimum. It is such a simple and efficient regularization technique that Geoffrey Hinton called it a “beautiful free lunch.”
%“ warm_start=True, when the fit() method is called, it just continues training where it left off instead of restarting from scratch”

\begin{equation}
MSE(X,\Theta) = \frac{1}{m} \sum_{i=1}^{m}(\hat{y}^{i} - y^{i})^2 = \frac{1}{m} \sum_{i=1}^{m}(\Theta^Tx^{i} - y^{i})^2
\end{equation}
T find the value of the vector $\Theta$ we can use the closed-form also called normal equation
\begin{equation}
\hat{\Theta} = (X^{T}X)^{-1}X^{T}y
\end{equation}
where $\hat{\Theta}$ is the value that minimizes the cost function and y is the vector of target values.

Fortunately the MSE cost function for a LR happens to be convex (choosing two points, the line segment never crosses the curve) that is, no local minima just one global minimum and it is also continuous and smooth function, thats to this: “Gradient Descent is guaranteed to approach arbitrarily close the global minimum (if you wait long enough and if the learning rate is not too high).”

Importantly, when using Gradient Descent, you should ensure that all features have a similar scale (e.g., using Scikit-Learn’s StandardScaler class), or else it will take much longer to converge. (see pag 202 Aurelien book). The search is in the parameter space, so the more parameters the longer (finding a needle in a 300 dimensional haystack s much harder than in a 3 dimensional one, but the haystack is convex, so we know the needle sis always at the bottom)!


To calculate the gradient descent we need to do the partial derivative of the cost function for each model parameter $\Theta$ in order to see how the cost function will change if a I change the parameter just a little bit(what is the slope of the mountain if I change $\Theta_i$ eg direction east and so on for each dimension ($\frac{\partial}{\partial \theta_j} MSE(\Theta$). Once we have the gradient vector (which points uphill) just subtract the gradient (weighted by the learning rate) from $\theta$ to go downhill

The main problem with Batch GD is that it uses the entire training set to compute the gradient at every step, which makes it slow when the training set is large. Stochastic GD picks a subset (a random instance) in the training set at every step and computes the gradient for that instance, this makes the algorithm faster, with the inconvenient that its randomness (stochasticity) makes it less regular than GD, the cost function rather than slowly going to the minimum it may bounce up and down, but still decreasing on the average. However, randomness is actually good to escape from local optima but it could never settle in the global minimum.
\textbf{simulated annealing} can solve this problem playing with the learning rate, it starts with a large one, to avoid local minimum, to make shorter steps as the algorithm approaches the global minimum (learning schedule).
Each round of iterations is called epoch so in SGD the algorithm goes through the training set epochs times. Note that since instances are picked randomly per each epoch some instances can be chosen more than once while others none.
%SGDRegressor “To perform Linear Regression using SGD with Scikit-Lear”

Mini batch GD: “at each step, instead of computing the gradients based on the full training set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-batch GD computes the gradients on small random sets of instances called mini-batches”. Better than SGD is we use GPU, the algorithm is less erratic than SGD especially with fairly large mini-batches

Importantly these 3 algorithms will yield the same predictions choosing one or another relies upon the use computational resources we wish.



\textbf{Model selection}
A linear regression model is something like this
\begin{equation}
y = \theta_0 + \theta_{1}x
\end{equation}
for example, $happiness = \theta_0 + \theta_1 wealth$., that is, the model is just a lienar fucntion of the input feature $wealth$ and $\theta_0, \theta_{1}$ are the model parameters. A linear makes a prediction $\hat{y}$ based on the weighted sum of the input features pus a bias term(or intercept), $\hat{y} = \theta_0 + \theta_1 x_1 + ...+ \theta_n x_n$. This can be written in a more compact form as
\begin{equation}
y = \Theta^{T} X 
\end{equation}
where $\Theta$ is the model's parameter vector ( a column so we need to transpose y have a row ($\Theta^{T} (1 \times n), X (n \times 1)$ to get an scalar $(1 \times 1)$ which is the actual prediction for that input instance ), X is the instance vector (note that $x_0$ is always 1).
In the previous section we sketched different evaluation metrics now we will see how to apply them in order to select the best model for our application. 
Model selection is the phase in the modeling process in which we opt for one model or other based on the model's performance which is being previously evaluated.

There are three main methods of model selection: Train-Test split, K-fold Cross Validation and Grid Search.
The train/test method split the data set into two portions testing set and test test. The training set is used to train the model and the testing set is used to test the model. The pros are flexibility and speed, the cons provides a high  variance estimate of out of sample. 

This method splits the data set into K equal partitions (“folds”), then use 1 fold as the testing set and the union of the other folds as the training set. Then the model is tested for accuracy. The process will follow the above steps K times, using different fold as the testing set each time. The average testing accuracy of the process is the testing accuracy.
%https://towardsdatascience.com/machine-learning-workflow-on-diabetes-data-part-01-573864fcc6b8
In K-fold Cross Validation the data set is divided into K equal partitions or “folds”, 1 fold is used as the testing set and the union of the other sets are the training set. The process is performed K times using different fol as the testing set each time. the average testing accuracy of the K iterations is the testing accuracy.
In Cross Validation we do not to set any parameter, apart from the number of folds (cv) from which we calculate the average performance across folds. The pros are more accurate estimate of out-of-sample accuracy, more “efficient” use of data in the sense that every observation is used for both training and testing. The cons are slow compared to Train/Test split.

In GridSearch we need a gamma parameter (for example in SVC with rbf kernel set the radius), GridSearchCV finds the gamma that optimizes the given evaluation metric (accuracy, precision etc).
%cross validation, no parameters only cv (number of folds)
%from sklearn.model_selection import cross_val_score; from sklearn.svm import SVC
%print('cross validation (accuracy)', cross_val_accuracy(clf, X,y, cv=5))
%print('cross validation (AUC)', cross_val_accuracy(clf, X,y, cv=5, scoring = 'roc_auc'))
%print('cross validation (recall)', cross_val_accuracy(clf, X,y, cv=5, scoring = 'recall'))
%not that we do not any parameter tunning just 5 folds and examine performance

%% Grid Search, we need gamma parameter
%from sklearn.model_selection import GridSearchCV
%grid_values = {'gamma': [0.001, 0.01, 0.1, 1 10]}
%grid_clf_acc = GridSearchCV(clf, param_grid = grid_values); grid_clf_acc.fit(X_train, y_train)
%#optimize for acc
% y_decision_fn_scores_acc = grid_clf_acc.decision_function(X_test); print('Grid best parameter (max. accuracy):', grid_clf_acc.best)
%print('Grid best score (accuracy):', grid_clf_acc.best_score_) 
%#optimize for auc
%grid_clf_auc = GridSearchCV(clf, param_grid = grid_values, scoring='roc_auc'); grid_clf_auc.fit(X_train, y_train)
%print('Grid best score (accuracy):', grid_clf_auc.best_score_) 
%the gamma parameter can be equal but also different depending on the metric used

The evaluation metric supported for model selection are
\begin{lstlisting}
%from sklearn.metrics.scorer import SCORERS
%sorted(list(SCORERS.keys()))
%['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'log_loss', 'mean_absolute_error', 'mean_squared_error', 'median_absolute_error', 'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']
\end{lstlisting}
The decision boundaries change when it is optimize for different metrics

Three data splits:
\begin{itemize}
	\item Training set: model building
	\item Evaluation set: model selection 
	\item Test set: final evaluation
\end{itemize}


%(Belsey's 80) approach is based on covariance matrix of the least squares coefficient vector. Decaying importance of the variables as ordered by the eigenvalues

%\ref{sse:interp}
%Feature importance scores:A feature may be important if it is highly correlated with the dependent variable (the thing being predicted). Correlation coefficients and other univariate (each attribute is considered independently) methods are common methods
%Complex predictive modeling algorithms perform feature importance and selection internally while constructing their mode Random Forest and Gradient Boosted Machines.

\section{Temp results}
Correlation 

Explanatory variables

['sexo', 'lat_manual', 'nivel_educativo', 'apoe', 'edad', 'hsnoct', 'sue_dia', 'sue_noc', 'sue_con', 'sue_man', 'sue_suf', 'sue_pro', 'sue_ron', 'sue_mov', 'sue_rui', 'sue_hor', 'sue_rec', 'imc', 'dempad', 'edempad', 'demmad', 'edemmad', 'audi', 'visu', 'a01', 'a02', 'a03', 'a04', 'a05', 'a06', 'a07', 'a08', 'a09', 'a10', 'a11', 'a12', 'a13', 'a14', 'sdhijos', 'numhij', 'sdvive', 'sdeconom', 'sdresid', 'sdestciv', 'sdtrabaja', 'sdocupac', 'sdatrb', 'hta', 'hta_ini', 'glu', 'lipid', 'tabac', 'tabac_ini', 'tabac_fin', 'tabac_cant', 'sp', 'cor', 'cor_ini', 'arri', 'arri_ini', 'card', 'card_ini', 'tir', 'ictus', 'ictus_num', 'ictus_ini', 'ictus_secu', 'tce', 'tce_num', 'tce_ini', 'tce_con', 'tce_secu', 'alfrut', 'alcar', 'alpesblan', 'alpeszul', 'alaves', 'alaceit', 'alpast', 'alpan', 'alverd', 'alleg', 'alemb', 'allact', 'alhuev', 'aldulc', 'scd_visita1', 'edadinicio_visita1', 'tpoevol_visita1', 'peorotros_visita1', 'preocupacion_visita1', 'eqm06_visita1', 'eqm07_visita1', 'eqm81_visita1', 'eqm82_visita1', 'eqm83_visita1', 'eqm84_visita1', 'eqm85_visita1', 'eqm86_visita1', 'eqm09_visita1', 'eqm10_visita1', 'act_aten_visita1', 'act_orie_visita1', 'act_mrec_visita1', 'act_memt_visita1', 'act_visu_visita1', 'act_expr_visita1', 'act_comp_visita1', 'act_ejec_visita1', 'act_prax_visita1', 'act_depre_visita1', 'act_ansi_visita1', 'act_apat_visita1', 'gds_visita1', 'stai_visita1', 'eq5dmov_visita1', 'eq5dcp_visita1', 'eq5dact_visita1', 'eq5ddol_visita1', 'eq5dans_visita1', 'eq5dsalud_visita1', 'eq5deva_visita1', 'relafami_visita1', 'relaamigo_visita1', 'relaocio_visita1', 'rsoled_visita1', 'ejfre_visita1', 'ejminut_visita1', 'valcvida_visita1', 'valsatvid_visita1', 'valfelc_visita1', 'conversion']

%-------------------------------------------------------------------------------
% SNIPPETS
%-------------------------------------------------------------------------------

%\begin{figure}[!ht]
%	\centering
%	\includegraphics[width=0.8\textwidth]{file_name}
%	\caption{}
%	\centering
%	\label{label:file_name}
%\end{figure}

%\begin{figure}[!ht]
%	\centering
%	\includegraphics[width=0.8\textwidth]{graph}
%	\caption{Blood pressure ranges and associated level of hypertension (American Heart Association, 2013).}
%	\centering
%	\label{label:graph}
%\end{figure}

%\begin{wrapfigure}{r}{0.30\textwidth}
%	\vspace{-40pt}
%	\begin{center}
%		\includegraphics[width=0.29\textwidth]{file_name}
%	\end{center}
%	\vspace{-20pt}
%	\caption{}
%	\label{label:file_name}
%\end{wrapfigure}

%\begin{wrapfigure}{r}{0.45\textwidth}
%	\begin{center}
%		\includegraphics[width=0.29\textwidth]{manometer}
%	\end{center}
%	\caption{Aneroid sphygmomanometer with stethoscope (Medicalexpo, 2012).}
%	\label{label:manometer}
%\end{wrapfigure}

%\begin{table}[!ht]\footnotesize
%	\centering
%	\begin{tabular}{cccccc}
%	\toprule
%	\multicolumn{2}{c} {Pearson's correlation test} & \multicolumn{4}{c} {Independent t-test} \\
%	\midrule
%	\multicolumn{2}{c} {Gender} & \multicolumn{2}{c} {Activity level} & \multicolumn{2}{c} {Gender} \\
%	\midrule
%	Males & Females & 1st level & 6th level & Males & Females \\
%	\midrule
%	\multicolumn{2}{c} {BMI vs. SP} & \multicolumn{2}{c} {Systolic pressure} & \multicolumn{2}{c} {Systolic Pressure} \\
%	\multicolumn{2}{c} {BMI vs. DP} & \multicolumn{2}{c} {Diastolic pressure} & \multicolumn{2}{c} {Diastolic pressure} \\
%	\multicolumn{2}{c} {BMI vs. MAP} & \multicolumn{2}{c} {MAP} & \multicolumn{2}{c} {MAP} \\
%	\multicolumn{2}{c} {W:H ratio vs. SP} & \multicolumn{2}{c} {BMI} & \multicolumn{2}{c} {BMI} \\
%	\multicolumn{2}{c} {W:H ratio vs. DP} & \multicolumn{2}{c} {W:H ratio} & \multicolumn{2}{c} {W:H ratio} \\
%	\multicolumn{2}{c} {W:H ratio vs. MAP} & \multicolumn{2}{c} {\% Body fat} & \multicolumn{2}{c} {\% Body fat} \\
%	\multicolumn{2}{c} {} & \multicolumn{2}{c} {Height} & \multicolumn{2}{c} {Height} \\
%	\multicolumn{2}{c} {} & \multicolumn{2}{c} {Weight} & \multicolumn{2}{c} {Weight} \\
%	\multicolumn{2}{c} {} & \multicolumn{2}{c} {Heart rate} & \multicolumn{2}{c} {Heart rate} \\
%	\bottomrule
%	\end{tabular}
%	\caption{Parameters that were analysed and related statistical test performed for current study. BMI - body mass index; SP - systolic pressure; DP - diastolic pressure; MAP - mean arterial pressure; W:H ratio - waist to hip ratio.}
%	\label{label:tests}
%\end{table}
